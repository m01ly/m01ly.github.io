<!DOCTYPE html><html lang="zh-Hans"><head><!--[if IE]><style>body{display:none;}</style><script>alert('IE浏览器下无法展示效果，请更换浏览器！');var headNode=document.getElementsByTagName('head')[0];var refresh=document.createElement('meta');refresh.setAttribute('http-equiv','Refresh');refresh.setAttribute('Content','0; url=http://outdatedbrowser.com/');headNode.appendChild(refresh);</script><![endif]--><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="m01ly"><meta name="baidu-site-verification" content="yvSXOjM1ag"><meta name="google-site-verification" content="riwnp5QCq6dRKbhBa3d3aDZrfGLQVhMFN9d4fMGYuoU"><meta name="description" content="description123456"><meta property="og:type" content="website"><meta property="og:title" content="Hexo"><meta property="og:url" content="https://m01ly.github.io/page/3/index.html"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="description123456"><meta property="og:locale"><meta property="article:author" content="m01ly"><meta property="article:tag" content="description123456"><meta name="twitter:card" content="summary"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="shortcut icon" href="/favicon.ico"><link href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css" rel="stylesheet"><link rel="stylesheet" href="/css/style.css"><title>Hexo</title><script src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"><script src="//cdn.bootcss.com/aos/2.2.0/aos.js"></script><script>var yiliaConfig={fancybox:!0,isHome:!0,isPost:!1,isArchive:!1,isTag:!1,isCategory:!1,fancybox_js:"//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js",search:!0}</script><script>yiliaConfig.jquery_ui=[!1]</script><script>yiliaConfig.rootUrl="/"</script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/rss+xml">
<link rel="stylesheet" href="/css/prism-a11y-dark.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="container"><div class="left-col"><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><form id="search-form"><input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false"> <i class="fa fa-times" onclick="resetSearch()"></i></form><div id="local-search-result"></div><p class="no-result">No results found <i class="fa fa-spinner fa-pulse"></i></p><div id="switch-btn" class="switch-btn"><div class="icon"><div class="icon-ctn"><div class="icon-wrap icon-house" data-idx="0"><div class="birdhouse"></div><div class="birdhouse_holes"></div></div><div class="icon-wrap icon-ribbon hide" data-idx="1"><div class="ribbon"></div></div><div class="icon-wrap icon-link hide" data-idx="2"><div class="loopback_l"></div><div class="loopback_r"></div></div><div class="icon-wrap icon-me hide" data-idx="3"><div class="user"></div><div class="shoulder"></div></div></div></div><div class="tips-box hide"><div class="tips-arrow"></div><ul class="tips-inner"><li>菜单</li><li>标签</li><li>友情链接</li><li>目标</li></ul></div></div><div id="switch-area" class="switch-area"><div class="switch-wrap"><section class="switch-part switch-part1"><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" rel="noopener" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" rel="noopener" href="https://music.163.com/" title="网易云音乐"></a></ul><ul class="social"><div class="donateIcon-position"><p style="display:block"><a class="donateIcon" href="javascript:void(0)" onmouseout='var qr=document.getElementById("donate");qr.style.display="none"' onmouseenter='var qr=document.getElementById("donate");qr.style.display="block"'>赏</a></p><div id="donate"><img id="multipay" src="/img/multipay.png" width="250px" alt="m01ly Multipay"><div class="triangle"></div></div></div></ul></nav></section><section class="switch-part switch-part2"><div class="widget tagcloud" id="js-tagcloud"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TLS/" rel="tag">TLS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zookeeper/" rel="tag">Zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/" rel="tag">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/" rel="tag">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BC%81%E4%B8%9A%E5%AE%89%E5%85%A8%E5%BB%BA%E8%AE%BE/" rel="tag">企业安全建设</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E5%85%A8%E5%B7%A5%E5%85%B7/" rel="tag">安全工具</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" rel="tag">安装教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8%E6%95%99%E7%A8%8B/" rel="tag">容器教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/" rel="tag">密码学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" rel="tag">插件开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E4%BB%93%E9%87%87%E9%9B%86%E9%A1%B9%E7%9B%AE/" rel="tag">数仓采集项目</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/" rel="tag">日志管理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/" rel="tag">流量分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" rel="tag">渗透测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%8F%E6%B4%9E%E5%A4%8D%E7%8E%B0/" rel="tag">漏洞复现</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%8F%E6%B4%9E%E6%89%AB%E6%8F%8F/" rel="tag">漏洞扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%BB%E5%8A%A8%E5%AE%89%E5%85%A8/" rel="tag">移动安全</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%B6%E5%9C%BAwriteup/" rel="tag">靶场writeup</a></li></ul></div></section><section class="switch-part switch-part3"><div id="js-friends"><a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/TechCatsLab">TechCatsLab</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://yangchenglong11.github.io">YangChengLong</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://jsharkc.github.io">LiuJiaChang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://blog.yusank.space">YusanKurban</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.lizebang.top">Lizebang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/sunanxiang">SunAnXiang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/DoubleWoodH">LinHao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.littlechao.top">ShiChao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/Txiaozhe">TangXiaoJi</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/LLLeon">JiaChenHui</a></div></section><section class="switch-part switch-part4"><div id="js-aboutme">不悲不喜，不卑不亢，努力成为一个更好的程序猿！</div></section></div></div></header></div></div><div class="hide-left-col" title="隐藏侧栏"><i class="fa fa-angle-double-left"></i></div><div class="mid-col"><nav id="mobile-nav"><div class="overlay"><div class="slider-trigger"></div><h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">m01ly</a></h1></div><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/" title="回到主页">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li><div class="clearfix"></div></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" href="https://music.163.com/" title="网易云音乐"></a></ul></nav></header></div><link class="menu-list" tags="标签" friends="友情链接" about="目标"></nav><div class="body-wrap"><article id="post-bigdata-zookeeper2-framework" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/12/01/bigdata-zookeeper2-framework/" class="article-date"><time class="published" datetime="2020-12-01T05:22:37.000Z" itemprop="datePublished">2020-12-01 发布</time> <time class="updated" datetime="2021-12-01T06:30:57.763Z" itemprop="dateUpdated">2021-12-01 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/12/01/bigdata-zookeeper2-framework/">Zookeeper学习笔记（二） 架构解析</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="1-节点类型"><a href="#1-节点类型" class="headerlink" title="1 节点类型"></a>1 节点类型</h2><p>​ <img src="/2020/12/01/bigdata-zookeeper2-framework/1638340173624.png" alt="1638340173624"></p><h2 id="2-Stat结构体"><a href="#2-Stat结构体" class="headerlink" title="2 Stat结构体"></a>2 Stat结构体</h2><p>（1）czxid-创建节点的事务zxid</p><p>每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。</p><p>事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</p><p>（2）ctime - znode被创建的毫秒数(从1970年开始)</p><p>（3）mzxid - znode最后更新的事务zxid</p><p>（4）mtime - znode最后修改的毫秒数(从1970年开始)</p><p>（5）pZxid-znode最后更新的子节点zxid</p><p>（6）cversion - znode子节点变化号，znode子节点修改次数</p><p>（7）dataversion - znode数据变化号</p><p>（8）aclVersion - znode访问控制列表的变化号</p><p>（9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。</p><p>（10）dataLength- znode的数据长度</p><p>（11）numChildren - znode子节点数量</p><h2 id="3-监听器原理（面试重点）"><a href="#3-监听器原理（面试重点）" class="headerlink" title="3 监听器原理（面试重点）"></a>3 监听器原理（面试重点）</h2><p><img src="/2020/12/01/bigdata-zookeeper2-framework/1638340203888.png" alt="1638340203888"></p><h2 id="4-选举机制（面试重点）"><a href="#4-选举机制（面试重点）" class="headerlink" title="4 选举机制（面试重点）"></a>4 选举机制（面试重点）</h2><p>（1）半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。</p><p>（2）Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。</p><p>（3）以一个简单的例子来说明整个选举的过程。</p><p>假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。</p><p><img src="/2020/12/01/bigdata-zookeeper2-framework/1638340230133.png" alt="1638340230133"></p><p>Zookeeper的选举机制</p><p>（1）服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOOKING；</p><p>（2）服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的（服务器1）大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOOKING</p><p>（3）服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；</p><p>（4）服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING；</p><p>（5）服务器5启动，同4一样当小弟。</p><h2 id="5-写数据流程"><a href="#5-写数据流程" class="headerlink" title="5 写数据流程"></a>5 写数据流程</h2><p><img src="/2020/12/01/bigdata-zookeeper2-framework/1638340252879.png" alt="1638340252879"></p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Zookeeper/" rel="tag">Zookeeper</a></li></ul></div><span class="post-count">总字数780</span> <span class="post-count">预计阅读2分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-zookeeper1-setup" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/12/01/bigdata-zookeeper1-setup/" class="article-date"><time class="published" datetime="2020-12-01T03:22:37.000Z" itemprop="datePublished">2020-12-01 发布</time> <time class="updated" datetime="2021-12-01T06:28:04.229Z" itemprop="dateUpdated">2021-12-01 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/12/01/bigdata-zookeeper1-setup/">Zookeeper学习笔记（一） 搭建教程</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="1-Zookeeper入门"><a href="#1-Zookeeper入门" class="headerlink" title="1 Zookeeper入门"></a>1 Zookeeper入门</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><p>Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。</p><p>Zookeeper从设计模式角度来理解，是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生了变化，Zookeeper就负责通知已经在Zookeeper上注册的那些观察者做出相应的反应.</p><p><strong>Zookeeper = 文件系统 + 通知机制</strong></p><p>​ <img src="/2020/12/01/bigdata-zookeeper1-setup/1638329279116.png" alt="1638329279116"></p><h2 id="1-2-特点"><a href="#1-2-特点" class="headerlink" title="1.2 特点"></a>1.2 特点</h2><p><img src="/2020/12/01/bigdata-zookeeper1-setup/1638329307358.png" alt="1638329307358"></p><p>（1）zookeeper：一个领导者（leader），多个跟随者（Followers）组成的集群。</p><p><strong>（2）集群中只要又半数以上节点存货，zookeeper集群就能正常服务。</strong></p><p>（3）全局数据一致性，每个server保存一份相同的数据副本，client无论连接到哪里，数据都是一致的。</p><p>（4）更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行。</p><p>（5）数据更新原子性，一次数据更新要么成功，要么失败。</p><p>（6）实时性：在一定时间范围内，client能读到最新数据</p><h2 id="1-3-数据结构"><a href="#1-3-数据结构" class="headerlink" title="1.3 数据结构"></a>1.3 数据结构</h2><p>ZooKeeper数据模型与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称作一个ZNode。每个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。</p><p><img src="/2020/12/01/bigdata-zookeeper1-setup/1638329609040.png" alt="1638329609040"></p><h2 id="1-4-应用场景"><a href="#1-4-应用场景" class="headerlink" title="1.4 应用场景"></a>1.4 应用场景</h2><p>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。</p><h3 id="1-4-1-统一服务命名服务"><a href="#1-4-1-统一服务命名服务" class="headerlink" title="1.4.1 统一服务命名服务"></a>1.4.1 统一服务命名服务</h3><p><img src="/2020/12/01/bigdata-zookeeper1-setup/1638329641759.png" alt="1638329641759"></p><h3 id="1-4-2-统一配置管理"><a href="#1-4-2-统一配置管理" class="headerlink" title="1.4.2  统一配置管理"></a>1.4.2 统一配置管理</h3><p><img src="/2020/12/01/bigdata-zookeeper1-setup/1638329675526.png" alt="1638329675526"></p><h3 id="1-4-3-统一集群管理"><a href="#1-4-3-统一集群管理" class="headerlink" title="1.4.3 统一集群管理"></a>1.4.3 统一集群管理</h3><p><img src="/2020/12/01/bigdata-zookeeper1-setup/1638329704193.png" alt="1638329704193"></p><h3 id="1-4-4-服务器节点动态上下线"><a href="#1-4-4-服务器节点动态上下线" class="headerlink" title="1.4.4  服务器节点动态上下线"></a>1.4.4 服务器节点动态上下线</h3><p><img src="/2020/12/01/bigdata-zookeeper1-setup/1638329743811.png" alt="1638329743811"></p><h3 id="1-4-5-软负载均衡"><a href="#1-4-5-软负载均衡" class="headerlink" title="1.4.5 软负载均衡"></a>1.4.5 软负载均衡</h3><p><img src="/2020/12/01/bigdata-zookeeper1-setup/1638329814579.png" alt="1638329814579"></p><h2 id="1-5-下载地址"><a href="#1-5-下载地址" class="headerlink" title="1.5 下载地址"></a>1.5 下载地址</h2><p>1）官网首页：</p><p><a target="_blank" rel="noopener" href="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a></p><p>2）下载截图</p><p><img src="/2020/12/01/bigdata-zookeeper1-setup/1638339458131.png" alt="1638339458131"></p><h1 id="2-Zookeeper安装"><a href="#2-Zookeeper安装" class="headerlink" title="2 Zookeeper安装"></a>2 Zookeeper安装</h1><h2 id="2-1-本地模式安装部署"><a href="#2-1-本地模式安装部署" class="headerlink" title="2.1 本地模式安装部署"></a>2.1 本地模式安装部署</h2><p>1）安装前准备</p><p>（1）安装Jdk</p><p>（2）拷贝Zookeeper安装包到Linux系统下</p><p>（3）解压到指定目录</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf zookeeper-3.5.7.tar.gz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）配置修改</p><p>（1）将/opt/module/zookeeper-3.5.7/conf这个路径下的zoo_sample.cfg修改为zoo.cfg；</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ <span class="token function">mv</span> zoo_sample.cfg zoo.cfg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）打开zoo.cfg文件，修改dataDir路径：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zookeeper-3.5.7<span class="token punctuation">]</span>$ vim zoo.cfg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>修改如下内容：</p><pre class="line-numbers language-bash"><code class="language-bash">dataDir<span class="token operator">=</span>/opt/module/zookeeper-3.5.7/zkData<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）在/opt/module/zookeeper-3.5.7/这个目录上创建zkData文件夹</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zookeeper-3.5.7<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> zkData<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）操作Zookeeper</p><p>（1）启动Zookeeper</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zookeeper-3.5.7<span class="token punctuation">]</span>$ bin/zkServer.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查看进程是否启动</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zookeeper-3.5.7<span class="token punctuation">]</span>$ jps
4020 Jps
4001 QuorumPeerMain<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（3）查看状态：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zookeeper-3.5.7<span class="token punctuation">]</span>$ bin/zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper-3.5.7/bin/<span class="token punctuation">..</span>/conf/zoo.cfg
Mode: standalone<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>（4）启动客户端：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zookeeper-3.5.7<span class="token punctuation">]</span>$ bin/zkCli.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（5）退出客户端：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 0<span class="token punctuation">]</span> quit<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（6）停止Zookeeper</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zookeeper-3.5.7<span class="token punctuation">]</span>$ bin/zkServer.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-2-配置参数解读"><a href="#2-2-配置参数解读" class="headerlink" title="2.2 配置参数解读"></a>2.2 配置参数解读</h2><p>Zookeeper中的配置文件zoo.cfg中参数含义解读如下：</p><p>1）tickTime =2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒</p><p>Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。</p><p>它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</p><p>2）initLimit =10：LF初始通信时限</p><p>集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。</p><p>3）syncLimit =5：LF同步通信时限</p><p>集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。</p><p>4）dataDir：数据文件目录+数据持久化路径</p><p>主要用于保存Zookeeper中的数据。</p><p>5）clientPort =2181：客户端连接端口</p><p>监听客户端连接的端口。</p><h2 id="2-3-分布式安装部署"><a href="#2-3-分布式安装部署" class="headerlink" title="2.3 分布式安装部署"></a>2.3 分布式安装部署</h2><p>1）集群规划</p><p>在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。</p><p>2）解压安装</p><p>（1）解压Zookeeper安装包到/opt/module/目录下</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf zookeeper-3.5.7.tar.gz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）同步/opt/module/zookeeper-3.5.7目录内容到hadoop103、hadoop104</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ xsync zookeeper-3.5.7/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）配置服务器编号</p><p>（1）在/opt/module/zookeeper-3.5.7/这个目录下创建zkData</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zookeeper-3.5.7<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> -p zkData<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）在/opt/module/zookeeper-3.5.7/zkData目录下创建一个myid的文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zkData<span class="token punctuation">]</span>$ <span class="token function">touch</span> myid<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码</p><p>（3）编辑myid文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zkData<span class="token punctuation">]</span>$ <span class="token function">vi</span> myid<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在文件中添加与server对应的编号：</p><pre class="line-numbers language-sh"><code class="language-sh">2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）拷贝配置好的zookeeper到其他机器上</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zkData<span class="token punctuation">]</span>$ xsync myid<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>并分别在hadoop103、hadoop104上修改myid文件中内容为3、4</p><p>4）配置zoo.cfg文件</p><p>（1）重命名/opt/module/zookeeper-3.5.7/conf这个目录下的zoo_sample.cfg为zoo.cfg</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ <span class="token function">mv</span> zoo_sample.cfg zoo.cfg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）打开zoo.cfg文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ vim zoo.cfg
<span class="token comment" spellcheck="true">#修改数据存储路径配置</span>
dataDir<span class="token operator">=</span>/opt/module/zookeeper-3.5.7/zkData
<span class="token comment" spellcheck="true">#增加如下配置</span>
<span class="token comment" spellcheck="true">#######################cluster##########################</span>
server.2<span class="token operator">=</span>hadoop102:2888:3888
server.3<span class="token operator">=</span>hadoop103:2888:3888
server.4<span class="token operator">=</span>hadoop104:2888:3888<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）同步zoo.cfg配置文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ xsync zoo.cfg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）配置参数解读</p><p>server.A=B:C:D。</p><p>A是一个数字，表示这个是第几号服务器；</p><p>集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</p><p>B是这个服务器的地址；</p><p>C是这个服务器Follower与集群中的Leader服务器交换信息的端口；</p><p>D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</p><p>5）集群操作</p><p>（1）分别启动Zookeeper</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zookeeper-3.5.7<span class="token punctuation">]</span>$ bin/zkServer.sh start
<span class="token punctuation">[</span>molly@hadoop103 zookeeper-3.5.7<span class="token punctuation">]</span>$ bin/zkServer.sh start
<span class="token punctuation">[</span>molly@hadoop104 zookeeper-3.5.7<span class="token punctuation">]</span>$ bin/zkServer.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（2）查看状态</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zookeeper-3.5.7<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># bin/zkServer.sh status</span>
JMX enabled by default
Using config: /opt/module/zookeeper-3.5.7/bin/<span class="token punctuation">..</span>/conf/zoo.cfg
Mode: follower

<span class="token punctuation">[</span>molly@hadoop103 zookeeper-3.5.7<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># bin/zkServer.sh status</span>
JMX enabled by default
Using config: /opt/module/zookeeper-3.5.7/bin/<span class="token punctuation">..</span>/conf/zoo.cfg
Mode: leader

<span class="token punctuation">[</span>molly@hadoop104 zookeeper-3.5.7<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># bin/zkServer.sh status</span>
JMX enabled by default
Using config: /opt/module/zookeeper-3.5.7/bin/<span class="token punctuation">..</span>/conf/zoo.cfg
Mode: follower<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-客户端命令行操作"><a href="#3-客户端命令行操作" class="headerlink" title="3 客户端命令行操作"></a>3 客户端命令行操作</h1><table><thead><tr><th>命令基本语法</th><th>功能描述</th></tr></thead><tbody><tr><td>help</td><td>显示所有操作命令</td></tr><tr><td>ls path</td><td>使用 ls 命令来查看当前znode的子节点 -w 监听子节点变化 -s 附加次级信息</td></tr><tr><td>create</td><td>普通创建 -s 含有序列 -e 临时（重启或者超时消失）</td></tr><tr><td>get path</td><td>获得节点的值 -w 监听节点内容变化 -s 附加次级信息</td></tr><tr><td>set</td><td>设置节点的具体值</td></tr><tr><td>stat</td><td>查看节点状态</td></tr><tr><td>delete</td><td>删除节点</td></tr><tr><td>deleteall</td><td>递归删除节点</td></tr></tbody></table><p>1）启动客户端</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop103 zookeeper-3.5.7<span class="token punctuation">]</span>$ bin/zkCli.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）显示所有操作命令</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 1<span class="token punctuation">]</span> <span class="token function">help</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）查看当前znode中所包含的内容</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 0<span class="token punctuation">]</span> <span class="token function">ls</span> /

<span class="token punctuation">[</span>zookeeper<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>4）查看当前节点详细数据</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 1<span class="token punctuation">]</span> <span class="token function">ls</span> -s /
<span class="token punctuation">[</span>zookeeper<span class="token punctuation">]</span>
cZxid <span class="token operator">=</span> 0x0
ctime <span class="token operator">=</span> Thu Jan 01 08:00:00 CST 1970
mZxid <span class="token operator">=</span> 0x0
mtime <span class="token operator">=</span> Thu Jan 01 08:00:00 CST 1970
pZxid <span class="token operator">=</span> 0x0
cversion <span class="token operator">=</span> -1
dataVersion <span class="token operator">=</span> 0
aclVersion <span class="token operator">=</span> 0
ephemeralOwner <span class="token operator">=</span> 0x0
dataLength <span class="token operator">=</span> 0
numChildren <span class="token operator">=</span> 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5）分别创建2个普通节点</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 3<span class="token punctuation">]</span> create /sanguo <span class="token string">"diaochan"</span>
Created /sanguo
<span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 4<span class="token punctuation">]</span> create /sanguo/shuguo <span class="token string">"liubei"</span>
Created /sanguo/shuguo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>6）获得节点的值</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 5<span class="token punctuation">]</span> get /sanguo
diaochan
<span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 6<span class="token punctuation">]</span> get -s /sanguo
diaochan
cZxid <span class="token operator">=</span> 0x100000003
ctime <span class="token operator">=</span> Wed Aug 29 00:03:23 CST 2018
mZxid <span class="token operator">=</span> 0x100000003
mtime <span class="token operator">=</span> Wed Aug 29 00:03:23 CST 2018
pZxid <span class="token operator">=</span> 0x100000004
cversion <span class="token operator">=</span> 1
dataVersion <span class="token operator">=</span> 0
aclVersion <span class="token operator">=</span> 0
ephemeralOwner <span class="token operator">=</span> 0x0
dataLength <span class="token operator">=</span> 7
numChildren <span class="token operator">=</span> 1
<span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 7<span class="token punctuation">]</span>
<span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 7<span class="token punctuation">]</span> get -s /sanguo/shuguo
liubei
cZxid <span class="token operator">=</span> 0x100000004
ctime <span class="token operator">=</span> Wed Aug 29 00:04:35 CST 2018
mZxid <span class="token operator">=</span> 0x100000004
mtime <span class="token operator">=</span> Wed Aug 29 00:04:35 CST 2018
pZxid <span class="token operator">=</span> 0x100000004
cversion <span class="token operator">=</span> 0
dataVersion <span class="token operator">=</span> 0
aclVersion <span class="token operator">=</span> 0
ephemeralOwner <span class="token operator">=</span> 0x0
dataLength <span class="token operator">=</span> 6
numChildren <span class="token operator">=</span> 0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>7）创建临时节点</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 7<span class="token punctuation">]</span> create -e /sanguo/wuguo <span class="token string">"zhouyu"</span>
Created /sanguo/wuguo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（1）在当前客户端是能查看到的</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 3<span class="token punctuation">]</span> <span class="token function">ls</span> /sanguo 
<span class="token punctuation">[</span>wuguo, shuguo<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（2）退出当前客户端然后再重启客户端</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 12<span class="token punctuation">]</span> quit
<span class="token punctuation">[</span>molly@hadoop104 zookeeper-3.5.7<span class="token punctuation">]</span>$ bin/zkCli.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（3）再次查看根目录下短暂节点已经删除</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 0<span class="token punctuation">]</span> <span class="token function">ls</span> /sanguo
<span class="token punctuation">[</span>shuguo<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>8）创建带序号的节点</p><p>​ （1）先创建一个普通的根节点/sanguo/weiguo</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 1<span class="token punctuation">]</span> create /sanguo/weiguo <span class="token string">"caocao"</span>
Created /sanguo/weiguo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>​ （2）创建带序号的节点</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 2<span class="token punctuation">]</span> create /sanguo/weiguo <span class="token string">"caocao"</span>
Node already exists: /sanguo/weiguo
<span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 3<span class="token punctuation">]</span> create -s /sanguo/weiguo <span class="token string">"caocao"</span>
Created /sanguo/weiguo0000000000
<span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 4<span class="token punctuation">]</span> create -s /sanguo/weiguo <span class="token string">"caocao"</span>
Created /sanguo/weiguo0000000001
<span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 5<span class="token punctuation">]</span> create -s /sanguo/weiguo <span class="token string">"caocao"</span>
Created /sanguo/weiguo0000000002
<span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 6<span class="token punctuation">]</span> <span class="token function">ls</span> /sanguo
<span class="token punctuation">[</span>shuguo, weiguo, weiguo0000000000, weiguo0000000001, weiguo0000000002, wuguo<span class="token punctuation">]</span>
<span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 6<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果节点下原来没有子节点，序号从0开始依次递增。如果原节点下已有2个节点，则再排序时从2开始，以此类推。</p><p>9）修改节点数据值</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 6<span class="token punctuation">]</span> <span class="token keyword">set</span> /sanguo/weiguo <span class="token string">"caopi"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>10）节点的值变化监听</p><p>​ （1）在hadoop104主机上注册监听/sanguo节点数据变化</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 26<span class="token punctuation">]</span> <span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 8<span class="token punctuation">]</span> get -w /sanguo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​ （2）在hadoop103主机上修改/sanguo节点的数据</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 1<span class="token punctuation">]</span> <span class="token keyword">set</span> /sanguo <span class="token string">"xishi"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​ （3）观察hadoop104主机收到数据变化的监听</p><pre class="line-numbers language-bash"><code class="language-bash">WATCHER::

WatchedEvent state:SyncConnected type:NodeDataChanged path:/sanguo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>11）节点的子节点变化监听（路径变化）</p><p>​ （1）在hadoop104主机上注册监听/sanguo节点的子节点变化</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 1<span class="token punctuation">]</span> <span class="token function">ls</span> -w /sanguo
<span class="token punctuation">[</span>aa0000000001, server101<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>​ （2）在hadoop103主机/sanguo节点上创建子节点</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 2<span class="token punctuation">]</span> create /sanguo/jin <span class="token string">"simayi"</span>
Created /sanguo/jin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>​ （3）观察hadoop104主机收到子节点变化的监听</p><pre class="line-numbers language-bash"><code class="language-bash">WATCHER::
WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/sanguo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>12）删除节点</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 4<span class="token punctuation">]</span> delete /sanguo/jin<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>13）递归删除节点</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 15<span class="token punctuation">]</span> deleteall /sanguo/shuguo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>14）查看节点状态</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>zk: localhost:2181<span class="token punctuation">(</span>CONNECTED<span class="token punctuation">)</span> 17<span class="token punctuation">]</span> <span class="token function">stat</span> /sanguo
cZxid <span class="token operator">=</span> 0x100000003
ctime <span class="token operator">=</span> Wed Aug 29 00:03:23 CST 2018
mZxid <span class="token operator">=</span> 0x100000011
mtime <span class="token operator">=</span> Wed Aug 29 00:21:23 CST 2018
pZxid <span class="token operator">=</span> 0x100000014
cversion <span class="token operator">=</span> 9
dataVersion <span class="token operator">=</span> 1
aclVersion <span class="token operator">=</span> 0
ephemeralOwner <span class="token operator">=</span> 0x0
dataLength <span class="token operator">=</span> 4
numChildren <span class="token operator">=</span> 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Zookeeper/" rel="tag">Zookeeper</a></li></ul></div><span class="post-count">总字数2.6k</span> <span class="post-count">预计阅读11分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-Spark2-framework" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/29/bigdata-Spark2-framework/" class="article-date"><time class="published" datetime="2020-11-29T09:29:54.000Z" itemprop="datePublished">2020-11-29 发布</time> <time class="updated" datetime="2021-11-30T02:12:28.930Z" itemprop="dateUpdated">2021-11-30 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/29/bigdata-Spark2-framework/">Spark学习笔记（二） 架构解析和RDD编程</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="1-Spark运行架构"><a href="#1-Spark运行架构" class="headerlink" title="1 Spark运行架构"></a>1 Spark运行架构</h1><h2 id="1-1-运行架构"><a href="#1-1-运行架构" class="headerlink" title="1.1 运行架构"></a>1.1 运行架构</h2><p>Spark框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。</p><p>如下图所示，它展示了一个 Spark执行时的基本结构。图形中的Driver表示master，负责管理整个集群中的作业任务调度。图形中的Executor 则是 slave，负责实际执行任务。</p><p><img src="/2020/11/29/bigdata-Spark2-framework/1638183145356.png" alt="1638183145356"></p><h2 id="1-2-核心组件"><a href="#1-2-核心组件" class="headerlink" title="1.2 核心组件"></a>1.2 核心组件</h2><p>由上图可以看出，对于Spark框架有两个核心组件：</p><h3 id="1-2-1-Driver"><a href="#1-2-1-Driver" class="headerlink" title="1.2.1 Driver"></a>1.2.1 Driver</h3><p>Spark驱动器节点，用于执行Spark任务中的main方法，负责实际代码的执行工作。Driver在Spark作业执行时主要负责：</p><p>Ø 将用户程序转化为作业（job）</p><p>Ø 在Executor之间调度任务(task)</p><p>Ø 跟踪Executor的执行情况</p><p>Ø 通过UI展示查询运行情况</p><p>实际上，我们无法准确地描述Driver的定义，因为在整个的编程过程中没有看到任何有关Driver的字眼。所以简单理解，所谓的Driver就是驱使整个应用运行起来的程序，也称之为Driver类。</p><h3 id="1-2-2-Executor"><a href="#1-2-2-Executor" class="headerlink" title="1.2.2 Executor"></a>1.2.2 Executor</h3><p>Spark Executor是集群中工作节点（Worker）中的一个JVM进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。</p><p>Executor有两个核心功能：</p><p>Ø 负责运行组成Spark应用的任务，并将结果返回给驱动器进程</p><p>Ø 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</p><h3 id="1-2-3-Master-amp-Worker"><a href="#1-2-3-Master-amp-Worker" class="headerlink" title="1.2.3 Master &amp; Worker"></a>1.2.3 Master &amp; Worker</h3><p>Spark集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master和Worker，这里的Master是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于Yarn环境中的RM, 而Worker呢，也是进程，一个Worker运行在集群中的一台服务器上，由Master分配资源对数据进行并行的处理和计算，类似于Yarn环境中NM。</p><h3 id="1-2-4-ApplicationMaster"><a href="#1-2-4-ApplicationMaster" class="headerlink" title="1.2.4 ApplicationMaster"></a>1.2.4 ApplicationMaster</h3><p>Hadoop用户向YARN集群提交应用程序时,提交程序中应该包含ApplicationMaster，用于向资源调度器申请执行任务的资源容器Container，运行用户自己的程序任务job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。</p><p>说的简单点就是，ResourceManager（资源）和Driver（计算）之间的解耦合靠的就是ApplicationMaster。</p><h2 id="1-3-核心概念"><a href="#1-3-核心概念" class="headerlink" title="1.3 核心概念"></a>1.3 核心概念</h2><h3 id="1-3-1-Executor与Core（核）"><a href="#1-3-1-Executor与Core（核）" class="headerlink" title="1.3.1 Executor与Core（核）"></a>1.3.1 Executor与Core（核）</h3><p>Spark Executor是集群中运行在工作节点（Worker）中的一个JVM进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点Executor的内存大小和使用的虚拟CPU核（Core）数量。</p><p>应用程序相关启动参数如下：</p><table><thead><tr><th>名称</th><th>说明</th></tr></thead><tbody><tr><td>–num-executors</td><td>配置Executor的数量</td></tr><tr><td>–executor-memory</td><td>配置每个Executor的内存大小</td></tr><tr><td>–executor-cores</td><td>配置每个Executor的虚拟CPU core数量</td></tr></tbody></table><h3 id="1-3-2-并行度（Parallelism）"><a href="#1-3-2-并行度（Parallelism）" class="headerlink" title="1.3.2 并行度（Parallelism）"></a>1.3.2 并行度（Parallelism）</h3><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们将整个集群并行执行任务的数量称之为并行度。那么一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。</p><h3 id="1-3-3-有向无环图（DAG）"><a href="#1-3-3-有向无环图（DAG）" class="headerlink" title="1.3.3 有向无环图（DAG）"></a>1.3.3 有向无环图（DAG）</h3><p><img src="/2020/11/29/bigdata-Spark2-framework/1638183179425.png" alt="1638183179425"></p><p>大数据计算引擎框架我们根据使用方式的不同一般会分为四类，其中第一类就是Hadoop所承载的MapReduce,它将计算分为两个阶段，分别为 Map阶段 和 Reduce阶段。对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算。 由于这样的弊端，催生了支持 DAG 框架的产生。因此，支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的 Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 Oozie 来说，大多还是批处理的任务。接下来就是以 Spark 为代表的第三代的计算引擎。第三代计算引擎的特点主要是 Job 内部的 DAG 支持（不跨越 Job），以及实时计算。</p><p>这里所谓的有向无环图，并不是真正意义的图形，而是由Spark程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来,这样更直观，更便于理解，可以用于表示程序的拓扑结构。</p><p>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。</p><h2 id="1-4-提交流程"><a href="#1-4-提交流程" class="headerlink" title="1.4 提交流程"></a>1.4 提交流程</h2><p>所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过Spark客户端提交给Spark运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又有细微的区别，我们这里不进行详细的比较，但是因为国内工作中，将Spark引用部署到Yarn环境中会更多一些，所以本课程中的提交流程是基于Yarn环境的。</p><p><img src="/2020/11/29/bigdata-Spark2-framework/1638183211849.png" alt="1638183211849"></p><p>Spark应用程序提交到Yarn环境中执行的时候，一般会有两种部署执行的方式：Client和Cluster。两种模式主要区别在于：Driver程序的运行节点位置。</p><h3 id="1-2-1-Yarn-Client模式"><a href="#1-2-1-Yarn-Client模式" class="headerlink" title="1.2.1 Yarn Client模式"></a>1.2.1 Yarn Client模式</h3><p>Client模式将用于监控和调度的Driver模块在客户端执行，而不是在Yarn中，所以一般用于测试。</p><p>Ø Driver在任务提交的本地机器上运行</p><p>Ø Driver启动后会和ResourceManager通讯申请启动ApplicationMaster</p><p>Ø ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，负责向ResourceManager申请Executor内存</p><p>Ø ResourceManager接到ApplicationMaster的资源申请后会分配container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程</p><p>Ø Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数</p><p>Ø 之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生成对应的TaskSet，之后将task分发到各个Executor上执行。</p><h3 id="1-2-2-Yarn-Cluster模式"><a href="#1-2-2-Yarn-Cluster模式" class="headerlink" title="1.2.2 Yarn Cluster模式"></a>1.2.2 Yarn Cluster模式</h3><p>Cluster模式将用于监控和调度的Driver模块启动在Yarn集群资源中执行。一般应用于实际生产环境。</p><p>Ø 在YARN Cluster模式下，任务提交后会和ResourceManager通讯申请启动ApplicationMaster，</p><p>Ø 随后ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver。</p><p>Ø Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配container，然后在合适的NodeManager上启动Executor进程</p><p>Ø Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数，</p><p>Ø 之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生成对应的TaskSet，之后将task分发到各个Executor上执行。</p><h1 id="2-Spark核心编程"><a href="#2-Spark核心编程" class="headerlink" title="2  Spark核心编程"></a>2 Spark核心编程</h1><p>Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p><p>Ø RDD : 弹性分布式数据集</p><p>Ø 累加器：分布式共享只写变量</p><p>Ø 广播变量：分布式共享只读变量</p><p>接下来我们一起看看这三大数据结构是如何在数据处理中使用的。</p><h2 id="2-1-RDD"><a href="#2-1-RDD" class="headerlink" title="2.1 RDD"></a>2.1 RDD</h2><h3 id="2-1-1-什么是RDD"><a href="#2-1-1-什么是RDD" class="headerlink" title="2.1.1 什么是RDD"></a>2.1.1 什么是RDD</h3><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。<strong>（所谓的RDD，其实就是要给数据结构，类似于链表中的Node，RDD中有适合并行计算的分区操作；RDD中封装了最小的计算单元，目的是更适合重复使用；Spark主要就是通过组合RDD的操作完成业务需求。）</strong></p><p>那Spark 怎么组合RDD？</p><p><strong>RDD的扩展功能采用的也是装饰者设计模式；RDD中的collect方法类似于IO中的read方法。RDD不存储任何数据，只封装逻辑。</strong></p><p><img src="/2020/11/29/bigdata-Spark2-framework/1638181914698.png" alt="1638181914698"></p><p>Ø 弹性</p><p>l 存储的弹性：内存与磁盘的自动切换；</p><p>l 容错的弹性：数据丢失可以自动恢复；</p><p>l 计算的弹性：计算出错重试机制；</p><p>l 分片的弹性：可根据需要重新分片。</p><p>Ø 分布式：数据存储在大数据集群不同节点上</p><p>Ø 数据集：<strong>RDD封装了计算逻辑，并不保存数据</strong></p><p>Ø 数据抽象：RDD是一个抽象类，需要子类具体实现</p><p>Ø 不可变：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑</p><p>Ø 可分区、并行计算</p><h3 id="2-1-2-核心属性"><a href="#2-1-2-核心属性" class="headerlink" title="2.1.2 核心属性"></a>2.1.2 核心属性</h3><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image086.jpg" alt="img"></p><p><img src="/2020/11/29/bigdata-Spark2-framework/1638182223714.png" alt="1638182223714"></p><p>1）分区列表</p><p>RDD数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。</p><p>2） 分区计算函数</p><p>Spark在计算时，是使用分区函数对每一个分区进行计算</p><p>3）RDD之间的依赖关系</p><p>RDD是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个RDD建立依赖关系</p><p>4） 分区器（可选）</p><p>当数据为KV类型数据时，可以通过设定分区器自定义数据的分区</p><p>5）首选位置（可选）</p><p>计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算</p><h3 id="2-1-3-执行原理"><a href="#2-1-3-执行原理" class="headerlink" title="2.1.3 执行原理"></a>2.1.3 执行原理</h3><p>从计算的角度来讲，数据处理过程中需要<strong>计算资源（内存 &amp; CPU）</strong>和<strong>计算模型（逻辑）</strong>。执行时，需<strong>要将计算资源和计算模型进行协调和整合</strong>。</p><p>Spark框架在执行时，先申请资源，然后将应用程序的数据处理逻辑<strong>分解成一个一个的计算任务</strong>。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。</p><p>RDD是Spark框架中用于数据处理的核心模型，接下来我们看看，在Yarn环境中，RDD的工作原理:</p><ol><li>启动Yarn集群环境</li></ol><p><img src="/2020/11/29/bigdata-Spark2-framework/1638182323787.png" alt="1638182323787"></p><ol start="2"><li>Spark通过申请资源创建调度节点和计算节点</li></ol><p><img src="/2020/11/29/bigdata-Spark2-framework/1638182335328.png" alt="1638182335328"></p><ol start="3"><li>Spark框架根据需求将计算逻辑根据分区划分成不同的任务</li></ol><p><img src="/2020/11/29/bigdata-Spark2-framework/1638182347091.png" alt="1638182347091"></p><ol start="4"><li>调度节点将任务根据计算节点状态发送到对应的计算节点进行计算</li></ol><p><img src="/2020/11/29/bigdata-Spark2-framework/1638182359929.png" alt="1638182359929"></p><p>从以上流程可以看出RDD在整个流程中主要用于将逻辑进行封装，并生成Task发送给Executor节点执行计算，接下来我们就一起看看Spark框架中RDD是具体是如何进行数据处理的。</p><h3 id="2-1-4-基础编程"><a href="#2-1-4-基础编程" class="headerlink" title="2.1.4 基础编程"></a>2.1.4 基础编程</h3><h4 id="2-1-4-1-RDD创建"><a href="#2-1-4-1-RDD创建" class="headerlink" title="2.1.4.1 RDD创建"></a>2.1.4.1 RDD创建</h4><p>在Spark中创建RDD的创建方式可以分为<strong>四种</strong>：</p><p><strong>1) 从集合（内存）中创建RDD</strong></p><p>从集合中创建RDD，<a target="_blank" rel="noopener" href="https://www.iteblog.com/archives/tag/spark/">Spark</a>主要提供了两个方法：parallelize和<strong>makeRDD(推荐使用)</strong></p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> sparkConf <span class="token operator">=</span>
    <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"spark"</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>
<span class="token keyword">val</span> rdd1 <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>
    List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
<span class="token keyword">val</span> rdd2 <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>
    List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
rdd1<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
rdd2<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>从外部存储（文件）创建RDD</li></ol><p>由外部存储系统的数据集创建RDD(<strong>textFile函数</strong>)包括：本地的文件系统，所有Hadoop支持的数据集，比如HDFS、HBase等。。</p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> sparkConf <span class="token operator">=</span>
    <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"spark"</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>
<span class="token keyword">val</span> fileRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"data/input.txt"</span><span class="token punctuation">)</span>
fileRDD<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>3) 从其他RDD创建</strong></p><p>主要是通过一个RDD运算完后，再产生新的RDD。详情请参考后续章节</p><p><strong>4) 直接创建RDD（new）</strong></p><p>使用new的方式直接构造RDD，一般由Spark框架自身使用。</p><h4 id="2-1-4-2-RDD并行度与分区"><a href="#2-1-4-2-RDD并行度与分区" class="headerlink" title="2.1.4.2 RDD并行度与分区"></a>2.1.4.2 RDD并行度与分区</h4><p>默认情况下，Spark可以将一个作业切分多个任务后，发送给Executor节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建RDD时指定。<strong>记住，这里的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。</strong></p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> sparkConf <span class="token operator">=</span>
    <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"spark"</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>
<span class="token keyword">val</span> dataRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">Int</span><span class="token punctuation">]</span> <span class="token operator">=</span>
    sparkContext<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>
        List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token number">4</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> fileRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span>
    sparkContext<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span>
        <span class="token string">"input"</span><span class="token punctuation">,</span>
        <span class="token number">2</span><span class="token punctuation">)</span>
fileRDD<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>l 读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark核心源码如下：</p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">def</span> positions<span class="token punctuation">(</span>length<span class="token operator">:</span> <span class="token builtin">Long</span><span class="token punctuation">,</span> numSlices<span class="token operator">:</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token operator">:</span> Iterator<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">Int</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
  <span class="token punctuation">(</span><span class="token number">0</span> until numSlices<span class="token punctuation">)</span><span class="token punctuation">.</span>iterator<span class="token punctuation">.</span>map <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> i <span class="token keyword">=></span>
    <span class="token keyword">val</span> start <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>i <span class="token operator">*</span> length<span class="token punctuation">)</span> <span class="token operator">/</span> numSlices<span class="token punctuation">)</span><span class="token punctuation">.</span>toInt
    <span class="token keyword">val</span> end <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> length<span class="token punctuation">)</span> <span class="token operator">/</span> numSlices<span class="token punctuation">)</span><span class="token punctuation">.</span>toInt
    <span class="token punctuation">(</span>start<span class="token punctuation">,</span> end<span class="token punctuation">)</span>
  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>l 读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异，具体Spark核心源码如下</p><pre class="line-numbers language-scala"><code class="language-scala">public InputSplit<span class="token punctuation">[</span><span class="token punctuation">]</span> getSplits<span class="token punctuation">(</span>JobConf job<span class="token punctuation">,</span> int numSplits<span class="token punctuation">)</span>
    throws IOException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    long totalSize <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>                           <span class="token comment" spellcheck="true">// compute total size</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span>FileStatus file<span class="token operator">:</span> files<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// check we have valid files</span>
      <span class="token keyword">if</span> <span class="token punctuation">(</span>file<span class="token punctuation">.</span>isDirectory<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token keyword">throw</span> <span class="token keyword">new</span> IOException<span class="token punctuation">(</span><span class="token string">"Not a file: "</span><span class="token operator">+</span> file<span class="token punctuation">.</span>getPath<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
      <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
      totalSize <span class="token operator">+=</span> file<span class="token punctuation">.</span>getLen<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    long goalSize <span class="token operator">=</span> totalSize <span class="token operator">/</span> <span class="token punctuation">(</span>numSplits <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">?</span> <span class="token number">1</span> <span class="token operator">:</span> numSplits<span class="token punctuation">)</span><span class="token punctuation">;</span>
    long minSize <span class="token operator">=</span> Math<span class="token punctuation">.</span>max<span class="token punctuation">(</span>job<span class="token punctuation">.</span>getLong<span class="token punctuation">(</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>input<span class="token punctuation">.</span>
      FileInputFormat<span class="token punctuation">.</span>SPLIT_MINSIZE<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> minSplitSize<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

    <span class="token keyword">for</span> <span class="token punctuation">(</span>FileStatus file<span class="token operator">:</span> files<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

    <span class="token keyword">if</span> <span class="token punctuation">(</span>isSplitable<span class="token punctuation">(</span>fs<span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
          long blockSize <span class="token operator">=</span> file<span class="token punctuation">.</span>getBlockSize<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
          long splitSize <span class="token operator">=</span> computeSplitSize<span class="token punctuation">(</span>goalSize<span class="token punctuation">,</span> minSize<span class="token punctuation">,</span> blockSize<span class="token punctuation">)</span><span class="token punctuation">;</span>

          <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
  <span class="token keyword">protected</span> long computeSplitSize<span class="token punctuation">(</span>long goalSize<span class="token punctuation">,</span> long minSize<span class="token punctuation">,</span>
                                       long blockSize<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> Math<span class="token punctuation">.</span>max<span class="token punctuation">(</span>minSize<span class="token punctuation">,</span> Math<span class="token punctuation">.</span>min<span class="token punctuation">(</span>goalSize<span class="token punctuation">,</span> blockSize<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-4-3-RDD转换算子"><a href="#2-1-4-3-RDD转换算子" class="headerlink" title="2.1.4.3 RDD转换算子"></a>2.1.4.3 RDD转换算子</h4><p>RDD根据数据处理方式的不同将算子整体上分为Value类型、双Value类型和Key-Value类型</p><p><strong>l Value类型</strong></p><h5 id="1-map"><a href="#1-map" class="headerlink" title="1)   map"></a>1) map</h5><p>Ø 函数签名</p><p>def map[U: ClassTag](f: T =&gt; U): RDD[U]</p><p>Ø 函数说明</p><p>将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。</p><p>val dataRDD: RDD[Int] = sparkContext.makeRDD(List(1,2,3,4))</p><p>val dataRDD1: RDD[Int] = dataRDD.map(</p><p>num =&gt; {</p><p>​ num * 2</p><p>}</p><p>)</p><p>val dataRDD2: RDD[String] = dataRDD1.map(</p><p>num =&gt; {</p><p>​ “” + num</p><p>}</p><p>)</p><p>v 小功能：从服务器日志数据apache.log中获取用户请求URL资源路径</p><h5 id="2-mapPartitions"><a href="#2-mapPartitions" class="headerlink" title="2)   mapPartitions"></a>2) mapPartitions</h5><p>Ø 函数签名</p><p>def mapPartitions[U: ClassTag](</p><p>f: Iterator[T] =&gt; Iterator[U],</p><p>preservesPartitioning: Boolean = false): RDD[U]</p><p>Ø 函数说明</p><p>将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。</p><p>val dataRDD1: RDD[Int] = dataRDD.mapPartitions(</p><p>datas =&gt; {</p><p>​ datas.filter(_==2)</p><p>}</p><p>)</p><p>v 小功能：获取每个数据分区的最大值</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image106.jpg" alt="img"> 思考一个问题：map和mapPartitions的区别？</p><p>Ø 数据处理角度</p><p>Map算子是分区内一个数据一个数据的执行，类似于串行操作。而mapPartitions算子是以分区为单位进行批处理操作。</p><p>Ø 功能的角度</p><p>Map算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。MapPartitions算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据</p><p>Ø 性能的角度</p><p>Map算子因为类似于串行操作，所以性能比较低，而是mapPartitions算子类似于批处理，所以性能较高。但是mapPartitions算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用map操作。</p><p>完成比完美更重要</p><h5 id="3-mapPartitionsWithIndex"><a href="#3-mapPartitionsWithIndex" class="headerlink" title="3)   mapPartitionsWithIndex"></a>3) mapPartitionsWithIndex</h5><p>Ø 函数签名</p><p>def mapPartitionsWithIndex[U: ClassTag](</p><p>f: (Int, Iterator[T]) =&gt; Iterator[U],</p><p>preservesPartitioning: Boolean = false): RDD[U]</p><p>Ø 函数说明</p><p>将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。</p><p>val dataRDD1 = dataRDD.mapPartitionsWithIndex(</p><p>(index, datas) =&gt; {</p><p>​ datas.map(index, _)</p><p>}</p><p>)</p><p>v 小功能：获取第二个数据分区的数据</p><h5 id="4-flatMap"><a href="#4-flatMap" class="headerlink" title="4)   flatMap"></a>4) flatMap</h5><p>Ø 函数签名</p><p>def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U]</p><p>Ø 函数说明</p><p>将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>List(1,2),List(3,4)</p><p>),1)</p><p>val dataRDD1 = dataRDD.flatMap(</p><p>list =&gt; list</p><p>)</p><p>v 小功能：将List(List(1,2),3,List(4,5))进行扁平化操作</p><h5 id="5-glom"><a href="#5-glom" class="headerlink" title="5)   glom"></a>5) glom</h5><p>Ø 函数签名</p><p>def glom(): RDD[Array[T]]</p><p>Ø 函数说明</p><p>将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4</p><p>),1)</p><p>val dataRDD1:RDD[Array[Int]] = dataRDD.glom()</p><p>v 小功能：计算所有分区最大值求和（分区内取最大值，分区间最大值求和）</p><h5 id="6-groupBy"><a href="#6-groupBy" class="headerlink" title="6)   groupBy"></a>6) groupBy</h5><p>Ø 函数签名</p><p>def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])]</p><p>Ø 函数说明</p><p>将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为shuffle。极限情况下，数据可能被分在同一个分区中</p><p>一个组的数据在一个分区中，但是并不是说一个分区中只有一个组</p><p>val dataRDD = sparkContext.makeRDD(List(1,2,3,4),1)</p><p>val dataRDD1 = dataRDD.groupBy(</p><p>_%2</p><p>)</p><p>v 小功能：将List(“Hello”, “hive”, “hbase”, “Hadoop”)根据单词首写字母进行分组。</p><p>v 小功能：从服务器日志数据apache.log中获取每个时间段访问量。</p><p>v 小功能：WordCount。</p><h5 id="7-filter"><a href="#7-filter" class="headerlink" title="7)   filter"></a>7) filter</h5><p>Ø 函数签名</p><p>def filter(f: T =&gt; Boolean): RDD[T]</p><p>Ø 函数说明</p><p>将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。</p><p>当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜。</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4</p><p>),1)</p><p>val dataRDD1 = dataRDD.filter(_%2 == 0)</p><p>v 小功能：从服务器日志数据apache.log中获取2015年5月17日的请求路径</p><h5 id="8-sample"><a href="#8-sample" class="headerlink" title="8)   sample"></a>8) sample</h5><p>Ø 函数签名</p><p>def sample(</p><p>withReplacement: Boolean,</p><p>fraction: Double,</p><p>seed: Long = Utils.random.nextLong): RDD[T]</p><p>Ø 函数说明</p><p>根据指定的规则从数据集中抽取数据</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4</p><p>),1)</p><p>// 抽取数据不放回（伯努利算法）</p><p>// 伯努利算法：又叫0、1分布。例如扔硬币，要么正面，要么反面。</p><p>// 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不要</p><p>// 第一个参数：抽取的数据是否放回，false：不放回</p><p>// 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；</p><p>// 第三个参数：随机数种子</p><p>val dataRDD1 = dataRDD.sample(false, 0.5)</p><p>// 抽取数据放回（泊松算法）</p><p>// 第一个参数：抽取的数据是否放回，true：放回；false：不放回</p><p>// 第二个参数：重复数据的几率，范围大于等于0.表示每一个元素被期望抽取到的次数</p><p>// 第三个参数：随机数种子</p><p>val dataRDD2 = dataRDD.sample(true, 2)</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image107.jpg" alt="img">思考一个问题：有啥用，抽奖吗？</p><h5 id="9-distinct"><a href="#9-distinct" class="headerlink" title="9)   distinct"></a>9) distinct</h5><p>Ø 函数签名</p><p>def distinct()(implicit ord: Ordering[T] = null): RDD[T]</p><p>def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]</p><p>Ø 函数说明</p><p>将数据集中重复的数据去重</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4,1,2</p><p>),1)</p><p>val dataRDD1 = dataRDD.distinct()</p><p>val dataRDD2 = dataRDD.distinct(2)</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image108.jpg" alt="img">思考一个问题：如果不用该算子，你有什么办法实现数据去重？</p><h5 id="10-coalesce"><a href="#10-coalesce" class="headerlink" title="10)  coalesce"></a>10) coalesce</h5><p>Ø 函数签名</p><p>def coalesce(numPartitions: Int, shuffle: Boolean = false,</p><p>​ partitionCoalescer: Option[PartitionCoalescer] = Option.empty)</p><p>​ (implicit ord: Ordering[T] = null)</p><p>: RDD[T]</p><p>Ø 函数说明</p><p>根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率</p><p>当spark程序中，存在过多的小任务的时候，可以通过coalesce方法，收缩合并分区，减少分区的个数，减小任务调度成本</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4,1,2</p><p>),6)</p><p>val dataRDD1 = dataRDD.coalesce(2)</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg" alt="img">思考一个问题：我想要扩大分区，怎么办？</p><h5 id="11-repartition"><a href="#11-repartition" class="headerlink" title="11)  repartition"></a>11) repartition</h5><p>Ø 函数签名</p><p>def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]</p><p>Ø 函数说明</p><p>该操作内部其实执行的是coalesce操作，参数shuffle的默认值为true。无论是将分区数多的RDD转换为分区数少的RDD，还是将分区数少的RDD转换为分区数多的RDD，repartition操作都可以完成，因为无论如何都会经shuffle过程。</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4,1,2</p><p>),2)</p><p>val dataRDD1 = dataRDD.repartition(4)</p><p>思考一个问题：coalesce和repartition区别？</p><h5 id="12-sortBy"><a href="#12-sortBy" class="headerlink" title="12)  sortBy"></a>12) sortBy</h5><p>Ø 函数签名</p><p>def sortBy[K](</p><p>f: (T) =&gt; K,</p><p>ascending: Boolean = true,</p><p>numPartitions: Int = this.partitions.length)</p><p>(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]</p><p>Ø 函数说明</p><p>该操作用于排序数据。在排序之前，可以将数据通过f函数进行处理，之后按照f函数处理的结果进行排序，默认为升序排列。排序后新产生的RDD的分区数与原RDD的分区数一致。中间存在shuffle的过程</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4,1,2</p><p>),2)</p><p>val dataRDD1 = dataRDD.sortBy(num=&gt;num, false, 4)</p><p>l 双Value类型</p><h5 id="13-intersection"><a href="#13-intersection" class="headerlink" title="13)  intersection"></a>13) intersection</h5><p>Ø 函数签名</p><p>def intersection(other: RDD[T]): RDD[T]</p><p>Ø 函数说明</p><p>对源RDD和参数RDD求交集后返回一个新的RDD</p><p>val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4))</p><p>val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6))</p><p>val dataRDD = dataRDD1.intersection(dataRDD2)</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image110.jpg" alt="img">思考一个问题：如果两个RDD数据类型不一致怎么办？</p><h5 id="14-union"><a href="#14-union" class="headerlink" title="14)  union"></a>14) union</h5><p>Ø 函数签名</p><p>def union(other: RDD[T]): RDD[T]</p><p>Ø 函数说明</p><p>对源RDD和参数RDD求并集后返回一个新的RDD</p><p>val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4))</p><p>val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6))</p><p>val dataRDD = dataRDD1.union(dataRDD2)</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg" alt="img">思考一个问题：如果两个RDD数据类型不一致怎么办？</p><h5 id="15-subtract"><a href="#15-subtract" class="headerlink" title="15)  subtract"></a>15) subtract</h5><p>Ø 函数签名</p><p>def subtract(other: RDD[T]): RDD[T]</p><p>Ø 函数说明</p><p>以一个RDD元素为主，去除两个RDD中重复元素，将其他元素保留下来。求差集</p><p>val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4))</p><p>val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6))</p><p>val dataRDD = dataRDD1.subtract(dataRDD2)</p><p>思考一个问题：如果两个RDD数据类型不一致怎么办？</p><h5 id="16-zip"><a href="#16-zip" class="headerlink" title="16)  zip"></a>16) zip</h5><p>Ø 函数签名</p><p>def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]</p><p>Ø 函数说明</p><p>将两个RDD中的元素，以键值对的形式进行合并。其中，键值对中的Key为第1个RDD中的元素，Value为第2个RDD中的相同位置的元素。</p><p>val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4))</p><p>val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6))</p><p>val dataRDD = dataRDD1.zip(dataRDD2)</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg" alt="img">思考一个问题：如果两个RDD数据类型不一致怎么办？</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg" alt="img">思考一个问题：如果两个RDD数据分区不一致怎么办？</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg" alt="img">思考一个问题：如果两个RDD分区数据数量不一致怎么办？</p><p>l Key - Value类型</p><h5 id="17-partitionBy"><a href="#17-partitionBy" class="headerlink" title="17)  partitionBy"></a>17) partitionBy</h5><p>Ø 函数签名</p><p>def partitionBy(partitioner: Partitioner): RDD[(K, V)]</p><p>Ø 函数说明</p><p>将数据按照指定Partitioner重新进行分区。Spark默认的分区器是HashPartitioner</p><p>val rdd: RDD[(Int, String)] =</p><p>sc.makeRDD(Array((1,”aaa”),(2,”bbb”),(3,”ccc”)),3)</p><p>import org.apache.spark.HashPartitioner</p><p>val rdd2: RDD[(Int, String)] =</p><p>rdd.partitionBy(new HashPartitioner(2))</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg" alt="img">思考一个问题：如果重分区的分区器和当前RDD的分区器一样怎么办？</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg" alt="img">思考一个问题：Spark还有其他分区器吗？</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg" alt="img">思考一个问题：如果想按照自己的方法进行数据分区怎么办？</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg" alt="img">思考一个问题：哪那么多问题？</p><h5 id="18-reduceByKey"><a href="#18-reduceByKey" class="headerlink" title="18)  reduceByKey"></a>18) reduceByKey</h5><p>Ø 函数签名</p><p>def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]</p><p>def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]</p><p>Ø 函数说明</p><p>可以将数据按照相同的Key对Value进行聚合</p><p>val dataRDD1 = sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val dataRDD2 = dataRDD1.reduceByKey(<em>+</em>)</p><p>val dataRDD3 = dataRDD1.reduceByKey(<em>+</em>, 2)</p><p>v 小功能：WordCount</p><h5 id="19-groupByKey"><a href="#19-groupByKey" class="headerlink" title="19)  groupByKey"></a>19) groupByKey</h5><p>Ø 函数签名</p><p>def groupByKey(): RDD[(K, Iterable[V])]</p><p>def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]</p><p>def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]</p><p>Ø 函数说明</p><p>将数据源的数据根据key对value进行分组</p><p>val dataRDD1 =</p><p>sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val dataRDD2 = dataRDD1.groupByKey()</p><p>val dataRDD3 = dataRDD1.groupByKey(2)</p><p>val dataRDD4 = dataRDD1.groupByKey(new HashPartitioner(2))</p><p>考一个问题：reduceByKey和groupByKey的区别？</p><p>从shuffle的角度：reduceByKey和groupByKey都存在shuffle的操作，但是reduceByKey可以在shuffle前对分区内相同key的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而groupByKey只是进行分组，不存在数据量减少的问题，reduceByKey性能比较高。</p><p>从功能的角度：reduceByKey其实包含分组和聚合的功能。groupByKey只能分组，不能聚合，所以在分组聚合的场合下，推荐使用reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用groupByKey</p><p>v 小功能：WordCount</p><h5 id="20-aggregateByKey"><a href="#20-aggregateByKey" class="headerlink" title="20)  aggregateByKey"></a>20) aggregateByKey</h5><p>Ø 函数签名</p><p>def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) =&gt; U,</p><p>combOp: (U, U) =&gt; U): RDD[(K, U)]</p><p>Ø 函数说明</p><p>将数据根据不同的规则进行分区内计算和分区间计算</p><p>val dataRDD1 =</p><p>sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val dataRDD2 =</p><p>dataRDD1.aggregateByKey(0)(<em>+</em>,<em>+</em>)</p><p>v 取出每个分区内相同key的最大值然后分区间相加</p><p>// TODO : 取出每个分区内相同key的最大值然后分区间相加</p><p>// aggregateByKey算子是函数柯里化，存在两个参数列表</p><p>// 1. 第一个参数列表中的参数表示初始值</p><p>// 2. 第二个参数列表中含有两个参数</p><p>// 2.1 第一个参数表示分区内的计算规则</p><p>// 2.2 第二个参数表示分区间的计算规则</p><p>val rdd =</p><p>sc.makeRDD(List(</p><p>​ (“a”,1),(“a”,2),(“c”,3),</p><p>​ (“b”,4),(“c”,5),(“c”,6)</p><p>),2)</p><p>// 0:(“a”,1),(“a”,2),(“c”,3) =&gt; (a,10)(c,10)</p><p>// =&gt; (a,10)(b,10)(c,20)</p><p>// 1:(“b”,4),(“c”,5),(“c”,6) =&gt; (b,10)(c,10)</p><p>val resultRDD =</p><p>rdd.aggregateByKey(10)(</p><p>​ (x, y) =&gt; math.max(x,y),</p><p>​ (x, y) =&gt; x + y</p><p>)</p><p>resultRDD.collect().foreach(println)</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg" alt="img">思考一个问题：分区内计算规则和分区间计算规则相同怎么办？（WordCount）</p><h5 id="21-foldByKey"><a href="#21-foldByKey" class="headerlink" title="21)  foldByKey"></a>21) foldByKey</h5><p>Ø 函数签名</p><p>def foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]</p><p>Ø 函数说明</p><p>当分区内计算规则和分区间计算规则相同时，aggregateByKey就可以简化为foldByKey</p><p>val dataRDD1 = sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val dataRDD2 = dataRDD1.foldByKey(0)(<em>+</em>)</p><h5 id="22-combineByKey"><a href="#22-combineByKey" class="headerlink" title="22)  combineByKey"></a>22) combineByKey</h5><p>Ø 函数签名</p><p>def combineByKey[C](</p><p>createCombiner: V =&gt; C,</p><p>mergeValue: (C, V) =&gt; C,</p><p>mergeCombiners: (C, C) =&gt; C): RDD[(K, C)]</p><p>Ø 函数说明</p><p>最通用的对key-value型rdd进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致。</p><p>小练习：将数据List((“a”, 88), (“b”, 95), (“a”, 91), (“b”, 93), (“a”, 95), (“b”, 98))求每个key的平均值</p><p>val list: List[(String, Int)] = List((“a”, 88), (“b”, 95), (“a”, 91), (“b”, 93), (“a”, 95), (“b”, 98))</p><p>val input: RDD[(String, Int)] = sc.makeRDD(list, 2)</p><p>val combineRdd: RDD[(String, (Int, Int))] = input.combineByKey(</p><p>(_, 1),</p><p>(acc: (Int, Int), v) =&gt; (acc._1 + v, acc._2 + 1),</p><p>(acc1: (Int, Int), acc2: (Int, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</p><p>)</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image112.jpg" alt="img">思考一个问题：reduceByKey、foldByKey、aggregateByKey、combineByKey的区别？</p><p>reduceByKey: 相同key的第一个数据不进行任何计算，分区内和分区间计算规则相同</p><p>foldByKey: 相同key的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同</p><p>aggregateByKey：相同key的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同</p><p>combineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同。</p><h5 id="23-sortByKey"><a href="#23-sortByKey" class="headerlink" title="23)  sortByKey"></a>23) sortByKey</h5><p>Ø 函数签名</p><p>def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)</p><p>: RDD[(K, V)]</p><p>Ø 函数说明</p><p>在一个(K,V)的RDD上调用，K必须实现Ordered接口(特质)，返回一个按照key进行排序的</p><p>val dataRDD1 = sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(true)</p><p>val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(false)</p><p>v 小功能：设置key为自定义类User</p><h5 id="24-join"><a href="#24-join" class="headerlink" title="24)  join"></a>24) join</h5><p>Ø 函数签名</p><p>def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]</p><p>Ø 函数说明</p><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素连接在一起的(K,(V,W))的RDD</p><p>val rdd: RDD[(Int, String)] = sc.makeRDD(Array((1, “a”), (2, “b”), (3, “c”)))</p><p>val rdd1: RDD[(Int, Int)] = sc.makeRDD(Array((1, 4), (2, 5), (3, 6)))</p><p>rdd.join(rdd1).collect().foreach(println)</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg" alt="img">思考一个问题：如果key存在不相等呢？</p><h5 id="25-leftOuterJoin"><a href="#25-leftOuterJoin" class="headerlink" title="25)  leftOuterJoin"></a>25) leftOuterJoin</h5><p>Ø 函数签名</p><p>def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]</p><p>Ø 函数说明</p><p>类似于SQL语句的左外连接</p><p>val dataRDD1 = sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val dataRDD2 = sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val rdd: RDD[(String, (Int, Option[Int]))] = dataRDD1.leftOuterJoin(dataRDD2)</p><h5 id="26-cogroup"><a href="#26-cogroup" class="headerlink" title="26)  cogroup"></a>26) cogroup</h5><p>Ø 函数签名</p><p>def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]</p><p>Ø 函数说明</p><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<v>,Iterable<w>))类型的RDD</w></v></p><p>val dataRDD1 = sparkContext.makeRDD(List((“a”,1),(“a”,2),(“c”,3)))</p><p>val dataRDD2 = sparkContext.makeRDD(List((“a”,1),(“c”,2),(“c”,3)))</p><p>val value: RDD[(String, (Iterable[Int], Iterable[Int]))] =</p><p>dataRDD1.cogroup(dataRDD2)</p><h4 id="2-1-4-4-案例实操"><a href="#2-1-4-4-案例实操" class="headerlink" title="2.1.4.4 案例实操"></a>2.1.4.4 案例实操</h4><ol><li>数据准备</li></ol><p>agent.log：时间戳，省份，城市，用户，广告，中间字段使用空格分隔。</p><ol start="2"><li>需求描述</li></ol><p>统计出每一个省份每个广告被点击数量排行的Top3</p><ol start="3"><li><p>需求分析</p></li><li><p>功能实现</p></li></ol><h4 id="2-1-4-5-RDD行动算子"><a href="#2-1-4-5-RDD行动算子" class="headerlink" title="2.1.4.5 RDD行动算子"></a>2.1.4.5 RDD行动算子</h4><h5 id="1-reduce"><a href="#1-reduce" class="headerlink" title="1)   reduce"></a>1) reduce</h5><p>Ø 函数签名</p><p>def reduce(f: (T, T) =&gt; T): T</p><p>Ø 函数说明</p><p><strong>聚集RDD中的所有元素，先聚合分区内数据，再聚合分区间数据</strong></p><p>val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))</p><p>// 聚合数据</p><p>val reduceResult: Int = rdd.reduce(<em>+</em>)</p><h5 id="2-collect"><a href="#2-collect" class="headerlink" title="2)   collect"></a>2) collect</h5><p>Ø 函数签名</p><p>def collect(): Array[T]</p><p>Ø 函数说明</p><p>在驱动程序（Driver）中，以数组Array的形式返回数据集的所有元素</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))</p><p>// 收集数据到Driver</p><p>rdd.collect().foreach(println)</p><h5 id="3-count"><a href="#3-count" class="headerlink" title="3)   count"></a>3) count</h5><p>Ø 函数签名</p><p>def count(): Long</p><p>Ø 函数说明</p><p>返回RDD中元素的个数</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))</p><p>// 返回RDD中元素的个数</p><p>val countResult: Long = rdd.count()</p><h5 id="4-first"><a href="#4-first" class="headerlink" title="4)   first"></a>4) first</h5><p>Ø 函数签名</p><p>def first(): T</p><p>Ø 函数说明</p><p>返回RDD中的第一个元素</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))</p><p>// 返回RDD中元素的个数</p><p>val firstResult: Int = rdd.first()</p><p>println(firstResult)</p><h5 id="5-take"><a href="#5-take" class="headerlink" title="5)   take"></a>5) take</h5><p>Ø 函数签名</p><p>def take(num: Int): Array[T]</p><p>Ø 函数说明</p><p>返回一个由RDD的前n个元素组成的数组</p><p>vval rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))</p><p>// 返回RDD中元素的个数</p><p>val takeResult: Array[Int] = rdd.take(2)</p><p>println(takeResult.mkString(“,”))</p><h5 id="6-takeOrdered"><a href="#6-takeOrdered" class="headerlink" title="6)   takeOrdered"></a>6) takeOrdered</h5><p>Ø 函数签名</p><p>def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]</p><p>Ø 函数说明</p><p>返回该RDD排序后的前n个元素组成的数组</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1,3,2,4))</p><p>// 返回RDD中元素的个数</p><p>val result: Array[Int] = rdd.takeOrdered(2)</p><h5 id="7-aggregate"><a href="#7-aggregate" class="headerlink" title="7)   aggregate"></a>7) aggregate</h5><p>Ø 函数签名</p><p>def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U</p><p>Ø 函数说明</p><p>分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4), 8)</p><p>// 将该RDD所有元素相加得到结果</p><p>//val result: Int = rdd.aggregate(0)(_ + _, _ + _)</p><p>val result: Int = rdd.aggregate(10)(_ + _, _ + _)</p><h5 id="8-fold"><a href="#8-fold" class="headerlink" title="8)   fold"></a>8) fold</h5><p>Ø 函数签名</p><p>def fold(zeroValue: T)(op: (T, T) =&gt; T): T</p><p>Ø 函数说明</p><p>折叠操作，aggregate的简化版操作</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))</p><p>val foldResult: Int = rdd.fold(0)(<em>+</em>)</p><h5 id="9-countByKey"><a href="#9-countByKey" class="headerlink" title="9)   countByKey"></a>9) countByKey</h5><p>Ø 函数签名</p><p>def countByKey(): Map[K, Long]</p><p>Ø 函数说明</p><p>统计每种key的个数</p><p>val rdd: RDD[(Int, String)] = sc.makeRDD(List((1, “a”), (1, “a”), (1, “a”), (2, “b”), (3, “c”), (3, “c”)))</p><p>// 统计每种key的个数</p><p>val result: collection.Map[Int, Long] = rdd.countByKey()</p><h5 id="10-save相关算子"><a href="#10-save相关算子" class="headerlink" title="10)  save相关算子"></a>10) save相关算子</h5><p>Ø 函数签名</p><p>def saveAsTextFile(path: String): Unit</p><p>def saveAsObjectFile(path: String): Unit</p><p>def saveAsSequenceFile(</p><p>path: String,</p><p>codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit</p><p>Ø 函数说明</p><p>将数据保存到不同格式的文件中</p><p>// 保存成Text文件</p><p>rdd.saveAsTextFile(“output”)</p><p>// 序列化成对象保存到文件</p><p>rdd.saveAsObjectFile(“output1”)</p><p>// 保存成Sequencefile文件</p><p>rdd.map((_,1)).saveAsSequenceFile(“output2”)</p><h5 id="11-foreach"><a href="#11-foreach" class="headerlink" title="11)  foreach"></a>11) foreach</h5><p>Ø 函数签名</p><p>def foreach(f: T =&gt; Unit): Unit = withScope {</p><p>val cleanF = sc.clean(f)</p><p>sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF))</p><p>}</p><p>Ø 函数说明</p><p>分布式遍历RDD中的每一个元素，调用指定函数</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))</p><p>// 收集后打印</p><p>rdd.map(num=&gt;num).collect().foreach(println)</p><p>println(“<strong><strong><strong>****</strong></strong></strong>“)</p><p>// 分布式打印</p><p>rdd.foreach(println)</p><h4 id="2-1-4-6-RDD序列化"><a href="#2-1-4-6-RDD序列化" class="headerlink" title="2.1.4.6 RDD序列化"></a>2.1.4.6 RDD序列化</h4><ol><li>闭包检查</li></ol><p>从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行。那么在scala的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12版本后闭包编译方式发生了改变</p><ol start="2"><li>序列化方法和属性</li></ol><p>从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行，看如下代码：</p><p>object serializable02_function {</p><p>def main(args: Array[String]): Unit = {</p><p>​ //1.创建SparkConf并设置App名称</p><p>​ val conf: SparkConf = new SparkConf().setAppName(“SparkCoreTest”).setMaster(“local[*]”)</p><p>​ //2.创建SparkContext，该对象是提交Spark App的入口</p><p>​ val sc: SparkContext = new SparkContext(conf)</p><p>​ //3.创建一个RDD</p><p>​ val rdd: RDD[String] = sc.makeRDD(Array(“hello world”, “hello spark”, “hive”, “atguigu”))</p><p>​ //3.1创建一个Search对象</p><p>​ val search = new Search(“hello”)</p><p>​ //3.2 函数传递，打印：ERROR Task not serializable</p><p>​ search.getMatch1(rdd).collect().foreach(println)</p><p>​ //3.3 属性传递，打印：ERROR Task not serializable</p><p>​ search.getMatch2(rdd).collect().foreach(println)</p><p>​ //4.关闭连接</p><p>​ sc.stop()</p><p>}</p><p>}</p><p>class Search(query:String) extends Serializable {</p><p>def isMatch(s: String): Boolean = {</p><p>​ s.contains(query)</p><p>}</p><p>// 函数序列化案例</p><p>def getMatch1 (rdd: RDD[String]): RDD[String] = {</p><p>​ //rdd.filter(this.isMatch)</p><p>​ rdd.filter(isMatch)</p><p>}</p><p>// 属性序列化案例</p><p>def getMatch2(rdd: RDD[String]): RDD[String] = {</p><p>​ //rdd.filter(x =&gt; x.contains(this.query))</p><p>​ rdd.filter(x =&gt; x.contains(query))</p><p>​ //val q = query</p><p>​ //rdd.filter(x =&gt; x.contains(q))</p><p>}</p><p>}</p><ol start="3"><li>Kryo序列化框架</li></ol><p>参考地址: <a target="_blank" rel="noopener" href="https://github.com/EsotericSoftware/kryo">https://github.com/EsotericSoftware/kryo</a></p><p>Java的序列化能够序列化任何的类。但是比较重（字节多），序列化后，对象的提交也比较大。Spark出于性能的考虑，Spark2.0开始支持另外一种Kryo序列化机制。Kryo速度是Serializable的10倍。当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kryo来序列化。</p><p>注意：即使使用Kryo序列化，也要继承Serializable接口。</p><p>object serializable_Kryo {</p><p>def main(args: Array[String]): Unit = {</p><p>​ val conf: SparkConf = new SparkConf()</p><p>​ .setAppName(“SerDemo”)</p><p>​ .setMaster(“local[*]”)</p><p>​ // 替换默认的序列化机制</p><p>​ .set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”)</p><p>​ // 注册需要使用 kryo 序列化的自定义类</p><p>​ .registerKryoClasses(Array(classOf[Searcher]))</p><p>​ val sc = new SparkContext(conf)</p><p>​ val rdd: RDD[String] = sc.makeRDD(Array(“hello world”, “hello atguigu”, “atguigu”, “hahah”), 2)</p><p>​ val searcher = new Searcher(“hello”)</p><p>​ val result: RDD[String] = searcher.getMatchedRDD1(rdd)</p><p>​ result.collect.foreach(println)</p><p>}</p><p>}</p><p>case class Searcher(val query: String) {</p><p>def isMatch(s: String) = {</p><p>​ s.contains(query)</p><p>}</p><p>def getMatchedRDD1(rdd: RDD[String]) = {</p><p>​ rdd.filter(isMatch)</p><p>}</p><p>def getMatchedRDD2(rdd: RDD[String]) = {</p><p>​ val q = query</p><p>​ rdd.filter(_.contains(q))</p><p>}</p><p>}</p><h4 id="2-1-4-7-RDD依赖关系"><a href="#2-1-4-7-RDD依赖关系" class="headerlink" title="2.1.4.7 RDD依赖关系"></a>2.1.4.7 RDD依赖关系</h4><ol><li>RDD 血缘关系</li></ol><p>RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p><p>val fileRDD: RDD[String] = sc.textFile(“input/1.txt”)</p><p>println(fileRDD.toDebugString)</p><p>println(“———————-“)</p><p>val wordRDD: RDD[String] = fileRDD.flatMap(_.split(“ “))</p><p>println(wordRDD.toDebugString)</p><p>println(“———————-“)</p><p>val mapRDD: RDD[(String, Int)] = wordRDD.map((_,1))</p><p>println(mapRDD.toDebugString)</p><p>println(“———————-“)</p><p>val resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(<em>+</em>)</p><p>println(resultRDD.toDebugString)</p><p>resultRDD.collect()</p><ol start="2"><li>RDD 依赖关系</li></ol><p>这里所谓的依赖关系，其实就是两个相邻RDD之间的关系</p><p>val sc: SparkContext = new SparkContext(conf)</p><p>val fileRDD: RDD[String] = sc.textFile(“input/1.txt”)</p><p>println(fileRDD.dependencies)</p><p>println(“———————-“)</p><p>val wordRDD: RDD[String] = fileRDD.flatMap(_.split(“ “))</p><p>println(wordRDD.dependencies)</p><p>println(“———————-“)</p><p>val mapRDD: RDD[(String, Int)] = wordRDD.map((_,1))</p><p>println(mapRDD.dependencies)</p><p>println(“———————-“)</p><p>val resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(<em>+</em>)</p><p>println(resultRDD.dependencies)</p><p>resultRDD.collect()</p><ol start="3"><li>RDD 窄依赖</li></ol><p>窄依赖表示每一个父(上游)RDD的Partition最多被子（下游）RDD的一个Partition使用，窄依赖我们形象的比喻为独生子女。</p><p>class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency<a href="rdd">T</a></p><ol start="4"><li>RDD 宽依赖</li></ol><p>宽依赖表示同一个父（上游）RDD的Partition被多个子（下游）RDD的Partition依赖，会引起Shuffle，总结：宽依赖我们形象的比喻为多生。</p><p>class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](</p><p>@transient private val <em>rdd: RDD[</em> &lt;: Product2[K, V]],</p><p>val partitioner: Partitioner,</p><p>val serializer: Serializer = SparkEnv.get.serializer,</p><p>val keyOrdering: Option[Ordering[K]] = None,</p><p>val aggregator: Option[Aggregator[K, V, C]] = None,</p><p>val mapSideCombine: Boolean = false)</p><p>extends Dependency[Product2[K, V]]</p><ol start="5"><li>RDD 阶段划分</li></ol><p>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。例如，DAG记录了RDD的转换过程和任务的阶段。</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image113.jpg" alt="img"> <img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image115.jpg" alt="img"> <img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image117.jpg" alt="img"></p><ol start="6"><li>RDD 阶段划分源码</li></ol><p>try {</p><p>// New stage creation may throw an exception if, for example, jobs are run on a</p><p>// HadoopRDD whose underlying HDFS files have been deleted.</p><p>finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</p><p>} catch {</p><p>case e: Exception =&gt;</p><p>logWarning(“Creating new stage failed due to exception - job: “ + jobId, e)</p><p>listener.jobFailed(e)</p><p>return</p><p>}</p><p>……</p><p>private def createResultStage(</p><p>rdd: RDD[_],</p><p>func: (TaskContext, Iterator[_]) =&gt; _,</p><p>partitions: Array[Int],</p><p>jobId: Int,</p><p>callSite: CallSite): ResultStage = {</p><p>val parents = getOrCreateParentStages(rdd, jobId)</p><p>val id = nextStageId.getAndIncrement()</p><p>val stage = new ResultStage(id, rdd, func, partitions, parents, jobId, callSite)</p><p>stageIdToStage(id) = stage</p><p>updateJobIdStageIdMaps(jobId, stage)</p><p>stage</p><p>}</p><p>……</p><p>private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {</p><p>getShuffleDependencies(rdd).map { shuffleDep =&gt;</p><p>getOrCreateShuffleMapStage(shuffleDep, firstJobId)</p><p>}.toList</p><p>}</p><p>……</p><p>private[scheduler] def getShuffleDependencies(</p><p>rdd: RDD[<em>]): HashSet[ShuffleDependency[</em>, _, _]] = {</p><p>val parents = new HashSet[ShuffleDependency[_, _, _]]</p><p>val visited = new HashSet[RDD[_]]</p><p>val waitingForVisit = new Stack[RDD[_]]</p><p>waitingForVisit.push(rdd)</p><p>while (waitingForVisit.nonEmpty) {</p><p>val toVisit = waitingForVisit.pop()</p><p>if (!visited(toVisit)) {</p><p>visited += toVisit</p><p>toVisit.dependencies.foreach {</p><p>case shuffleDep: ShuffleDependency[_, _, _] =&gt;</p><p>​ parents += shuffleDep</p><p>case dependency =&gt;</p><p>​ waitingForVisit.push(dependency.rdd)</p><p>}</p><p>}</p><p>}</p><p>parents</p><p>}</p><ol start="7"><li>RDD 任务划分</li></ol><p>RDD任务切分中间分为：Application、Job、Stage和Task</p><p>l Application：初始化一个SparkContext即生成一个Application；</p><p>l Job：一个Action算子就会生成一个Job；</p><p>l Stage：Stage等于宽依赖(ShuffleDependency)的个数加1；</p><p>l Task：一个Stage阶段中，最后一个RDD的分区个数就是Task的个数。</p><p>注意：Application-&gt;Job-&gt;Stage-&gt;Task每一层都是1对n的关系。</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image119.gif" alt="img"></p><ol start="8"><li>RDD 任务划分源码</li></ol><p>val tasks: Seq[Task[_]] = try {</p><p>stage match {</p><p>case stage: ShuffleMapStage =&gt;</p><p>partitionsToCompute.map { id =&gt;</p><p>​ val locs = taskIdToLocations(id)</p><p>​ val part = stage.rdd.partitions(id)</p><p>​ new ShuffleMapTask(stage.id, stage.latestInfo.attemptId,</p><p>​ taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, Option(jobId),</p><p>​ Option(sc.applicationId), sc.applicationAttemptId)</p><p>}</p><p>case stage: ResultStage =&gt;</p><p>partitionsToCompute.map { id =&gt;</p><p>​ val p: Int = stage.partitions(id)</p><p>​ val part = stage.rdd.partitions(p)</p><p>​ val locs = taskIdToLocations(id)</p><p>​ new ResultTask(stage.id, stage.latestInfo.attemptId,</p><p>​ taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics,</p><p>​ Option(jobId), Option(sc.applicationId), sc.applicationAttemptId)</p><p>}</p><p>}</p><p>……</p><p>val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()</p><p>……</p><p>override def findMissingPartitions(): Seq[Int] = {</p><p>mapOutputTrackerMaster</p><p>.findMissingPartitions(shuffleDep.shuffleId)</p><p>.getOrElse(0 until numPartitions)</p><p>}</p><h4 id="2-1-4-8-RDD持久化"><a href="#2-1-4-8-RDD持久化" class="headerlink" title="2.1.4.8 RDD持久化"></a>2.1.4.8 RDD持久化</h4><ol><li>RDD Cache缓存</li></ol><p>RDD通过Cache或者Persist方法将前面的计算结果缓存，默认情况下会把数据以缓存在JVM的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action算子时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p><p>// cache操作会增加血缘关系，不改变原有的血缘关系</p><p>println(wordToOneRdd.toDebugString)</p><p>// 数据缓存。</p><p>wordToOneRdd.cache()</p><p>// 可以更改存储级别</p><p>//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)</p><p>存储级别</p><p>object StorageLevel {</p><p>val NONE = new StorageLevel(false, false, false, false)</p><p>val DISK_ONLY = new StorageLevel(true, false, false, false)</p><p>val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)</p><p>val MEMORY_ONLY = new StorageLevel(false, true, false, true)</p><p>val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)</p><p>val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)</p><p>val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)</p><p>val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)</p><p>val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)</p><p>val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)</p><p>val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</p><p>val OFF_HEAP = new StorageLevel(true, true, true, false, 1)</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image121.gif" alt="img"></p><p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p><p>Spark会自动对一些Shuffle操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点Shuffle失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用persist或cache。</p><ol start="2"><li>RDD CheckPoint检查点</li></ol><p>所谓的检查点其实就是通过将RDD中间结果写入磁盘</p><p>由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。</p><p>对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p><p>// 设置检查点路径</p><p>sc.setCheckpointDir(“./checkpoint1”)</p><p>// 创建一个RDD，读取指定位置文件:hello atguigu atguigu</p><p>val lineRdd: RDD[String] = sc.textFile(“input/1.txt”)</p><p>// 业务逻辑</p><p>val wordRdd: RDD[String] = lineRdd.flatMap(line =&gt; line.split(“ “))</p><p>val wordToOneRdd: RDD[(String, Long)] = wordRdd.map {</p><p>word =&gt; {</p><p>​ (word, System.currentTimeMillis())</p><p>}</p><p>}</p><p>// 增加缓存,避免再重新跑一个job做checkpoint</p><p>wordToOneRdd.cache()</p><p>// 数据检查点：针对wordToOneRdd做检查点计算</p><p>wordToOneRdd.checkpoint()</p><p>// 触发执行逻辑</p><p>wordToOneRdd.collect().foreach(println)</p><ol start="3"><li>缓存和检查点区别</li></ol><p>1）Cache缓存只是将数据保存起来，不切断血缘依赖。Checkpoint检查点切断血缘依赖。</p><p>2）Cache缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint的数据通常存储在HDFS等容错、高可用的文件系统，可靠性高。</p><p>3）建议对checkpoint()的RDD使用Cache缓存，这样checkpoint的job只需从Cache缓存中读取数据即可，否则需要再从头计算一次RDD。</p><h4 id="2-1-4-9-RDD分区器"><a href="#2-1-4-9-RDD分区器" class="headerlink" title="2.1.4.9 RDD分区器"></a>2.1.4.9 RDD分区器</h4><p>Spark目前支持Hash分区和Range分区，和用户自定义分区。Hash分区为当前的默认分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle后进入哪个分区，进而决定了Reduce的个数。</p><p>Ø 只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None</p><p>Ø 每个RDD的分区ID范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。</p><ol><li>Hash分区：对于给定的key，计算其hashCode,并除以分区个数取余</li></ol><p>class HashPartitioner(partitions: Int) extends Partitioner {</p><p>require(partitions &gt;= 0, s”Number of partitions ($partitions) cannot be negative.”)</p><p>def numPartitions: Int = partitions</p><p>def getPartition(key: Any): Int = key match {</p><p>case null =&gt; 0</p><p>case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions)</p><p>}</p><p>override def equals(other: Any): Boolean = other match {</p><p>case h: HashPartitioner =&gt;</p><p>h.numPartitions == numPartitions</p><p>case _ =&gt;</p><p>false</p><p>}</p><p>override def hashCode: Int = numPartitions</p><p>}</p><ol start="2"><li>Range分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</li></ol><p>class RangePartitioner[K : Ordering : ClassTag, V](</p><p>partitions: Int,</p><p>rdd: RDD[_ &lt;: Product2[K, V]],</p><p>private var ascending: Boolean = true)</p><p>extends Partitioner {</p><p>// We allow partitions = 0, which happens when sorting an empty RDD under the default settings.</p><p>require(partitions &gt;= 0, s”Number of partitions cannot be negative but found $partitions.”)</p><p>private var ordering = implicitly[Ordering[K]]</p><p>// An array of upper bounds for the first (partitions - 1) partitions</p><p>private var rangeBounds: Array[K] = {</p><p>…</p><p>}</p><p>def numPartitions: Int = rangeBounds.length + 1</p><p>private var binarySearch: ((Array[K], K) =&gt; Int) = CollectionsUtils.makeBinarySearch[K]</p><p>def getPartition(key: Any): Int = {</p><p>val k = key.asInstanceOf[K]</p><p>var partition = 0</p><p>if (rangeBounds.length &lt;= 128) {</p><p>// If we have less than 128 partitions naive search</p><p>​ while (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) {</p><p>​ partition += 1</p><p>}</p><p>} else {</p><p>// Determine which binary search method to use only once.</p><p>partition = binarySearch(rangeBounds, k)</p><p>// binarySearch either returns the match location or -[insertion point]-1</p><p>if (partition &lt; 0) {</p><p>​ partition = -partition-1</p><p>}</p><p>if (partition &gt; rangeBounds.length) {</p><p>​ partition = rangeBounds.length</p><p>}</p><p>}</p><p>if (ascending) {</p><p>partition</p><p>} else {</p><p>rangeBounds.length - partition</p><p>}</p><p>}</p><p>override def equals(other: Any): Boolean = other match {</p><p>…</p><p>}</p><p>override def hashCode(): Int = {</p><p>…</p><p>}</p><p>@throws(classOf[IOException])</p><p>private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException {</p><p>…</p><p>}</p><p>@throws(classOf[IOException])</p><p>private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {</p><p>…</p><p>}</p><p>}</p><h4 id="2-1-4-10-RDD文件读取与保存"><a href="#2-1-4-10-RDD文件读取与保存" class="headerlink" title="2.1.4.10 RDD文件读取与保存"></a>2.1.4.10 RDD文件读取与保存</h4><p>Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。</p><p>文件格式分为：text文件、csv文件、sequence文件以及Object文件；</p><p>文件系统分为：本地文件系统、HDFS、HBASE以及数据库。</p><p>Ø text文件</p><p>// 读取输入文件</p><p>val inputRDD: RDD[String] = sc.textFile(“input/1.txt”)</p><p>// 保存数据</p><p>inputRDD.saveAsTextFile(“output”)</p><p>Ø sequence文件</p><p>SequenceFile文件是<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/hadoop">Hadoop</a>用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。在SparkContext中，可以调用sequenceFile<a href="path">keyClass, valueClass</a>。</p><p>// 保存数据为SequenceFile</p><p>dataRDD.saveAsSequenceFile(“output”)</p><p>// 读取SequenceFile文件</p><p>sc.sequenceFile<a href="%22output%22">Int,Int</a>.collect().foreach(println)</p><p>Ø object对象文件</p><p>对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFile<a href="path">T: ClassTag</a>函数接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。因为是序列化所以要指定类型。</p><p>// 保存数据</p><p>dataRDD.saveAsObjectFile(“output”)</p><p>// 读取数据</p><p>sc.objectFile<a href="%22output%22">Int</a>.collect().foreach(println)</p><h2 id="2-2-累加器"><a href="#2-2-累加器" class="headerlink" title="2.2 累加器"></a>2.2 累加器</h2><h3 id="2-2-1-实现原理"><a href="#2-2-1-实现原理" class="headerlink" title="2.2.1 实现原理"></a>2.2.1 实现原理</h3><p>累加器用来把Executor端变量信息聚合到Driver端。在Driver程序中定义的变量，在Executor端的每个Task都会得到这个变量的一份新的副本，每个task更新这些副本的值后，传回Driver端进行merge。</p><h3 id="2-2-2-基础编程"><a href="#2-2-2-基础编程" class="headerlink" title="2.2.2 基础编程"></a>2.2.2 基础编程</h3><h4 id="2-2-2-1-系统累加器"><a href="#2-2-2-1-系统累加器" class="headerlink" title="2.2.2.1 系统累加器"></a>2.2.2.1 系统累加器</h4><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true">// 声明累加器</span>
<span class="token keyword">var</span> sum <span class="token operator">=</span> sc<span class="token punctuation">.</span>longAccumulator<span class="token punctuation">(</span><span class="token string">"sum"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
rdd<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>
  num <span class="token keyword">=></span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token comment" spellcheck="true">// 使用累加器</span>
    sum<span class="token punctuation">.</span>add<span class="token punctuation">(</span>num<span class="token punctuation">)</span>
  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true">// 获取累加器的值</span>
println<span class="token punctuation">(</span><span class="token string">"sum = "</span> <span class="token operator">+</span> sum<span class="token punctuation">.</span>value<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-2-2-2-自定义累加器"><a href="#2-2-2-2-自定义累加器" class="headerlink" title="2.2.2.2 自定义累加器"></a>2.2.2.2 自定义累加器</h4><pre class="line-numbers language-scala"><code class="language-scala"><span class="token comment" spellcheck="true">// 自定义累加器</span>
<span class="token comment" spellcheck="true">// 1. 继承AccumulatorV2，并设定泛型</span>
<span class="token comment" spellcheck="true">// 2. 重写累加器的抽象方法</span>
<span class="token keyword">class</span> WordCountAccumulator <span class="token keyword">extends</span> AccumulatorV2<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Long</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

<span class="token keyword">var</span> map <span class="token operator">:</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Long</span><span class="token punctuation">]</span> <span class="token operator">=</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 累加器是否为初始状态</span>
<span class="token keyword">override</span> <span class="token keyword">def</span> isZero<span class="token operator">:</span> <span class="token builtin">Boolean</span> <span class="token operator">=</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
  map<span class="token punctuation">.</span>isEmpty
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// 复制累加器</span>
<span class="token keyword">override</span> <span class="token keyword">def</span> copy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> AccumulatorV2<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Long</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
  <span class="token keyword">new</span> WordCountAccumulator
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// 重置累加器</span>
<span class="token keyword">override</span> <span class="token keyword">def</span> reset<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
  map<span class="token punctuation">.</span>clear<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// 向累加器中增加数据 (In)</span>
<span class="token keyword">override</span> <span class="token keyword">def</span> add<span class="token punctuation">(</span>word<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token comment" spellcheck="true">// 查询map中是否存在相同的单词</span>
    <span class="token comment" spellcheck="true">// 如果有相同的单词，那么单词的数量加1</span>
    <span class="token comment" spellcheck="true">// 如果没有相同的单词，那么在map中增加这个单词</span>
    map<span class="token punctuation">(</span>word<span class="token punctuation">)</span> <span class="token operator">=</span> map<span class="token punctuation">.</span>getOrElse<span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">0L</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1L</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// 合并累加器</span>
<span class="token keyword">override</span> <span class="token keyword">def</span> merge<span class="token punctuation">(</span>other<span class="token operator">:</span> AccumulatorV2<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Long</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

  <span class="token keyword">val</span> map1 <span class="token operator">=</span> map
  <span class="token keyword">val</span> map2 <span class="token operator">=</span> other<span class="token punctuation">.</span>value

  <span class="token comment" spellcheck="true">// 两个Map的合并</span>
  map <span class="token operator">=</span> map1<span class="token punctuation">.</span>foldLeft<span class="token punctuation">(</span>map2<span class="token punctuation">)</span><span class="token punctuation">(</span>
    <span class="token punctuation">(</span> innerMap<span class="token punctuation">,</span> kv <span class="token punctuation">)</span> <span class="token keyword">=></span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
      innerMap<span class="token punctuation">(</span>kv<span class="token punctuation">.</span>_1<span class="token punctuation">)</span> <span class="token operator">=</span> innerMap<span class="token punctuation">.</span>getOrElse<span class="token punctuation">(</span>kv<span class="token punctuation">.</span>_1<span class="token punctuation">,</span> <span class="token number">0L</span><span class="token punctuation">)</span> <span class="token operator">+</span> kv<span class="token punctuation">.</span>_2
      innerMap
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
  <span class="token punctuation">)</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// 返回累加器的结果 （Out）</span>
<span class="token keyword">override</span> <span class="token keyword">def</span> value<span class="token operator">:</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Long</span><span class="token punctuation">]</span> <span class="token operator">=</span> map
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-3-广播变量"><a href="#2-3-广播变量" class="headerlink" title="2.3 广播变量"></a>2.3 广播变量</h2><h3 id="2-3-1-实现原理"><a href="#2-3-1-实现原理" class="headerlink" title="2.3.1 实现原理"></a>2.3.1 实现原理</h3><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。</p><h3 id="2-3-2-基础编程"><a href="#2-3-2-基础编程" class="headerlink" title="2.3.2 基础编程"></a>2.3.2 基础编程</h3><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> rdd1 <span class="token operator">=</span> sc<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>List<span class="token punctuation">(</span> <span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"b"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"c"</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"d"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> list <span class="token operator">=</span> List<span class="token punctuation">(</span> <span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"b"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"c"</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"d"</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span> <span class="token punctuation">)</span>
<span class="token comment" spellcheck="true">// 声明广播变量</span>
<span class="token keyword">val</span> broadcast<span class="token operator">:</span> Broadcast<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> sc<span class="token punctuation">.</span>broadcast<span class="token punctuation">(</span>list<span class="token punctuation">)</span>

<span class="token keyword">val</span> resultRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">Int</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> rdd1<span class="token punctuation">.</span>map <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
  <span class="token keyword">case</span> <span class="token punctuation">(</span>key<span class="token punctuation">,</span> num<span class="token punctuation">)</span> <span class="token keyword">=></span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token keyword">var</span> num2 <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token comment" spellcheck="true">// 使用广播变量</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> v<span class="token punctuation">)</span> <span class="token keyword">&lt;-</span> broadcast<span class="token punctuation">.</span>value<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token keyword">if</span> <span class="token punctuation">(</span>k <span class="token operator">==</span> key<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        num2 <span class="token operator">=</span> v
      <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token punctuation">(</span>key<span class="token punctuation">,</span> <span class="token punctuation">(</span>num<span class="token punctuation">,</span> num2<span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li></ul></div><span class="post-count">总字数11.9k</span> <span class="post-count">预计阅读50分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-Spark1-setup" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/29/bigdata-Spark1-setup/" class="article-date"><time class="published" datetime="2020-11-29T03:15:49.000Z" itemprop="datePublished">2020-11-29 发布</time> <time class="updated" datetime="2021-12-02T02:41:58.591Z" itemprop="dateUpdated">2021-12-02 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/29/bigdata-Spark1-setup/">Spark学习笔记（一） 搭建Spark</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="1-Spark概述"><a href="#1-Spark概述" class="headerlink" title="1 Spark概述"></a>1 Spark概述</h1><h2 id="1-1-spark-or-haddop"><a href="#1-1-spark-or-haddop" class="headerlink" title="1.1 spark or haddop"></a>1.1 spark or haddop</h2><p>MR框架主要应用于数据的一次性计算：存存储介质中读取数据，然后进行过处理后，再存储到文件中。IO读取多，效率低。</p><p>1）Spark是基于MR框架的，但是优化了其中的计算过程，使用内存来代替计算结果。减少了磁盘IO，因此快。（MR多作业之间会多次磁盘的IO，因此慢）</p><p>2）Spark基于Scala语言开发的，更适合迭代计算和数据挖掘计算</p><p>3） Spark中计算模型非常丰富（MR中只有两个计算模型：mapper和reducer）;spark的计算模型有：map,filter,groupby,sortby。</p><p>工作中：是Spark中和Yarn联合使用：资源用的是Yarn,计算用的是spark。</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638156481825.png" alt="1638156481825"></p><h2 id="1-2-Spark核心模块"><a href="#1-2-Spark核心模块" class="headerlink" title="1.2 Spark核心模块"></a>1.2 Spark核心模块</h2><h2 id="1-1-Spark-核心模块"><a href="#1-1-Spark-核心模块" class="headerlink" title="1.1 Spark 核心模块"></a>1.1 Spark 核心模块</h2><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176912041.png" alt="1638176912041"></p><p>1）<strong>Spark Core</strong></p><p>Spark Core中提供了Spark最基础与最核心的功能，Spark其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib都是在Spark Core的基础上进行扩展的</p><p>2） <strong>Spark SQL</strong></p><p>Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言（HQL）来查询数据。</p><ol start="3"><li><strong>Spark Streaming</strong></li></ol><p>Spark Streaming是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。</p><p>4） <strong>Spark MLlib</strong></p><p>MLlib是Spark提供的一个机器学习算法库。MLlib不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。</p><p>5）<strong>Spark GraphX</strong></p><p>GraphX是Spark面向图计算提供的框架与算法库。</p><h1 id="2-Spark快速上手"><a href="#2-Spark快速上手" class="headerlink" title="2 Spark快速上手"></a>2 Spark快速上手</h1><p>在大数据早期的课程中我们已经学习了MapReduce框架的原理及基本使用，并了解了其底层数据处理的实现方式。接下来，就让咱们走进Spark的世界，了解一下它是如何带领我们完成数据处理的。</p><h2 id="2-1-创建Maven项目"><a href="#2-1-创建Maven项目" class="headerlink" title="2.1  创建Maven项目"></a>2.1 创建Maven项目</h2><h3 id="2-1-1-增加Scala插件"><a href="#2-1-1-增加Scala插件" class="headerlink" title="2.1.1 增加Scala插件"></a>2.1.1 增加Scala插件</h3><p>Spark由Scala语言开发的，所以本课件接下来的开发所使用的语言也为Scala，咱们当前使用的Spark版本为3.0.0，默认采用的Scala编译版本为2.12，所以后续开发时。我们依然采用这个版本。开发前请保证IDEA开发工具中含有Scala开发插件。</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176561427.png" alt="1638176561427"></p><h3 id="2-1-2-增加依赖关系"><a href="#2-1-2-增加依赖关系" class="headerlink" title="2.1.2 增加依赖关系"></a>2.1.2 增加依赖关系</h3><p>修改Maven项目中的POM文件，增加Spark框架的依赖关系。本课件基于Spark3.0版本，使用时请注意对应版本。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.spark<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>spark-core_2.12<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.0.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>
        <span class="token comment" spellcheck="true">&lt;!-- 该插件用于将Scala代码编译成class文件 --></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>net.alchim31.maven<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>scala-maven-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.2.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>
                    <span class="token comment" spellcheck="true">&lt;!-- 声明绑定到maven的compile阶段 --></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>
                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>testCompile<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-assembly-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.1.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRefs</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRef</span><span class="token punctuation">></span></span>jar-with-dependencies<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRef</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRefs</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>make-assembly<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>
                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>single<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-1-3-WordCount"><a href="#2-1-3-WordCount" class="headerlink" title="2.1.3 WordCount"></a>2.1.3 WordCount</h3><p>为了能直观地感受Spark框架的效果，接下来我们实现一个大数据学科中最常见的教学案例WordCount</p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token comment" spellcheck="true">// 创建Spark运行配置对象</span>
<span class="token keyword">val</span> sparkConf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"WordCount"</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 创建Spark上下文环境对象（连接对象）</span>
<span class="token keyword">val</span> sc <span class="token operator">:</span> SparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 读取文件数据</span>
<span class="token keyword">val</span> fileRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"input/word.txt"</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 将文件中的数据进行分词</span>
<span class="token keyword">val</span> wordRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> fileRDD<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span> _<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span> <span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 转换数据结构 word => (word, 1)</span>
<span class="token keyword">val</span> word2OneRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> wordRDD<span class="token punctuation">.</span>map<span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 将转换结构后的数据按照相同的单词进行分组聚合</span>
<span class="token keyword">val</span> word2CountRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> word2OneRDD<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>_<span class="token operator">+</span>_<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 将数据聚合结果采集到内存中</span>
<span class="token keyword">val</span> word2Count<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> word2CountRDD<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 打印结果</span>
word2Count<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">//关闭Spark连接</span>
sc<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>执行过程中，会产生大量的执行日志，如果为了能够更好的查看程序的执行结果，可以在项目的resources目录中创建log4j.properties文件，并添加日志配置信息：</p><pre class="line-numbers language-properties"><code class="language-properties"><span class="token attr-name">log4j.rootCategory</span><span class="token punctuation">=</span><span class="token attr-value">ERROR, console</span>
<span class="token attr-name">log4j.appender.console</span><span class="token punctuation">=</span><span class="token attr-value">org.apache.log4j.ConsoleAppender</span>
<span class="token attr-name">log4j.appender.console.target</span><span class="token punctuation">=</span><span class="token attr-value">System.err</span>
<span class="token attr-name">log4j.appender.console.layout</span><span class="token punctuation">=</span><span class="token attr-value">org.apache.log4j.PatternLayout</span>
<span class="token attr-name">log4j.appender.console.layout.ConversionPattern</span><span class="token punctuation">=</span><span class="token attr-value">%d&amp;#123;yy/MM/dd HH:mm:ss&amp;#125; %p %c&amp;#123;1&amp;#125;: %m%n</span>

<span class="token comment" spellcheck="true"># Set the default spark-shell log level to ERROR. When running the spark-shell, the</span>
<span class="token comment" spellcheck="true"># log level for this class is used to overwrite the root logger's log level, so that</span>
<span class="token comment" spellcheck="true"># the user can have different defaults for the shell and regular Spark apps.</span>
<span class="token attr-name">log4j.logger.org.apache.spark.repl.Main</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>

<span class="token comment" spellcheck="true"># Settings to quiet third party logs that are too verbose</span>
<span class="token attr-name">log4j.logger.org.spark_project.jetty</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>
<span class="token attr-name">log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>
<span class="token attr-name">log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>
<span class="token attr-name">log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>
<span class="token attr-name">log4j.logger.org.apache.parquet</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>
<span class="token attr-name">log4j.logger.parquet</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>

<span class="token comment" spellcheck="true"># SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support</span>
<span class="token attr-name">log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler</span><span class="token punctuation">=</span><span class="token attr-value">FATAL</span>
<span class="token attr-name">log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-1-4-异常处理"><a href="#2-1-4-异常处理" class="headerlink" title="2.1.4 异常处理"></a>2.1.4 异常处理</h3><p>如果本机操作系统是Windows，在程序中使用了Hadoop相关的东西，比如写入文件到HDFS，则会遇到如下异常：</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176731823.png" alt="1638176731823"></p><p>出现这个问题的原因，并不是程序的错误，而是windows系统用到了hadoop相关的服务，解决办法是通过配置关联到windows的系统依赖就可以了</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176805506.png" alt="1638176805506"></p><p>在IDEA中配置Run Configuration，添加HADOOP_HOME变量</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176855314.png" alt="1638176855314"></p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176879056.png" alt="1638176879056"></p><h1 id="3-Spark运行环境"><a href="#3-Spark运行环境" class="headerlink" title="3 Spark运行环境"></a>3 Spark运行环境</h1><p>Spark作为一个数据处理框架和计算引擎，被设计在所有常见的集群环境中运行, 在国内工作中主流的环境为Yarn，不过逐渐<strong>容器式环境</strong>也慢慢流行起来。接下来，我们就分别看看不同环境下Spark的运行。</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176475372.png" alt="1638176475372"></p><h2 id="3-1-Local模式"><a href="#3-1-Local模式" class="headerlink" title="3.1  Local模式"></a>3.1 Local模式</h2><p>所谓的<strong>Local模式，就是不需要其他任何节点资源就可以在本地执行Spark代码的环境</strong>，一般用于教学，调试，演示等，之前在IDEA中运行代码的环境我们称之为<strong>开发环境</strong>，不太一样。（<strong>Local=本机提供资源+spark提供计算</strong>）</p><h3 id="3-1-1-解压缩文件"><a href="#3-1-1-解压缩文件" class="headerlink" title="3.1.1 解压缩文件"></a>3.1.1 解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到Linux并解压缩，放置在指定位置，路径中不要包含中文或空格，课件后续如果涉及到解压缩操作，不再强调。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">tar</span> -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
<span class="token function">cd</span> /opt/module 
<span class="token function">mv</span> spark-3.0.0-bin-hadoop3.2 spark-local<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="3-1-2-启动Local环境"><a href="#3-1-2-启动Local环境" class="headerlink" title="3.1.2 启动Local环境"></a>3.1.2 启动Local环境</h3><ol><li>进入解压缩后的路径，执行如下指令</li></ol><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-shell<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638173742050.png" alt="1638173742050"></p><ol start="2"><li>启动成功后，可以输入网址进行Web UI<strong>监控页面</strong>访问</li></ol><p>http://虚拟机地址:4040</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176427950.png" alt="1638176427950"></p><h3 id="3-1-3-命令行工具"><a href="#3-1-3-命令行工具" class="headerlink" title="3.1.3 命令行工具"></a>3.1.3 命令行工具</h3><p>在解压缩文件夹下的data目录中，添加word.txt文件。在命令行工具中执行如下代码指令（和IDEA中代码简化版一致）</p><pre class="line-numbers language-scala"><code class="language-scala">sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"data/word.txt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>_<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>_<span class="token operator">+</span>_<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176440182.png" alt="1638176440182"></p><h3 id="3-1-4-退出本地模式"><a href="#3-1-4-退出本地模式" class="headerlink" title="3.1.4 退出本地模式"></a>3.1.4 退出本地模式</h3><p>按键Ctrl+C或输入Scala指令</p><pre class="line-numbers language-bash"><code class="language-bash">:quit<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-1-5-提交应用"><a href="#3-1-5-提交应用" class="headerlink" title="3.1.5 提交应用"></a>3.1.5 提交应用</h3><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master local<span class="token punctuation">[</span>2<span class="token punctuation">]</span> \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>–class表示要执行程序的主类，此处可以更换为咱们自己写的应用程序</li><li>–master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟CPU核数量;不知道可以用*</li><li>spark-examples_2.12-3.0.0.jar 运行的应用类所在的jar包，实际使用时，可以设定为咱们自己打的jar包</li><li>数字10表示程序的入口参数，用于设定当前应用的任务数量</li></ol><h2 id="3-2-Standalone模式"><a href="#3-2-Standalone模式" class="headerlink" title="3.2  Standalone模式"></a>3.2 Standalone模式</h2><p>local本地模式毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，这里我们来看看只使用<strong>Spark自身节点运行的集群模式</strong>，也就是我们所谓的独立部署（Standalone）模式。Spark的Standalone模式体现了经典的master-slave模式。（<strong>Standalone=spark提供资源+spark提供计算</strong>）</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638174301463.png" alt="1638174301463"></p><p>集群规划:</p><table><thead><tr><th></th><th>Linux1</th><th>Linux2</th><th>Linux3</th></tr></thead><tbody><tr><td>Spark</td><td>Worker Master</td><td>Worker</td><td>Worker</td></tr></tbody></table><h3 id="3-2-1-解压缩文件"><a href="#3-2-1-解压缩文件" class="headerlink" title="3.2.1 解压缩文件"></a>3.2.1 解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到Linux并解压缩在指定位置</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">tar</span> -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
<span class="token function">cd</span> /opt/module 
<span class="token function">mv</span> spark-3.0.0-bin-hadoop3.2 spark-standalone<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="3-2-2-修改配置文件"><a href="#3-2-2-修改配置文件" class="headerlink" title="3.2.2 修改配置文件"></a>3.2.2 修改配置文件</h3><ol><li>进入解压缩后路径的conf目录，修改slaves.template文件名为slaves</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">mv</span> slaves.template slaves 修改slaves文件，添加worker节点<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-bash"><code class="language-bash">linux1
linux2
linux3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="2"><li>修改spark-env.sh.template文件名为spark-env.sh</li></ol><pre><code>mv spark-env.sh.template spark-env.sh</code></pre><p>修改spark-env.sh文件，添加JAVA_HOME环境变量和集群对应的master节点。指定master机器，和集群间spark通信的端口。</p><pre class="line-numbers language-sh"><code class="language-sh">export JAVA_HOME=/opt/module/jdk1.8.0_144
SPARK_MASTER_HOST=linux1
SPARK_MASTER_PORT=7077<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>注意：7077端口，相当于hadoop3内部通信的8020端口，此处的端口需要确认自己的Hadoop配置</p><ol start="5"><li>分发spark-standalone目录</li></ol><pre class="line-numbers language-bash"><code class="language-bash">xsync spark-standalone<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-2-3-启动集群"><a href="#3-2-3-启动集群" class="headerlink" title="3.2.3 启动集群"></a>3.2.3 启动集群</h3><ol><li>执行脚本命令：</li></ol><pre class="line-numbers language-bash"><code class="language-bash">sbin/start-all.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638174700872.png" alt="1638174700872"></p><ol start="2"><li>查看三台服务器运行进程</li></ol><pre class="line-numbers language-bash"><code class="language-bash">xcall jps
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>linux1<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>
3330 Jps
3238 Worker
3163 Master
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>linux2<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>
2966 Jps
2908 Worker
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>linux3<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>
2978 Worker
3036 Jps<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="3"><li>查看Master资源监控Web UI界面: <a target="_blank" rel="noopener" href="http://linux1:8080/">http://linux1:8080</a></li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638174744996.png" alt="1638174744996"></p><h3 id="3-2-4-提交应用"><a href="#3-2-4-提交应用" class="headerlink" title="3.2.4 提交应用"></a>3.2.4 提交应用</h3><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://linux1:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>–class表示要执行程序的主</li><li>–master spark://linux1:7077 独立部署模式，连接到Spark集群</li><li>spark-examples_2.12-3.0.0.jar 运行类所在的jar包</li><li>数字10表示程序的入口参数，用于设定当前应用的任务数量</li></ol><p>执行任务时，会产生多个Java进程</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175491608.png" alt="1638175491608"></p><p>执行任务时，默认采用服务器集群节点的总核数，每个节点内存1024M。</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175504305.png" alt="1638175504305"></p><h3 id="3-2-5-提交参数说明"><a href="#3-2-5-提交参数说明" class="headerlink" title="3.2.5 提交参数说明"></a>3.2.5 提交参数说明</h3><p>在提交应用中，一般会同时一些提交参数</p><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-submit \
--class <span class="token operator">&lt;</span>main-class<span class="token operator">></span>
--master <span class="token operator">&lt;</span>master-url<span class="token operator">></span> \
<span class="token punctuation">..</span>. <span class="token comment" spellcheck="true"># other options</span>
<span class="token operator">&lt;</span>application-jar<span class="token operator">></span> \
<span class="token punctuation">[</span>application-arguments<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><table><thead><tr><th>参数</th><th>解释</th><th>可选值举例</th></tr></thead><tbody><tr><td>–class</td><td>Spark程序中包含主函数的类</td><td></td></tr><tr><td>–master</td><td>Spark程序运行的模式(环境)</td><td>*<em>模式：local[</em>]、spark://linux1:7077、 Yarn**</td></tr><tr><td>–executor-memory 1G</td><td>指定每个executor可用内存为1G</td><td>符合集群内存配置即可，具体情况具体分析。</td></tr><tr><td>–total-executor-cores 2</td><td>指定所有executor使用的cpu核数为2个</td><td></td></tr><tr><td>–executor-cores</td><td>指定每个executor使用的cpu核数</td><td></td></tr><tr><td>application-jar</td><td>打包好的应用jar，包含依赖。这个URL在集群中全局可见。 比如hdfs:// 共享存储系统，如果是file:// path，那么所有的节点的path都包含同样的jar</td><td></td></tr><tr><td>application-arguments</td><td>传给main()方法的参数</td><td></td></tr></tbody></table><h3 id="3-2-6-配置历史服务"><a href="#3-2-6-配置历史服务" class="headerlink" title="3.2.6 配置历史服务"></a>3.2.6 配置历史服务</h3><p>由于spark-shell停止掉后，集群监控linux1:4040页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况。**(历史信息存到了HDFS中)**</p><ol><li>修改spark-defaults.conf.template文件名为spark-defaults.conf</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">mv</span> spark-defaults.conf.template spark-defaults.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="2"><li>修改spark-default.conf文件，配置日志存储路径</li></ol><pre class="line-numbers language-sh"><code class="language-sh">spark.eventLog.enabled     true
spark.eventLog.dir        hdfs://linux1:8020/directory<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>注意：需要启动hadoop集群，HDFS上的directory目录需要提前存在。</strong></p><pre class="line-numbers language-bash"><code class="language-bash">sbin/start-dfs.sh
hadoop fs -mkdir /directory<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="3"><li>修改spark-env.sh文件, 添加日志配置</li></ol><pre class="line-numbers language-sh"><code class="language-sh">export SPARK_HISTORY_OPTS="
-Dspark.history.ui.port=18080 
-Dspark.history.fs.logDirectory=hdfs://linux1:8020/directory 
-Dspark.history.retainedApplications=30"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>l 参数1含义：WEB UI访问的端口号为18080<br>l 参数2含义：指定历史服务器日志存储路径<br>l 参数3含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。</p><ol start="4"><li>分发配置文件</li></ol><pre class="line-numbers language-bash"><code class="language-bash">xsync conf <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="5"><li>重新启动集群和历史服务</li></ol><pre><code>sbin/start-all.sh

sbin/start-history-server.sh</code></pre><ol start="6"><li>重新执行任务</li></ol><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://linux1:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="7"><li>查看历史服务：<a target="_blank" rel="noopener" href="http://linux1:18080/">http://linux1:18080</a></li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175185282.png" alt="1638175185282"></p><h3 id="3-2-7-配置高可用（HA）"><a href="#3-2-7-配置高可用（HA）" class="headerlink" title="3.2.7 配置高可用（HA）"></a>3.2.7 配置高可用（HA）</h3><p>所谓的高可用是因为当前集群中的Master节点只有一个，所以会存在单点故障问题。<strong>所以为了解决单点故障问题，需要在集群中配置多个Master节点</strong>，一旦处于活动状态的Master发生故障时，由备用Master提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper设置。</p><p><strong>集群规划</strong>:</p><table><thead><tr><th></th><th>Linux1</th><th>Linux2</th><th>Linux3</th></tr></thead><tbody><tr><td>Spark</td><td>Master Zookeeper Worker</td><td>Master Zookeeper Worker</td><td>Zookeeper Worker</td></tr></tbody></table><ol><li>停止集群</li></ol><pre class="line-numbers language-bash"><code class="language-bash">sbin/stop-all.sh <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="2"><li>启动Zookeeper</li></ol><pre class="line-numbers language-bash"><code class="language-bash">xstart zk <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="3"><li>修改spark-env.sh文件添加如下配置</li></ol><p>注释如下内容：</p><p>#SPARK_MASTER_HOST=linux1</p><p>#SPARK_MASTER_PORT=7077</p><p>添加如下内容:</p><p>#Master监控页面默认访问端口为8080，但是可能会和Zookeeper冲突，所以改成8989，也可以自定义，访问UI监控页面时请注意</p><pre class="line-numbers language-sh"><code class="language-sh">SPARK_MASTER_WEBUI_PORT=8989
export SPARK_DAEMON_JAVA_OPTS="
-Dspark.deploy.recoveryMode=ZOOKEEPER 
-Dspark.deploy.zookeeper.url=linux1,linux2,linux3 
-Dspark.deploy.zookeeper.dir=/spark"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="4"><li>分发配置文件</li></ol><pre class="line-numbers language-bash"><code class="language-bash">xsync conf/ <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="5"><li>启动集群</li></ol><pre class="line-numbers language-bash"><code class="language-bash">sbin/start-all.sh <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175383495.png" alt="1638175383495"></p><ol start="6"><li>启动linux2的单独Master节点，此时linux2节点Master状态处于备用状态</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@linux2 spark-standalone<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># sbin/start-master.sh </span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175397500.png" alt="1638175397500"></p><ol start="7"><li>提交应用到高可用集群</li></ol><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://linux1:7077,linux2:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="8"><li>停止linux1的Master资源监控进程</li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175436542.png" alt="1638175436542"></p><ol start="9"><li>查看linux2的Master 资源监控Web UI，稍等一段时间后，linux2节点的Master状态提升为活动状态</li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175451854.png" alt="1638175451854"></p><h2 id="3-3-Yarn模式"><a href="#3-3-Yarn模式" class="headerlink" title="3.3  Yarn模式"></a>3.3 Yarn模式</h2><p>独立部署（Standalone）模式由Spark自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是你也要记住，Spark主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的Yarn环境下Spark是如何工作的（其实是因为在国内工作中，Yarn使用的非常多）。</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175863107.png" alt="1638175863107"></p><h3 id="3-3-1-解压缩文件"><a href="#3-3-1-解压缩文件" class="headerlink" title="3.3.1 解压缩文件"></a>3.3.1 解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到linux并解压缩，放置在指定位置。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">tar</span> -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
<span class="token function">cd</span> /opt/module 
<span class="token function">mv</span> spark-3.0.0-bin-hadoop3.2 spark-yarn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="3-3-2-修改配置文件"><a href="#3-3-2-修改配置文件" class="headerlink" title="3.3.2 修改配置文件"></a>3.3.2 修改配置文件</h3><ol><li>修改hadoop配置文件/opt/module/hadoop/etc/hadoop/yarn-site.xml, 并分发</li></ol><pre class="line-numbers language-xml"><code class="language-xml"><span class="token comment" spellcheck="true">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.pmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token comment" spellcheck="true">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.vmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>修改conf/spark-env.sh，添加JAVA_HOME和YARN_CONF_DIR配置</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">mv</span> spark-env.sh.template spark-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查找yarn的位置</p><pre class="line-numbers language-sh"><code class="language-sh">export JAVA_HOME=/opt/module/jdk1.8.0_144
YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="3-3-3-启动HDFS以及YARN集群"><a href="#3-3-3-启动HDFS以及YARN集群" class="headerlink" title="3.3.3 启动HDFS以及YARN集群"></a>3.3.3 启动HDFS以及YARN集群</h3><p>瞅啥呢，自己启动去！</p><h3 id="3-3-4-提交应用"><a href="#3-3-4-提交应用" class="headerlink" title="3.3.4 提交应用"></a>3.3.4 提交应用</h3><p>master指定为yarn，部署模式为集群模式。</p><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode cluster \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175901609.png" alt="1638175901609"></p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175915913.png" alt="1638175915913"></p><p>查看 <a target="_blank" rel="noopener" href="http://linux2:8088/">http://linux2:8088</a> 页面，点击History，查看历史页面</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175935231.png" alt="1638175935231"></p><h3 id="3-3-5-配置历史服务器"><a href="#3-3-5-配置历史服务器" class="headerlink" title="3.3.5 配置历史服务器"></a>3.3.5 配置历史服务器</h3><ol><li>修改spark-defaults.conf.template文件名为spark-defaults.conf</li></ol><pre><code>mv spark-defaults.conf.template spark-defaults.conf</code></pre><ol start="2"><li>修改spark-default.conf文件，配置日志存储路径</li></ol><pre><code>spark.eventLog.enabled     true
spark.eventLog.dir        hdfs://linux1:8020/directory</code></pre><p>注意：需要启动hadoop集群，HDFS上的目录需要提前存在。</p><pre><code>[root@linux1 hadoop]# sbin/start-dfs.sh
[root@linux1 hadoop]# hadoop fs -mkdir /directory</code></pre><ol start="3"><li>修改spark-env.sh文件, 添加日志配置</li></ol><pre><code>export SPARK_HISTORY_OPTS=&quot;
-Dspark.history.ui.port=18080 
-Dspark.history.fs.logDirectory=hdfs://linux1:8020/directory 
-Dspark.history.retainedApplications=30&quot;</code></pre><p>l 参数1含义：WEB UI访问的端口号为18080</p><p>l 参数2含义：指定历史服务器日志存储路径</p><p>l 参数3含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。</p><ol start="4"><li>修改spark-defaults.conf</li></ol><pre><code>spark.yarn.historyServer.address=linux1:18080
spark.history.ui.port=18080</code></pre><ol start="5"><li>启动历史服务</li></ol><pre><code>sbin/start-history-server.sh </code></pre><ol start="6"><li>重新提交应用</li></ol><pre><code>bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10</code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176022879.png" alt="1638176022879"></p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176037995.png" alt="1638176037995"></p><ol start="7"><li>Web页面查看日志：<a target="_blank" rel="noopener" href="http://linux2:8088/">http://linux2:8088</a></li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176059473.png" alt="1638176059473"></p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176071216.png" alt="1638176071216"></p><h2 id="3-4-K8S-amp-Mesos模式"><a href="#3-4-K8S-amp-Mesos模式" class="headerlink" title="3.4  K8S &amp; Mesos模式"></a>3.4 K8S &amp; Mesos模式</h2><p>Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核,在Twitter得到广泛使用,管理着Twitter超过30,0000台服务器上的应用部署，但是在国内，依然使用着传统的Hadoop大数据框架，所以国内使用Mesos框架的并不多，但是原理其实都差不多，这里我们就不做过多讲解了。</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176091921.png" alt="1638176091921"></p><p>容器化部署是目前业界很流行的一项技术，基于Docker镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes（k8s），而Spark也在最近的版本中支持了k8s部署模式。这里我们也不做过多的讲解。给个链接大家自己感受一下：<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/running-on-kubernetes.html">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176106974.png" alt="1638176106974"></p><h2 id="3-5-Windows模式"><a href="#3-5-Windows模式" class="headerlink" title="3.5  Windows模式"></a>3.5 Windows模式</h2><p>在同学们自己学习时，每次都需要启动虚拟机，启动集群，这是一个比较繁琐的过程，并且会占大量的系统资源，导致系统执行变慢，不仅仅影响学习效果，也影响学习进度，Spark非常暖心地提供了可以在windows系统下启动本地集群的方式，这样，在不使用虚拟机的情况下，也能学习Spark的基本使用！</p><p>在后续的教学中，为了能够给同学们更加流畅的教学效果和教学体验，我们一般情况下都会采用windows系统的集群来学习Spark。</p><h3 id="3-5-1-解压缩文件"><a href="#3-5-1-解压缩文件" class="headerlink" title="3.5.1 解压缩文件"></a>3.5.1 解压缩文件</h3><p>将文件spark-3.0.0-bin-hadoop3.2.tgz解压缩到无中文无空格的路径中</p><h3 id="3-5-2-启动本地环境"><a href="#3-5-2-启动本地环境" class="headerlink" title="3.5.2 启动本地环境"></a>3.5.2 启动本地环境</h3><ol><li>执行解压缩文件路径下bin目录中的spark-shell.cmd文件，启动Spark本地环境</li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176202265.png" alt="1638176202265"></p><ol start="2"><li>在bin目录中创建input目录，并添加word.txt文件, 在命令行中输入脚本代码</li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176219502.png" alt="1638176219502"></p><h3 id="3-5-3-命令行提交应用"><a href="#3-5-3-命令行提交应用" class="headerlink" title="3.5.3 命令行提交应用"></a>3.5.3 命令行提交应用</h3><p>在DOS命令行窗口中执行提交指令</p><pre class="line-numbers language-bash"><code class="language-bash">spark-submit --class org.apache.spark.examples.SparkPi --master local<span class="token punctuation">[</span>2<span class="token punctuation">]</span> <span class="token punctuation">..</span>/examples/jars/spark-examples_2.12-3.0.0.jar 10<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="3-6-部署模式对比"><a href="#3-6-部署模式对比" class="headerlink" title="3.6  部署模式对比"></a>3.6 部署模式对比</h2><table><thead><tr><th>模式</th><th>Spark安装机器数</th><th>需启动的进程</th><th>所属者</th><th>应用场景</th></tr></thead><tbody><tr><td>Local</td><td>1</td><td>无</td><td>Spark</td><td>测试</td></tr><tr><td>Standalone</td><td>3</td><td>Master及Worker</td><td>Spark</td><td>单独部署</td></tr><tr><td>Yarn</td><td>1</td><td>Yarn及HDFS</td><td>Hadoop</td><td>混合部署</td></tr></tbody></table><h2 id="3-7-端口号"><a href="#3-7-端口号" class="headerlink" title="3.7  端口号"></a>3.7 端口号</h2><ul><li><p>Spark查看当前Spark-shell运行任务情况端口号：4040（计算）</p></li><li><p>Spark Master内部通信服务端口号：7077</p></li><li><p>Standalone模式下，Spark Master Web端口号：8080（资源）</p></li><li><p>Spark历史服务器端口号：18080</p></li><li><p>Hadoop YARN任务运行情况查看端口号：8088</p></li></ul><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li></ul></div><span class="post-count">总字数4.8k</span> <span class="post-count">预计阅读20分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-HBase2-framework" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/25/bigdata-HBase2-framework/" class="article-date"><time class="published" datetime="2020-11-25T03:46:51.000Z" itemprop="datePublished">2020-11-25 发布</time> <time class="updated" datetime="2021-11-25T08:18:37.003Z" itemprop="dateUpdated">2021-11-25 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/25/bigdata-HBase2-framework/">HBase学习笔记（二） HBase架构解析</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="1-HBase架构解析"><a href="#1-HBase架构解析" class="headerlink" title="1 HBase架构解析"></a>1 HBase架构解析</h1><h2 id="1-1-RegionServer-架构"><a href="#1-1-RegionServer-架构" class="headerlink" title="1.1 RegionServer 架构"></a>1.1 RegionServer 架构</h2><p><img src="/2020/11/25/bigdata-HBase2-framework/1637810890299.png" alt="1637810890299"></p><p><strong>1）StoreFile</strong></p><p>保存实际数据的物理文件，<strong>StoreFile以Hfile的形式存储在HDFS上</strong>。每个Store会有一个或多个StoreFile（HFile），数据在每个StoreFile中都是有序的。</p><p><strong>2）MemStore</strong></p><p><strong>写缓存</strong>，由于HFile中的数据要求是有序的，所以<strong>数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile</strong>，每次刷写都会形成一个新的HFile。</p><p><strong>3）WAL</strong></p><p>由于数据要经MemStore排序后才能刷写到HFile，<strong>但把数据保存在内存中会有很高的概率导致数据丢失</strong>，为了解决这个问题，<strong>数据会先写在一个叫做Write-Ahead logfile的文件中</strong>，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</p><p><strong>4）BlockCache</strong></p><p><strong>读缓存</strong>，每次查询出的数据会缓存在BlockCache中，方便下次查询（LRU机制清理）。</p><h2 id="1-2-写流程"><a href="#1-2-写流程" class="headerlink" title="1.2 写流程"></a>1.2 写流程</h2><p><img src="/2020/11/25/bigdata-HBase2-framework/1637810932896.png" alt="1637810932896"></p><p>写流程：</p><p>1）Client先访问<strong>zookeeper</strong>，获取<strong>hbase:meta</strong>（存储业务表的元数据）表位于哪个Region Server。可以看到<strong>hbase:meta</strong>表存在hadoop104中。</p><p><img src="/2020/11/25/bigdata-HBase2-framework/1637825177104.png" alt="1637825177104"></p><p>2）访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。途中发现：该表是存储在hadoop103的（scan ‘student’）。</p><p><img src="/2020/11/25/bigdata-HBase2-framework/1637825241657.png" alt="1637825241657"></p><p>3）与目标Region Server进行通讯；</p><p>4）将数据顺序写入（追加）到WAL；</p><p>5）将数据写入对应的MemStore，数据会在MemStore进行排序；</p><p>6）向客户端发送ack；</p><p>7）等达到MemStore的刷写时机后，将数据刷写到HFile。</p><h2 id="1-3-MemStore-Flush"><a href="#1-3-MemStore-Flush" class="headerlink" title="1.3 MemStore Flush"></a>1.3 MemStore Flush</h2><p><img src="/2020/11/25/bigdata-HBase2-framework/1637810958068.png" alt="1637810958068"></p><p><strong>MemStore刷写时机：</strong></p><p>0）手动刷写 ：刷写’stu’表</p><pre class="line-numbers language-bash"><code class="language-bash">flush <span class="token string">'stu'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>1）当某个memstore的大小达到了<strong>hbase.hregion.memstore.flush.size（默认值128M）</strong>，其<strong>所在region的所有memstore都会刷写</strong>。</p><p>当memstore的大小达到了</p><pre class="line-numbers language-bash"><code class="language-bash">hbase.hregion.memstore.flush.size（默认值128M）* hbase.hregion.memstore.block.multiplier（默认值4）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>时，会<strong>阻止继续往</strong>该memstore写数据。</p><p>2）当region server中memstore的总大小达到</p><pre class="line-numbers language-bash"><code class="language-bash">java_heapsize*hbase.regionserver.global.memstore.size（默认值0.4）*hbase.regionserver.global.memstore.size.lower.limit（默认值0.95）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>region会按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有memstore的总大小减小到上述值以下。</p><p>当region server中memstore的总大小达到</p><pre class="line-numbers language-bash"><code class="language-bash">java_heapsize*hbase.regionserver.global.memstore.size（默认值0.4）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>时，会阻止继续往所有的memstore写数据。</p><ol start="3"><li>到达自动刷写的时间，也会触发memstore flush。自动刷新的时间间隔由该属性进行配置<strong>hbase.regionserver.optionalcacheflushinterval（默认1小时）</strong>。</li></ol><p>4)当WAL文件的数量超过hbase.regionserver.max.logs，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到hbase.regionserver.max.logs以下（该属性名已经废弃，现无需手动设置，最大值为32）。</p><h2 id="1-4-读流程"><a href="#1-4-读流程" class="headerlink" title="1.4 读流程"></a>1.4 读流程</h2><h3 id="1-4-1-整体流程"><a href="#1-4-1-整体流程" class="headerlink" title="1.4.1 整体流程"></a>1.4.1 整体流程</h3><p>和写的流程是一样的。</p><p><img src="/2020/11/25/bigdata-HBase2-framework/1637810986292.png" alt="1637810986292"></p><h3 id="1-4-2-Merge细节"><a href="#1-4-2-Merge细节" class="headerlink" title="1.4.2 Merge细节"></a>1.4.2 Merge细节</h3><p><img src="/2020/11/25/bigdata-HBase2-framework/1637811017103.png" alt="1637811017103"></p><p><strong>读流程</strong></p><p>1）Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。</p><p>2）访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</p><p>3）与目标Region Server进行通讯；</p><p>4）分别在MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。</p><p>5）将查询到的新的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。</p><p>6）将合并后的最终结果返回给客户端。</p><h2 id="1-5-StoreFile-Compaction"><a href="#1-5-StoreFile-Compaction" class="headerlink" title="1.5 StoreFile Compaction"></a>1.5 StoreFile Compaction</h2><p>由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。<strong>为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction</strong>。</p><p>Compaction分为两种，分别是<strong>Minor Compaction</strong>和<strong>Major Compaction</strong>。Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，并<strong>清理掉部分过期和删除的数据</strong>。Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，<strong>并且会清理掉所有过期和删除的数据。</strong></p><p><img src="/2020/11/25/bigdata-HBase2-framework/1637811135391.png" alt="1637811135391"></p><h2 id="1-6-Region-Split"><a href="#1-6-Region-Split" class="headerlink" title="1.6 Region Split"></a>1.6 Region Split</h2><p>默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。</p><p><strong>Region Split时机：</strong></p><p>1)当1个region中的某个Store下所有StoreFile的总大小超过hbase.hregion.max.filesize，该Region就会进行拆分（0.94版本之前）。</p><p>2)当1个region中的某个Store下所有StoreFile的总大小超过*<em>Min(initialSize x R^3 ,hbase.hregion.max.filesize”)**，该Region就会进行拆分。其中initialSize的默认值为2</em>hbase.hregion.memstore.flush.size，R为当前Region Server中属于该Table的Region个数（0.94版本之后）。</p><p>具体的切分策略为：</p><p>第一次split：1^3 * 256 = 256MB</p><p>第二次split：2^3 * 256 = 2048MB</p><p>第三次split：3^3 * 256 = 6912MB</p><p>第四次split：4^3 * 256 = 16384MB &gt; 10GB，因此取较小的值10GB</p><p>后面每次split的size都是10GB了。</p><p>3)Hbase 2.0引入了新的split策略：如果当前RegionServer上该表只有一个Region，按照2 * hbase.hregion.memstore.flush.size分裂，否则按照hbase.hregion.max.filesize分裂。</p><p><img src="/2020/11/25/bigdata-HBase2-framework/1637811167674.png" alt="1637811167674"></p><h1 id="2-HBase优化"><a href="#2-HBase优化" class="headerlink" title="2 HBase优化"></a>2 HBase优化</h1><h2 id="2-1-预分区-region"><a href="#2-1-预分区-region" class="headerlink" title="2.1 预分区(region)"></a>2.1 预分区(region)</h2><p>建表的时候就设计好几个分区。</p><p>每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。</p><p>1）手动设定预分区</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token operator">></span> create <span class="token string">'staff1'</span>,<span class="token string">'info'</span>,SPLITS <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token string">'1000'</span>,<span class="token string">'2000'</span>,<span class="token string">'3000'</span>,<span class="token string">'4000'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2)生成16进制序列预分区</p><pre class="line-numbers language-bash"><code class="language-bash">create <span class="token string">'staff2'</span>,<span class="token string">'info'</span>,<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'&amp;#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3)按照文件中设置的规则预分区</p><p>创建splits.txt文件内容如下：</p><pre class="line-numbers language-bash"><code class="language-bash">aaaa
bbbb
cccc
dddd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>然后执行：</p><pre class="line-numbers language-bash"><code class="language-bash">create <span class="token string">'staff3'</span>,<span class="token string">'info'</span>,SPLITS_FILE <span class="token operator">=</span><span class="token operator">></span> <span class="token string">'splits.txt'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4)使用JavaAPI创建预分区</p><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//自定义算法，产生一系列Hash散列值存储在二维数组中</span>
<span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">]</span> splitKeys <span class="token operator">=</span> 某个散列值函数
<span class="token comment" spellcheck="true">//创建HbaseAdmin实例</span>
HBaseAdmin hAdmin <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">HBaseAdmin</span><span class="token punctuation">(</span>HbaseConfiguration<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment" spellcheck="true">//创建HTableDescriptor实例</span>
HTableDescriptor tableDesc <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">HTableDescriptor</span><span class="token punctuation">(</span>tableName<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment" spellcheck="true">//通过HTableDescriptor实例和散列值二维数组创建带有预分区的Hbase表</span>
hAdmin<span class="token punctuation">.</span><span class="token function">createTable</span><span class="token punctuation">(</span>tableDesc<span class="token punctuation">,</span> splitKeys<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-2-RowKey设计"><a href="#2-2-RowKey设计" class="headerlink" title="2.2 RowKey设计"></a>2.2 RowKey设计</h2><p>一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈rowkey常用的设计方案。</p><p>1)生成随机数、hash、散列值</p><p>比如：<br>原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7<br>原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd<br>原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913<br>在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。</p><p>2)字符串反转</p><p>20170524000001转成10000042507102<br>20170524000002转成20000042507102</p><p>这样也可以在一定程度上散列逐步put进来的数据。</p><p>3)字符串拼接</p><p>20170524000001_a12e<br>20170524000001_93i7</p><p><strong>rowkey设计原则：</strong></p><p>唯一性 散列性 长度</p><p><img src="/2020/11/25/bigdata-HBase2-framework/1637828239660.png" alt="1637828239660"></p><p><img src="/2020/11/25/bigdata-HBase2-framework/1637828222514.png" alt="1637828222514"></p><p><img src="/2020/11/25/bigdata-HBase2-framework/1637828263210.png" alt="1637828263210"></p><h2 id="2-3-内存优化"><a href="#2-3-内存优化" class="headerlink" title="2.3 内存优化"></a>2.3 内存优化</h2><p>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~36G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。</p><h2 id="2-4-基础优化"><a href="#2-4-基础优化" class="headerlink" title="2.4 基础优化"></a>2.4 基础优化</h2><p>1)Zookeeper会话超时时间</p><p>hbase-site.xml</p><p>属性：zookeeper.session.timeout</p><p>解释：默认值为90000毫秒（90s）。当某个RegionServer挂掉，90s之后Master才能察觉到。可适当减小此值，以加快Master响应，可调整至60000毫秒。</p><p>2)设置RPC监听数量</p><p>hbase-site.xml</p><p>属性：zookeeper.session.timeout<br>解释：默认值为90000毫秒（90s）。当某个RegionServer挂掉，90s之后Master才能察觉到。可适当减小此值，以加快Master响应，可调整至60000毫秒。</p><p>3)手动控制Major Compaction</p><p>hbase-site.xml</p><p>属性：hbase.hregion.majorcompaction</p><p>解释：默认值：604800000秒（7天）， Major Compaction的周期，若关闭自动Major Compaction，可将其设为0</p><p>4)优化HStore文件大小</p><p>hbase-site.xml</p><p>属性：hbase.hregion.max.filesize<br>解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。</p><ol start="5"><li>优化HBase客户端缓存</li></ol><p>hbase-site.xml</p><p>属性：hbase.client.write.buffer<br>解释：默认值2097152bytes（2M）用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。</p><p>6)指定scan.next扫描HBase所获取的行数</p><p>hbase-site.xml</p><p>属性：hbase.client.scanner.caching<br>解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。</p><p>7)BlockCache占用RegionServer堆内存的比例</p><p>hbase-site.xml</p><p>属性：hfile.block.cache.size<br>解释：默认0.4，读请求比较多的情况下，可适当调大</p><p>8.MemStore占用RegionServer堆内存的比例</p><p>hbase-site.xml</p><p>属性：hbase.regionserver.global.memstore.size<br>解释：默认0.4，写请求较多的情况下，可适当调大</p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li></ul></div><span class="post-count">总字数2.5k</span> <span class="post-count">预计阅读9分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-HBase4-phoenix" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/25/bigdata-HBase4-phoenix/" class="article-date"><time class="published" datetime="2020-11-25T03:23:17.000Z" itemprop="datePublished">2020-11-25 发布</time> <time class="updated" datetime="2021-11-29T02:36:28.249Z" itemprop="dateUpdated">2021-11-29 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/25/bigdata-HBase4-phoenix/">HBase学习笔记（四） HBase整合phoenix和Hive</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="1-整合Phoenix"><a href="#1-整合Phoenix" class="headerlink" title="1 整合Phoenix"></a>1 整合Phoenix</h1><h2 id="1-1-Phoenix简介"><a href="#1-1-Phoenix简介" class="headerlink" title="1.1 Phoenix简介"></a>1.1 Phoenix简介</h2><h3 id="1-1-1-Phoenix定义"><a href="#1-1-1-Phoenix定义" class="headerlink" title="1.1.1 Phoenix定义"></a>1.1.1 Phoenix定义</h3><p><a target="_blank" rel="noopener" href="https://phoenix.apache.org/">Phoenix</a>是HBase的开源SQL皮肤。可以使用标准JDBC API代替HBase客户端API来创建表，插入数据和查询HBase数据。（Phoenix是HBase的JDBC类型客户端）。</p><p>可以把Phoenix理解成数据库，并且有事务，底层存储就是HBase。</p><p>Phoenix查询数据可以用sql查询的方式，Phoenix默认执行的语句到了hbase中都是大写的，除非使用双引号。</p><h3 id="1-1-2-Phoenix特点"><a href="#1-1-2-Phoenix特点" class="headerlink" title="1.1.2 Phoenix特点"></a>1.1.2 Phoenix特点</h3><p>1）容易集成：如Spark，Hive，Pig，Flume和Map Reduce；</p><p>2）操作简单：DML命令（CRUD）以及通过DDL（建库建表）命令创建和操作表和版本化增量更改；</p><p>3）支持HBase二级索引创建。</p><h3 id="1-1-3-Phoenix架构"><a href="#1-1-3-Phoenix架构" class="headerlink" title="1.1.3 Phoenix架构"></a>1.1.3 Phoenix架构</h3><p>Phoenix怎么和HBase去集成呢？</p><p><strong>（1）第一种方式：薄客户端：</strong>下图是Phoenix thin client，Phoenix thin client写Sql命令，Regionserver中的Phoenix Query server服务将其转化为Hbase语句。</p><p><strong>（2）第二种方式：厚客户端：</strong>Phoenix thick client将Phoenix Query server服务封装到本身的Phoenix 客户端里面。Phoenix 内部直接将SQL转化为Hbase语句。</p><p><img src="/2020/11/25/bigdata-HBase4-phoenix/1637834908636.png" alt="1637834908636"></p><h2 id="1-2-Phoenix快速入门"><a href="#1-2-Phoenix快速入门" class="headerlink" title="1.2 Phoenix快速入门"></a>1.2 Phoenix快速入门</h2><h3 id="1-2-1-安装"><a href="#1-2-1-安装" class="headerlink" title="1.2.1 安装"></a>1.2.1 安装</h3><p>1.官网地址</p><p><a target="_blank" rel="noopener" href="http://phoenix.apache.org/">http://phoenix.apache.org/</a></p><p>2.Phoenix部署</p><p>1）上传并解压tar包</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz -C /opt/module/
<span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ <span class="token function">mv</span> apache-phoenix-5.0.0-HBase-2.0-bin phoenix<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）复制Phoenix server包并拷贝到各个节点的hbase/lib：用于Phoenix 和连接hbase。分发后，重启hbase</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ <span class="token function">cd</span> /opt/module/phoenix/
<span class="token punctuation">[</span>molly@hadoop102 phoenix<span class="token punctuation">]</span>$ <span class="token function">cp</span> /opt/module/phoenix/phoenix-5.0.0-HBase-2.0-server.jar /opt/module/hbase/lib/
<span class="token punctuation">[</span>molly@hadoop102 phoenix<span class="token punctuation">]</span>$ xsync /opt/module/hbase/lib/phoenix-5.0.0-HBase-2.0-server.jar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>4）配置环境变量</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#phoenix</span>
<span class="token function">export</span> PHOENIX_HOME<span class="token operator">=</span>/opt/module/phoenix
<span class="token function">export</span> PHOENIX_CLASSPATH<span class="token operator">=</span><span class="token variable">$PHOENIX_HOME</span>
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$PHOENIX_HOME</span>/bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>5）重启HBase</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ stop-hbase.sh
<span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ start-hbase.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="6"><li>厚Phoenix连接Hbase：sqlline.py</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop101 phoenix<span class="token punctuation">]</span>$ /opt/module/phoenix/bin/sqlline.py hadoop102,hadoop103,hadoop104:2181<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/25/bigdata-HBase4-phoenix/1637835818060.png" alt="1637835818060"></p><ol start="7"><li>薄Phoenix连接Hbase：sqlline-thin.py</li></ol><p>需要先启用queryserver.py，再启用sqlline-thin.py指定去找queryserver（端口8765）。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop101 phoenix<span class="token punctuation">]</span>$ /opt/module/phoenix/bin/queryserver.py 
<span class="token punctuation">[</span>molly@hadoop101 phoenix<span class="token punctuation">]</span>$ /opt/module/phoenix/bin/sqlline-thin.py hadoop102：8765<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/2020/11/25/bigdata-HBase4-phoenix/1637836402464.png" alt="1637836402464"></p><h3 id="1-2-2-Phoenix-Shell操作"><a href="#1-2-2-Phoenix-Shell操作" class="headerlink" title="1.2.2 Phoenix Shell操作"></a>1.2.2 Phoenix Shell操作</h3><h4 id="1-2-2-1-schema的操作"><a href="#1-2-2-1-schema的操作" class="headerlink" title="1.2.2.1 schema的操作"></a>1.2.2.1 schema的操作</h4><p>1）创建schema(库)</p><p>默认情况下，在phoenix中不能直接创建schema。需要将如下的参数添加到Hbase中conf目录下的hbase-site.xml 和 phoenix中bin目录下的 hbase-site.xml中</p><pre class="line-numbers language-xml"><code class="language-xml">  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>phoenix.schema.isNamespaceMappingEnabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>重新启动Hbase和连接phoenix客户端.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ stop-hbase.sh
<span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ start-hbase.sh
<span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ /opt/module/phoenix/bin/sqlline.py hadoop102,hadoop103,hadoop104:2181<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>创建schema</p><pre class="line-numbers language-bash"><code class="language-bash">create schema bigdata<span class="token punctuation">;</span>
create schema <span class="token keyword">if</span> exists bigdata<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>注意:在phoenix中，schema名，表名，字段名等会自动转换为大写，若要小写，使用双引号，如”student”。</strong></p><h4 id="1-2-2-2-表的操作"><a href="#1-2-2-2-表的操作" class="headerlink" title="1.2.2.2 表的操作"></a>1.2.2.2 表的操作</h4><p>1）显示所有表</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">!</span>table 或 <span class="token operator">!</span>tables<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）创建表</p><p>直接指定单个列作为RowKey</p><pre class="line-numbers language-bash"><code class="language-bash">CREATE TABLE IF NOT EXISTS student<span class="token punctuation">(</span>
<span class="token function">id</span> VARCHAR primary key,
name VARCHAR,
addr VARCHAR<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>指定多个列的联合作为RowKey</p><p>3）插入数据</p><pre class="line-numbers language-bash"><code class="language-bash">upsert into student values<span class="token punctuation">(</span><span class="token string">'1001'</span>,<span class="token string">'zhangsan'</span>,<span class="token string">'beijing'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）查询记录</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token keyword">select</span> * from student<span class="token punctuation">;</span>
<span class="token keyword">select</span> * from student where id<span class="token operator">=</span><span class="token string">'1001'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>5）删除记录</p><pre class="line-numbers language-bash"><code class="language-bash">delete from student where id<span class="token operator">=</span><span class="token string">'1001'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>6）删除表</p><pre class="line-numbers language-bash"><code class="language-bash">drop table student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>7）退出命令行</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">!</span>quit<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="1-2-2-3-表的映射"><a href="#1-2-2-3-表的映射" class="headerlink" title="1.2.2.3 表的映射"></a>1.2.2.3 表的映射</h4><p>1）表的关系</p><p>默认情况下，直接在HBase中创建的表，通过Phoenix是查看不到的。如果要在Phoenix中操作直接在HBase中创建的表，则需要在Phoenix中进行表的映射。映射方式有两种：视图映射和表映射。</p><p>2）命令行中创建表test</p><p>HBase 中test的表结构如下，两个列族info1、info2。</p><table><thead><tr><th>Rowkey</th><th>info1</th><th>info2</th></tr></thead><tbody><tr><td>id</td><td>name</td><td>address</td></tr></tbody></table><p>启动HBase Shell</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ /opt/module/hbase/bin/hbase shell<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>创建HBase表test</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:001:0<span class="token operator">></span> create <span class="token string">'test'</span>,<span class="token string">'info1'</span>,<span class="token string">'info2'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）视图映射</p><p>Phoenix创建的视图是只读的，所以只能用来做查询，无法通过视图对源数据进行修改等操作。在phoenix中创建关联test表的视图</p><pre class="line-numbers language-bash"><code class="language-bash">0: jdbc:phoenix:hadoop101,hadoop102,hadoop103<span class="token operator">></span> create view <span class="token string">"test"</span><span class="token punctuation">(</span>id varchar primary key,<span class="token string">"info1"</span><span class="token keyword">.</span><span class="token string">"name"</span> varchar, <span class="token string">"info2"</span><span class="token keyword">.</span><span class="token string">"address"</span> varchar<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>删除视图</p><pre class="line-numbers language-bash"><code class="language-bash">0: jdbc:phoenix:hadoop101,hadoop102,hadoop103<span class="token operator">></span> drop view <span class="token string">"test"</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）表映射</p><p>使用Apache Phoenix创建对HBase的表映射，有两种方法：</p><p>（1）HBase中不存在表时，可以直接使用create table指令创建需要的表,系统将会自动在Phoenix和HBase中创建person_infomation的表，并会根据指令内的参数对表结构进行初始化。</p><p>（2）当HBase中已经存在表时，可以以类似创建视图的方式创建关联表，只需要将create view改为create table即可。</p><pre class="line-numbers language-bash"><code class="language-bash">0: jdbc:phoenix:hadoop101,hadoop102,hadoop103<span class="token operator">></span> create table <span class="token string">"test"</span><span class="token punctuation">(</span>id varchar primary key,<span class="token string">"info1"</span><span class="token keyword">.</span><span class="token string">"name"</span> varchar, <span class="token string">"info2"</span><span class="token keyword">.</span><span class="token string">"address"</span> varchar<span class="token punctuation">)</span> column_encoded_bytes<span class="token operator">=</span>0<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>1.2.2.4表映射中数值类型的问题</p><p>Hbase中存储数值类型的值(如int,long等)会按照正常数字的补码进行存储. 而phoenix对数字的存储做了特殊的处理. phoenix 为了解决遇到正负数同时存在时，导致负数排到了正数的后面（负数高位为1，正数高位为0，字典序0 &lt; 1）的问题。 phoenix在存储数字时会对高位进行转换.原来为1,转换为0， 原来为0，转换为1.</p><p>因此，如果hbase表中的数据的写是由phoenix写入的,不会出现问题，因为对数字的编解码都是phoenix来负责。如果hbase表中的数据不是由phoenix写入的，数字的编码由hbase负责. 而phoenix读数据时要对数字进行解码。 因为编解码方式不一致。导致数字出错.</p><p>1） 在hbase中创建表，并插入数值类型的数据</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:001:0<span class="token operator">></span> create <span class="token string">'person'</span>,<span class="token string">'info'</span>
hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:001:0<span class="token operator">></span> put <span class="token string">'person'</span>,<span class="token string">'1001'</span>, <span class="token string">'info:salary'</span>,Bytes.toBytes<span class="token punctuation">(</span>123456<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意: 如果要插入数字类型，需要通过Bytes.toBytes(123456)来实现。</p><p>2）在phoenix中创建映射表并查询数据</p><pre class="line-numbers language-bash"><code class="language-bash">create table <span class="token string">"person"</span><span class="token punctuation">(</span>id varchar primary key,<span class="token string">"info"</span><span class="token keyword">.</span><span class="token string">"salary"</span> integer <span class="token punctuation">)</span> 

column_encoded_bytes<span class="token operator">=</span>0

<span class="token keyword">select</span> * from <span class="token string">"person"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>会发现数字显示有问题</p><p>3） 解决办法:</p><p>在phoenix中创建表时使用无符号的数值类型. unsigned_long</p><pre class="line-numbers language-bash"><code class="language-bash">create table <span class="token string">"person"</span><span class="token punctuation">(</span>id varchar primary key,<span class="token string">"info"</span><span class="token keyword">.</span><span class="token string">"salary"</span> unsigned_long <span class="token punctuation">)</span> 
column_encoded_bytes<span class="token operator">=</span>0<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="1-2-3-Phoenix-JDBC-API操作"><a href="#1-2-3-Phoenix-JDBC-API操作" class="headerlink" title="1.2.3 Phoenix JDBC API操作"></a>1.2.3 Phoenix JDBC API操作</h3><h4 id="1-2-3-1-Thin-Client"><a href="#1-2-3-1-Thin-Client" class="headerlink" title="1.2.3.1 Thin Client"></a>1.2.3.1 Thin Client</h4><p>1）启动query server</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ queryserver.py start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）创建项目并导入依赖</p><pre class="line-numbers language-xml"><code class="language-xml"> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
​      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.phoenix<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
​      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>phoenix-queryserver-client<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
​      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>5.0.0-HBase-2.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）编写代码</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>atguigu<span class="token punctuation">;</span>

<span class="token keyword">import</span> java<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>*<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>phoenix<span class="token punctuation">.</span>queryserver<span class="token punctuation">.</span>client<span class="token punctuation">.</span>ThinClientUtil<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">PhoenixTest</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
<span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> SQLException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        String connectionUrl <span class="token operator">=</span> ThinClientUtil<span class="token punctuation">.</span><span class="token function">getConnectionUrl</span><span class="token punctuation">(</span><span class="token string">"hadoop102"</span><span class="token punctuation">,</span> <span class="token number">8765</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        Connection connection <span class="token operator">=</span> DriverManager<span class="token punctuation">.</span><span class="token function">getConnection</span><span class="token punctuation">(</span>connectionUrl<span class="token punctuation">)</span><span class="token punctuation">;</span>
        PreparedStatement preparedStatement <span class="token operator">=</span> connection<span class="token punctuation">.</span><span class="token function">prepareStatement</span><span class="token punctuation">(</span><span class="token string">"select * from student"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        ResultSet resultSet <span class="token operator">=</span> preparedStatement<span class="token punctuation">.</span><span class="token function">executeQuery</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token keyword">while</span> <span class="token punctuation">(</span>resultSet<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>resultSet<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\t"</span> <span class="token operator">+</span> resultSet<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//关闭</span>
        connection<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="1-2-3-2-Thick-Client"><a href="#1-2-3-2-Thick-Client" class="headerlink" title="1.2.3.2 Thick Client"></a>1.2.3.2 Thick Client</h4><p>1）在pom中加入依赖</p><pre class="line-numbers language-xml"><code class="language-xml"> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.phoenix<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>phoenix-core<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>5.0.0-HBase-2.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclusions</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>exclusion</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.glassfish<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>javax.el<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclusion</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>exclusions</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>

        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.glassfish<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>javax.el<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.0.1-b06<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）编写代码</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>atguigu<span class="token punctuation">.</span>phoenix<span class="token punctuation">.</span>thin<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>*<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Properties<span class="token punctuation">;</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">TestThick</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> SQLException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        String url <span class="token operator">=</span> <span class="token string">"jdbc:phoenix:hadoop102,hadoop103,hadoop104:2181"</span><span class="token punctuation">;</span>
        Properties props <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        props<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">"phoenix.schema.isNamespaceMappingEnabled"</span><span class="token punctuation">,</span><span class="token string">"true"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        Connection connection <span class="token operator">=</span> DriverManager<span class="token punctuation">.</span><span class="token function">getConnection</span><span class="token punctuation">(</span>url<span class="token punctuation">,</span>props<span class="token punctuation">)</span><span class="token punctuation">;</span>
        PreparedStatement ps <span class="token operator">=</span> connection<span class="token punctuation">.</span><span class="token function">prepareStatement</span><span class="token punctuation">(</span><span class="token string">"select * from \"test\""</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        ResultSet rs <span class="token operator">=</span> ps<span class="token punctuation">.</span><span class="token function">executeQuery</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">while</span><span class="token punctuation">(</span>rs<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>rs<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">":"</span> <span class="token operator">+</span>rs<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="1-3-Phoenix二级索引"><a href="#1-3-Phoenix二级索引" class="headerlink" title="1.3 Phoenix二级索引"></a>1.3 Phoenix二级索引</h2><h3 id="1-3-1-二级索引配置文件"><a href="#1-3-1-二级索引配置文件" class="headerlink" title="1.3.1 二级索引配置文件"></a>1.3.1 二级索引配置文件</h3><p>1.添加如下配置到HBase的HRegionserver节点的hbase-site.xml</p><pre class="line-numbers language-xml"><code class="language-xml">  <span class="token comment" spellcheck="true">&lt;!-- phoenix regionserver 配置参数--></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.regionserver.wal.codec<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.region.server.rpc.scheduler.factory.class<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.rpc.controllerfactory.class<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-3-2-全局二级索引"><a href="#1-3-2-全局二级索引" class="headerlink" title="1.3.2 全局二级索引"></a>1.3.2 全局二级索引</h3><p>Global Index是默认的索引格式，创建全局索引时，会在HBase中建立一张新表。也就是说索引数据和数据表是存放在不同的表中的，因此全局索引适用于多读少写的业务场景。</p><p>写数据的时候会消耗大量开销，因为索引表也要更新，而索引表是分布在不同的数据节点上的，跨节点的数据传输带来了较大的性能消耗。</p><p>在读数据的时候Phoenix会选择索引表来降低查询消耗的时间。</p><ol><li>创建单个字段的全局索引</li></ol><pre class="line-numbers language-bash"><code class="language-bash">CREATE INDEX my_index ON my_table <span class="token punctuation">(</span>my_col<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/25/bigdata-HBase4-phoenix/1637837037624.png" alt="1637837037624"></p><p>如果想查询的字段不是索引字段的话索引表不会被使用，也就是说不会带来查询速度的提升。</p><p><img src="/2020/11/25/bigdata-HBase4-phoenix/1637837064704.png"></p><p>2）创建携带其他字段的全局索引</p><pre class="line-numbers language-bash"><code class="language-bash">CREATE INDEX my_index ON my_table <span class="token punctuation">(</span>v1<span class="token punctuation">)</span> INCLUDE <span class="token punctuation">(</span>v2<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/25/bigdata-HBase4-phoenix/1637837124230.png" alt="1637837124230"></p><h3 id="1-3-3-本地二级索引"><a href="#1-3-3-本地二级索引" class="headerlink" title="1.3.3 本地二级索引"></a>1.3.3 本地二级索引</h3><p>Local Index适用于写操作频繁的场景。</p><p>索引数据和数据表的数据是存放在同一张表中（且是同一个Region），避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销。</p><pre class="line-numbers language-bash"><code class="language-bash">CREATE LOCAL INDEX my_index ON my_table <span class="token punctuation">(</span>my_column<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/25/bigdata-HBase4-phoenix/1637837158438.png" alt="1637837158438"></p><h1 id="2-与Hive的集成"><a href="#2-与Hive的集成" class="headerlink" title="2 与Hive的集成"></a>2 与Hive的集成</h1><h2 id="2-1-HBase与Hive的对比"><a href="#2-1-HBase与Hive的对比" class="headerlink" title="2.1 HBase与Hive的对比"></a>2.1 HBase与Hive的对比</h2><h3 id="2-1-1-Hive"><a href="#2-1-1-Hive" class="headerlink" title="2.1.1 Hive"></a>2.1.1 Hive</h3><p>(1) 数据分析工具</p><p>Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。</p><p>(2) 用于数据分析、清洗</p><p>Hive适用于离线的数据分析和清洗，延迟较高。</p><p>(3) 基于HDFS、MapReduce</p><p>Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。</p><h3 id="2-1-2-HBase"><a href="#2-1-2-HBase" class="headerlink" title="2.1.2 HBase"></a>2.1.2 HBase</h3><p>(1) 数据库</p><p>是一种面向列族存储的非关系型数据库。</p><p>(2) 用于存储结构化和非结构化的数据</p><p>适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</p><p>(3) 基于HDFS</p><p>数据持久化存储的体现形式是HFile，存放于DataNode中，被ResionServer以region的形式进行管理。</p><p>(4) 延迟较低，接入在线业务使用</p><p>面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。</p><h2 id="2-2-HBase与Hive集成使用"><a href="#2-2-HBase与Hive集成使用" class="headerlink" title="2.2 HBase与Hive集成使用"></a>2.2 HBase与Hive集成使用</h2><p>在hive-site.xml中添加zookeeper的属性，如下：</p><pre class="line-numbers language-xml"><code class="language-xml">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.zookeeper.quorum<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop102,hadoop103,hadoop104<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.zookeeper.client.port<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>2181<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>案例一</p><p>目标：建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表。</p><p>分步实现：</p><p>1.在Hive中创建表同时关联HBase</p><pre class="line-numbers language-bash"><code class="language-bash">CREATE TABLE hive_hbase_emp_table<span class="token punctuation">(</span>
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
<span class="token function">comm</span> double,
deptno int<span class="token punctuation">)</span>
STORED BY <span class="token string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span>
WITH SERDEPROPERTIES <span class="token punctuation">(</span><span class="token string">"hbase.columns.mapping"</span> <span class="token operator">=</span> <span class="token string">":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno"</span><span class="token punctuation">)</span>
TBLPROPERTIES <span class="token punctuation">(</span><span class="token string">"hbase.table.name"</span> <span class="token operator">=</span> <span class="token string">"hbase_emp_table"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>提示：完成之后，可以分别进入Hive和HBase查看，都生成了对应的表</p><p>2.在Hive中创建临时中间表，用于load文件中的数据</p><p>提示：不能将数据直接load进Hive所关联HBase的那张表中</p><pre class="line-numbers language-bash"><code class="language-bash">CREATE TABLE emp<span class="token punctuation">(</span>
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
<span class="token function">comm</span> double,
deptno int<span class="token punctuation">)</span>
row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3.向Hive中间表中load数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> load data local inpath <span class="token string">'/home/admin/softwares/data/emp.txt'</span> into table emp<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4.通过insert命令将中间表中的数据导入到Hive关联Hbase的那张表中</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> insert into table hive_hbase_emp_table <span class="token keyword">select</span> * from emp<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5.查看Hive以及关联的HBase表中是否已经成功的同步插入了数据<br>Hive：</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> <span class="token keyword">select</span> * from hive_hbase_emp_table<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>HBase：</p><pre class="line-numbers language-bash"><code class="language-bash">Hbase<span class="token operator">></span> scan <span class="token string">'hbase_emp_table'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>案例二</p><p>目标：在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。</p><p>注：该案例2紧跟案例1的脚步，所以完成此案例前，请先完成案例1。</p><p>分步实现：</p><p>1.在Hive中创建外部表</p><pre class="line-numbers language-bash"><code class="language-bash">CREATE EXTERNAL TABLE relevance_hbase_emp<span class="token punctuation">(</span>
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
<span class="token function">comm</span> double,
deptno int<span class="token punctuation">)</span>
STORED BY 
<span class="token string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span>
WITH SERDEPROPERTIES <span class="token punctuation">(</span><span class="token string">"hbase.columns.mapping"</span> <span class="token operator">=</span> 
<span class="token string">":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno"</span><span class="token punctuation">)</span> 
TBLPROPERTIES <span class="token punctuation">(</span><span class="token string">"hbase.table.name"</span> <span class="token operator">=</span> <span class="token string">"hbase_emp_table"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2.关联后就可以使用Hive函数进行一些分析操作了</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from relevance_hbase_emp<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li></ul></div><span class="post-count">总字数3k</span> <span class="post-count">预计阅读13分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-HBase3-API" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/25/bigdata-HBase3-API/" class="article-date"><time class="published" datetime="2020-11-25T03:22:51.000Z" itemprop="datePublished">2020-11-25 发布</time> <time class="updated" datetime="2021-11-25T09:05:31.708Z" itemprop="dateUpdated">2021-11-25 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/25/bigdata-HBase3-API/">HBase学习笔记（三） HBase的API使用</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1 环境准备"></a>1 环境准备</h2><p>新建项目后在pom.xml中添加依赖</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.hbase<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>hbase-server<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.0.5<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.hbase<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>hbase-client<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.0.5<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>API官方手册：</p><p><a target="_blank" rel="noopener" href="https://hbase.apache.org/2.2/apidocs/index.html">https://hbase.apache.org/2.2/apidocs/index.html</a></p><h2 id="2-DDL"><a href="#2-DDL" class="headerlink" title="2 DDL"></a>2 DDL</h2><p>创建HBase_DDL类</p><h3 id="2-1-判断表是否存在和创建命名空间"><a href="#2-1-判断表是否存在和创建命名空间" class="headerlink" title="2.1 判断表是否存在和创建命名空间"></a>2.1 判断表是否存在和创建命名空间</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>conf<span class="token punctuation">.</span>Configuration<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>HBaseConfiguration<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>NamespaceDescriptor<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>NamespaceExistException<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>TableName<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>client<span class="token punctuation">.</span>*<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Bytes<span class="token punctuation">;</span>

<span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">HBase_DDL</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">//TODO 判断表是否存在</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">boolean</span> <span class="token function">isTableExist</span><span class="token punctuation">(</span>String tableName<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//1.创建配置信息并配置</span>
        Configuration configuration <span class="token operator">=</span> HBaseConfiguration<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"hbase.zookeeper.quorum"</span><span class="token punctuation">,</span> <span class="token string">"hadoop102,hadoop103,hadoop104"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//2.获取与HBase的连接</span>
        Connection connection <span class="token operator">=</span> ConnectionFactory<span class="token punctuation">.</span><span class="token function">createConnection</span><span class="token punctuation">(</span>configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//3.获取DDL操作对象</span>
        Admin admin <span class="token operator">=</span> connection<span class="token punctuation">.</span><span class="token function">getAdmin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//4.1判断表是否存在操作</span>
        <span class="token keyword">boolean</span> exists <span class="token operator">=</span> admin<span class="token punctuation">.</span><span class="token function">tableExists</span><span class="token punctuation">(</span>TableName<span class="token punctuation">.</span><span class="token function">valueOf</span><span class="token punctuation">(</span>tableName<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//4.2 创建命名空间</span>
         <span class="token comment" spellcheck="true">//42.1创建命名空间描述器</span>
        NamespaceDescriptor namespaceDescriptor <span class="token operator">=</span> NamespaceDescriptor<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span>ns<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">//4.2.2执行创建命名空间操作</span>
        <span class="token keyword">try</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            admin<span class="token punctuation">.</span><span class="token function">createNamespace</span><span class="token punctuation">(</span>namespaceDescriptor<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">NamespaceExistException</span> e<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"命名空间已存在！"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">Exception</span> e<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            e<span class="token punctuation">.</span><span class="token function">printStackTrace</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//5.关闭连接</span>
        admin<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        connection<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//6.返回结果</span>
        <span class="token keyword">return</span> exists<span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-2-创建表"><a href="#2-2-创建表" class="headerlink" title="2.2 创建表"></a>2.2 创建表</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>conf<span class="token punctuation">.</span>Configuration<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>HBaseConfiguration<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>NamespaceDescriptor<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>NamespaceExistException<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>TableName<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>client<span class="token punctuation">.</span>*<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Bytes<span class="token punctuation">;</span>

<span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">HBase_DDL</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">//TODO 创建表</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">createTable</span><span class="token punctuation">(</span>String tableName<span class="token punctuation">,</span> String<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> cfs<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//1.判断是否存在列族信息</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>cfs<span class="token punctuation">.</span>length <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"请设置列族信息！"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//2.判断表是否存在</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">isTableExist</span><span class="token punctuation">(</span>tableName<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"需要创建的表已存在！"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//3.创建配置信息并配置</span>
        Configuration configuration <span class="token operator">=</span> HBaseConfiguration<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"hbase.zookeeper.quorum"</span><span class="token punctuation">,</span> <span class="token string">"hadoop102,hadoop103,hadoop104"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//4.获取与HBase的连接</span>
        Connection connection <span class="token operator">=</span> ConnectionFactory<span class="token punctuation">.</span><span class="token function">createConnection</span><span class="token punctuation">(</span>configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//5.获取DDL操作对象</span>
        Admin admin <span class="token operator">=</span> connection<span class="token punctuation">.</span><span class="token function">getAdmin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//6.创建表描述器构造器</span>
        TableDescriptorBuilder tableDescriptorBuilder <span class="token operator">=</span> TableDescriptorBuilder<span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span>TableName<span class="token punctuation">.</span><span class="token function">valueOf</span><span class="token punctuation">(</span>tableName<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//7.循环添加列族信息</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>String cf <span class="token operator">:</span> cfs<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            ColumnFamilyDescriptorBuilder columnFamilyDescriptorBuilder <span class="token operator">=</span> ColumnFamilyDescriptorBuilder<span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span>Bytes<span class="token punctuation">.</span><span class="token function">toBytes</span><span class="token punctuation">(</span>cf<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            tableDescriptorBuilder<span class="token punctuation">.</span><span class="token function">setColumnFamily</span><span class="token punctuation">(</span>columnFamilyDescriptorBuilder<span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//8.执行创建表的操作</span>
        admin<span class="token punctuation">.</span><span class="token function">createTable</span><span class="token punctuation">(</span>tableDescriptorBuilder<span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//9.关闭资源</span>
        admin<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        connection<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-3-删除表"><a href="#2-3-删除表" class="headerlink" title="2.3 删除表"></a>2.3 删除表</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>conf<span class="token punctuation">.</span>Configuration<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>HBaseConfiguration<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>NamespaceDescriptor<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>NamespaceExistException<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>TableName<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>client<span class="token punctuation">.</span>*<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Bytes<span class="token punctuation">;</span>

<span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">HBase_DDL</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">//TODO 删除表</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">dropTable</span><span class="token punctuation">(</span>String tableName<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//1.判断表是否存在</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">isTableExist</span><span class="token punctuation">(</span>tableName<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"需要删除的表不存在！"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//2.创建配置信息并配置</span>
        Configuration configuration <span class="token operator">=</span> HBaseConfiguration<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"hbase.zookeeper.quorum"</span><span class="token punctuation">,</span> <span class="token string">"hadoop102,hadoop103,hadoop104"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//3.获取与HBase的连接</span>
        Connection connection <span class="token operator">=</span> ConnectionFactory<span class="token punctuation">.</span><span class="token function">createConnection</span><span class="token punctuation">(</span>configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//4.获取DDL操作对象</span>
        Admin admin <span class="token operator">=</span> connection<span class="token punctuation">.</span><span class="token function">getAdmin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//5.使表下线</span>
        TableName name <span class="token operator">=</span> TableName<span class="token punctuation">.</span><span class="token function">valueOf</span><span class="token punctuation">(</span>tableName<span class="token punctuation">)</span><span class="token punctuation">;</span>
        admin<span class="token punctuation">.</span><span class="token function">disableTable</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//6.执行删除表操作</span>
        admin<span class="token punctuation">.</span><span class="token function">deleteTable</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//7.关闭资源</span>
        admin<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        connection<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-DML"><a href="#3-DML" class="headerlink" title="3 DML"></a>3 DML</h2><p>创建类HBase_DML</p><p>（1） 插入数据</p><p>（2） 单条数据查询</p><p>（3） 扫描数据</p><p>（4） 删除数据</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>conf<span class="token punctuation">.</span>Configuration<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>Cell<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>HBaseConfiguration<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>TableName<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>client<span class="token punctuation">.</span>*<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hbase<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Bytes<span class="token punctuation">;</span>

<span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">HBase_DML</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">//TODO 插入数据</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">putData</span><span class="token punctuation">(</span>String tableName<span class="token punctuation">,</span> String rowKey<span class="token punctuation">,</span> String cf<span class="token punctuation">,</span> String cn<span class="token punctuation">,</span> String value<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//1.获取配置信息并设置连接参数</span>
        Configuration configuration <span class="token operator">=</span> HBaseConfiguration<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"hbase.zookeeper.quorum"</span><span class="token punctuation">,</span> <span class="token string">"hadoop102,hadoop103,hadoop104"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//2.获取连接</span>
        Connection connection <span class="token operator">=</span> ConnectionFactory<span class="token punctuation">.</span><span class="token function">createConnection</span><span class="token punctuation">(</span>configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//3.获取表的连接</span>
        Table table <span class="token operator">=</span> connection<span class="token punctuation">.</span><span class="token function">getTable</span><span class="token punctuation">(</span>TableName<span class="token punctuation">.</span><span class="token function">valueOf</span><span class="token punctuation">(</span>tableName<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment" spellcheck="true">//4.1 插入数据</span>
        <span class="token comment" spellcheck="true">//4.1.1 创建Put对象</span>
        Put put <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Put</span><span class="token punctuation">(</span>Bytes<span class="token punctuation">.</span><span class="token function">toBytes</span><span class="token punctuation">(</span>rowKey<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">//4.1.2 放入数据</span>
        put<span class="token punctuation">.</span><span class="token function">addColumn</span><span class="token punctuation">(</span>Bytes<span class="token punctuation">.</span><span class="token function">toBytes</span><span class="token punctuation">(</span>cf<span class="token punctuation">)</span><span class="token punctuation">,</span> Bytes<span class="token punctuation">.</span><span class="token function">toBytes</span><span class="token punctuation">(</span>cn<span class="token punctuation">)</span><span class="token punctuation">,</span> Bytes<span class="token punctuation">.</span><span class="token function">toBytes</span><span class="token punctuation">(</span>value<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">//4.1.3 执行插入数据操作</span>
        table<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span>put<span class="token punctuation">)</span><span class="token punctuation">;</span>
     <span class="token comment" spellcheck="true">//4.2 单条数据查询</span>
          <span class="token comment" spellcheck="true">//4.2.1 创建Get对象</span>
        Get get <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Get</span><span class="token punctuation">(</span>Bytes<span class="token punctuation">.</span><span class="token function">toBytes</span><span class="token punctuation">(</span>rowKey<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">// 指定列族查询</span>
        <span class="token comment" spellcheck="true">// get.addFamily(Bytes.toBytes(cf));</span>
        <span class="token comment" spellcheck="true">// 指定列族:列查询</span>
        <span class="token comment" spellcheck="true">// get.addColumn(Bytes.toBytes(cf), Bytes.toBytes(cn));</span>
        <span class="token comment" spellcheck="true">//4.2.2查询数据</span>
        Result result <span class="token operator">=</span> table<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>get<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">//4.2.3解析result</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>Cell cell <span class="token operator">:</span> result<span class="token punctuation">.</span><span class="token function">rawCells</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"ROW:"</span> <span class="token operator">+</span> Bytes<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span>CellUtil<span class="token punctuation">.</span><span class="token function">cloneRow</span><span class="token punctuation">(</span>cell<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span>
                        <span class="token string">" CF:"</span> <span class="token operator">+</span> Bytes<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span>CellUtil<span class="token punctuation">.</span><span class="token function">cloneFamily</span><span class="token punctuation">(</span>cell<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">+</span>
                        <span class="token string">" CL:"</span> <span class="token operator">+</span> Bytes<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span>CellUtil<span class="token punctuation">.</span><span class="token function">cloneQualifier</span><span class="token punctuation">(</span>cell<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">+</span>
                        <span class="token string">" VALUE:"</span> <span class="token operator">+</span> Bytes<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span>CellUtil<span class="token punctuation">.</span><span class="token function">cloneValue</span><span class="token punctuation">(</span>cell<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

 <span class="token comment" spellcheck="true">//4.3 scan数据</span>
        <span class="token comment" spellcheck="true">//4.3.1创建Scan对象</span>
        Scan scan <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Scan</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">//4.3.2扫描数据</span>
        ResultScanner results <span class="token operator">=</span> table<span class="token punctuation">.</span><span class="token function">getScanner</span><span class="token punctuation">(</span>scan<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//4.3.3 解析results</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>Result result <span class="token operator">:</span> results<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span>Cell cell <span class="token operator">:</span> result<span class="token punctuation">.</span><span class="token function">rawCells</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
      System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>                       Bytes<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span>CellUtil<span class="token punctuation">.</span><span class="token function">cloneRow</span><span class="token punctuation">(</span>cell<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">":"</span><span class="token operator">+</span> Bytes<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span>CellUtil<span class="token punctuation">.</span><span class="token function">cloneFamily</span><span class="token punctuation">(</span>cell<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">":"</span> <span class="token operator">+</span>  Bytes<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span>CellUtil<span class="token punctuation">.</span><span class="token function">cloneQualifier</span><span class="token punctuation">(</span>cell<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span><span class="token string">":"</span> <span class="token operator">+</span>  Bytes<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span>CellUtil<span class="token punctuation">.</span><span class="token function">cloneValue</span><span class="token punctuation">(</span>cell<span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
 <span class="token comment" spellcheck="true">//4.4 删除数据</span>
         <span class="token comment" spellcheck="true">//4.4.1创建Delete对象</span>
        Delete delete <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Delete</span><span class="token punctuation">(</span>Bytes<span class="token punctuation">.</span><span class="token function">toBytes</span><span class="token punctuation">(</span>rowKey<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 指定列族删除数据</span>
        <span class="token comment" spellcheck="true">// delete.addFamily(Bytes.toBytes(cf));</span>
        <span class="token comment" spellcheck="true">// 指定列族:列删除数据(所有版本)</span>
        <span class="token comment" spellcheck="true">// delete.addColumn(Bytes.toBytes(cf), Bytes.toBytes(cn));</span>
        <span class="token comment" spellcheck="true">// 指定列族:列删除数据(指定版本)</span>
        <span class="token comment" spellcheck="true">// delete.addColumns(Bytes.toBytes(cf), Bytes.toBytes(cn));</span>
        <span class="token comment" spellcheck="true">//4.4.2执行删除数据操作</span>
        table<span class="token punctuation">.</span><span class="token function">delete</span><span class="token punctuation">(</span>delete<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">//5.关闭连接</span>
        table<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        connection<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li></ul></div><span class="post-count">总字数1.2k</span> <span class="post-count">预计阅读6分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-HBase1-setup" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/25/bigdata-HBase1-setup/" class="article-date"><time class="published" datetime="2020-11-24T22:46:51.000Z" itemprop="datePublished">2020-11-25 发布</time> <time class="updated" datetime="2021-11-25T07:12:51.979Z" itemprop="dateUpdated">2021-11-25 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/25/bigdata-HBase1-setup/">HBase学习笔记（一） 安装教程</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="1-HBase简介"><a href="#1-HBase简介" class="headerlink" title="1 HBase简介"></a>1 HBase简介</h1><h2 id="1-1-HBase定义"><a href="#1-1-HBase定义" class="headerlink" title="1.1 HBase定义"></a>1.1 HBase定义</h2><p><a target="_blank" rel="noopener" href="https://hbase.apache.org/">HBase</a>是一种分布式、可扩展、支持海量数据存储的<strong>NoSQL</strong>数据库。HBase底层是HDFS。用于实时处理场景。</p><h2 id="1-2-HBase数据模型"><a href="#1-2-HBase数据模型" class="headerlink" title="1.2 HBase数据模型"></a>1.2 HBase数据模型</h2><p>逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的底层物理存储结构（K-V）来看，HBase更像是一个<strong>multi-dimensional map</strong>。</p><h3 id="1-2-1-HBase逻辑结构"><a href="#1-2-1-HBase逻辑结构" class="headerlink" title="1.2.1 HBase逻辑结构"></a>1.2.1 HBase逻辑结构</h3><p>再HBase中列不能单独存在，必须属于某个列族。一个列族有多个列。</p><p>Row key:行键 唯一性 自动排序（字典排序）。</p><p>region:分区，表横向切开 去维护。一个Region是存到一起的，即存到同一个机器中。</p><p>store： 按照列族去切割，一个store存到一个文件中。</p><p>​ <img src="/2020/11/25/bigdata-HBase1-setup/1637809034793.png" alt="1637809034793"></p><h3 id="1-2-2-HBase物理存储结构（底层真实存储）"><a href="#1-2-2-HBase物理存储结构（底层真实存储）" class="headerlink" title="1.2.2 HBase物理存储结构（底层真实存储）"></a>1.2.2 HBase物理存储结构（底层真实存储）</h3><p>物理存储（真实存储的结构）其实是以KV进行存储，并存储在HDFS中，例如第一条数据：</p><p><img src="/2020/11/25/bigdata-HBase1-setup/1637821581208.png" alt="1637821581208"></p><p>通过时间戳TimeStamp去标识不同版本，因为HBase不能修改，因此，修改一条数据就是通过追加一条新数据，但是加一个新的时间戳。因此显示的时候，显示时间戳最大的数据即可。过期的数据会按照一定的机制去删除。</p><p>因此Hbase修改和新增的Type都是PUT。删除的Type是deletecolumn。</p><p><img src="/2020/11/25/bigdata-HBase1-setup/1637809177548.png" alt="1637809177548"></p><h3 id="1-2-3-数据模型"><a href="#1-2-3-数据模型" class="headerlink" title="1.2.3 数据模型"></a>1.2.3 数据模型</h3><p><strong>1）Name Space</strong></p><p>命名空间，类似于关系型<strong>数据库的database</strong>概念，每个命名空间下有多个表。HBase有两个自带的命名空间，分别是<strong>hbase和default</strong>，<strong>hbase中存放的是HBase内置的表，default表是用户默认使用的命名空间</strong>。</p><p><strong>2）Table</strong></p><p>类似于<strong>关系型数据库的表</strong>概念。不同的是，<strong>HBase定义表时只需要声明列族即可，不需要声明具体的列</strong>。这意味着，往HBase写入数据时，<strong>字段可以动态、按需指定</strong>。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景。</p><p><strong>3）Row</strong></p><p>HBase表中的每行数据都由一个RowKey和多个Column（列）组成，数据是按照RowKey的<strong>字典顺序</strong>存储的，并且<strong>查询数据时只能根据RowKey进行检索</strong>，所以RowKey的设计十分重要（<strong>如果根据其他列去查询例如名字，就会全表扫描，因此效率很低，通常不会使用</strong>）。</p><p><strong>4）Column</strong></p><p>HBase中的每个列都由<strong>Column Family(列族)**和</strong>Column Qualifier（列限定符）**进行限定，例如info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义。</p><p><strong>5）Time Stamp</strong></p><p><strong>用于标识数据的不同版本</strong>（version），每条数据写入时，系统会自动为其加上该字段，其值为写入HBase的时间。</p><p><strong>6）Cell</strong></p><p>由{rowkey, column Family：column Qualifier, time Stamp} 唯一确定的单元。cell中的数据全部是字节码形式存贮。</p><h2 id="1-3-HBase基本架构（不完整版本）"><a href="#1-3-HBase基本架构（不完整版本）" class="headerlink" title="1.3 HBase基本架构（不完整版本）"></a>1.3 HBase基本架构（不完整版本）</h2><p>架构角色：</p><p><strong>1）Region Server</strong></p><p>Region Server为 Region的管理者，其实现类为HRegionServer，主要作用如下:</p><p>对于数据的操作：get, put, delete；（查询修改删除）</p><p>对于Region的操作：splitRegion（切分）、compactRegion（合并storeFile）。</p><p><strong>2）Master</strong></p><p>Master是所有<strong>Region Server</strong>的管理者，其实现类为HMaster，主要作用如下：</p><p>对于表的操作：create, delete, alter（涉及元数据的管理）</p><p>对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。</p><p><strong>3）Zookeeper</strong></p><p>HBase通过Zookeeper来做master的高可用、<strong>RegionServer的监控</strong>、元数据的入口以及集群配置的维护等工作。</p><p><strong>4）HDFS</strong></p><p>HDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。</p><p><img src="/2020/11/25/bigdata-HBase1-setup/1637809993409.png" alt="1637809993409"></p><h1 id="2-HBase安装部署"><a href="#2-HBase安装部署" class="headerlink" title="2 HBase安装部署"></a>2 HBase安装部署</h1><h2 id="2-1-HBase的解压"><a href="#2-1-HBase的解压" class="headerlink" title="2.1 HBase的解压"></a>2.1 HBase的解压</h2><p>解压Hbase到指定目录：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf hbase-2.0.5-bin.tar.gz -C /opt/module
<span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">mv</span> /opt/module/hbase-2.0.5 /opt/module/hbase<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>配置环境变量</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> vim /etc/profile.d/my_env.sh
<span class="token comment" spellcheck="true">#添加</span>
<span class="token comment" spellcheck="true">#HBASE_HOME</span>
<span class="token function">export</span> HBASE_HOME<span class="token operator">=</span>/opt/module/hbase
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$HBASE_HOME</span>/bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-2-HBase的配置文件"><a href="#2-2-HBase的配置文件" class="headerlink" title="2.2 HBase的配置文件"></a>2.2 HBase的配置文件</h2><p>修改HBase对应的配置文件。</p><p>1）hbase-env.sh修改内容：</p><p>不要使用内置的ZK。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">export</span> HBASE_MANAGES_ZK<span class="token operator">=</span>false<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）hbase-site.xml修改内容：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.rootdir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://hadoop102:8020/hbase<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.cluster.distributed<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.zookeeper.quorum<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop102,hadoop103,hadoop104<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）regionservers：类似于workers</p><pre class="line-numbers language-bash"><code class="language-bash">vim regionservers
hadoop102
hadoop103
hadoop104<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-3-HBase远程发送到其他集群"><a href="#2-3-HBase远程发送到其他集群" class="headerlink" title="2.3 HBase远程发送到其他集群"></a>2.3 HBase远程发送到其他集群</h2><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ xsync hbase/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-4-Zookeeper正常部署"><a href="#2-4-Zookeeper正常部署" class="headerlink" title="2.4 Zookeeper正常部署"></a>2.4 Zookeeper正常部署</h2><p>首先保证Zookeeper集群的正常部署，并启动之：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 zookeeper-3.5.7<span class="token punctuation">]</span>$ bin/zkServer.sh start
<span class="token punctuation">[</span>molly@hadoop103 zookeeper-3.5.7<span class="token punctuation">]</span>$ bin/zkServer.sh start
<span class="token punctuation">[</span>molly@hadoop104 zookeeper-3.5.7<span class="token punctuation">]</span>$ bin/zkServer.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="2-5Hadoop正常部署"><a href="#2-5Hadoop正常部署" class="headerlink" title="2.5Hadoop正常部署"></a>2.5Hadoop正常部署</h2><p>Hadoop集群的正常部署并启动：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-dfs.sh
<span class="token punctuation">[</span>molly@hadoop103 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-yarn.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="2-6-HBase服务的启动"><a href="#2-6-HBase服务的启动" class="headerlink" title="2.6 HBase服务的启动"></a>2.6 HBase服务的启动</h2><h3 id="2-6-1-单点启动"><a href="#2-6-1-单点启动" class="headerlink" title="2.6.1 单点启动"></a>2.6.1 单点启动</h3><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hbase<span class="token punctuation">]</span>$ bin/hbase-daemon.sh start master
<span class="token punctuation">[</span>molly@hadoop102 hbase<span class="token punctuation">]</span>$ bin/hbase-daemon.sh start regionserver<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。</p><p>修复提示：错误提示如下所示：</p><p><img src="/2020/11/25/bigdata-HBase1-setup/1637823414826.png" alt="1637823414826"></p><p>a、同步时间服务</p><p>将几台机器进行时间同步操作。</p><p>b、属性：hbase.master.maxclockskew设置更大的值</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hbase.master.maxclockskew<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>180000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>Time difference of regionserver from master<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-6-2-群启"><a href="#2-6-2-群启" class="headerlink" title="2.6.2 群启"></a>2.6.2 群启</h3><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hbase<span class="token punctuation">]</span>$ bin/start-hbase.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>对应的停止服务：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hbase<span class="token punctuation">]</span>$ bin/stop-hbase.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/25/bigdata-HBase1-setup/1637822682585.png" alt="1637822682585"></p><p>可以看到在那台机器去启用Hbase,那台就是HMaster，这里可以看到102是HMaster</p><p><img src="/2020/11/25/bigdata-HBase1-setup/1637822698155.png" alt="1637822698155"></p><p>查看ZK：</p><p><img src="/2020/11/25/bigdata-HBase1-setup/1637822773695.png" alt="1637822773695"></p><h2 id="2-7-查看HBase页面"><a href="#2-7-查看HBase页面" class="headerlink" title="2.7 查看HBase页面"></a>2.7 查看HBase页面</h2><p>启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：</p><p><a target="_blank" rel="noopener" href="http://linux01:16010/">http://hadoop102:16010</a> 可以看到有3个region server</p><p><img src="/2020/11/25/bigdata-HBase1-setup/1637822822917.png" alt="1637822822917"></p><h2 id="2-8-高可用-可选"><a href="#2-8-高可用-可选" class="headerlink" title="2.8 高可用(可选)"></a>2.8 高可用(可选)</h2><p>在HBase中HMaster负责监控<strong>HRegionServer</strong>的生命周期，均衡RegionServer的负载，如果HMaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对HMaster的高可用配置。</p><p>1）关闭HBase集群（如果没有开启则跳过此步）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hbase<span class="token punctuation">]</span>$ bin/stop-hbase.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）在conf目录下创建backup-masters文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hbase<span class="token punctuation">]</span>$ <span class="token function">touch</span> conf/backup-masters<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）在backup-masters文件中配置高可用HMaster节点</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hbase<span class="token punctuation">]</span>$ <span class="token keyword">echo</span> hadoop103 <span class="token operator">></span> conf/backup-masters<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）将整个conf目录scp到其他节点</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hbase<span class="token punctuation">]</span>$ <span class="token function">scp</span> -r conf/ hadoop103:/opt/module/hbase/
<span class="token punctuation">[</span>molly@hadoop102 hbase<span class="token punctuation">]</span>$ <span class="token function">scp</span> -r conf/ hadoop104:/opt/module/hbase/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>5）打开页面测试查看</p><p><a target="_blank" rel="noopener" href="http://linux01:16010/">http://hadooo102:16010</a></p><h1 id="3-HBase-Shell操作"><a href="#3-HBase-Shell操作" class="headerlink" title="3 HBase Shell操作"></a>3 HBase Shell操作</h1><h2 id="3-1-基本操作"><a href="#3-1-基本操作" class="headerlink" title="3.1 基本操作"></a>3.1 基本操作</h2><p>1）进入HBase客户端命令行</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hbase<span class="token punctuation">]</span>$ bin/hbase shell<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）查看帮助命令</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:001:0<span class="token operator">></span> <span class="token function">help</span> <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="3-2-namespace的操作"><a href="#3-2-namespace的操作" class="headerlink" title="3.2 namespace的操作"></a>3.2 namespace的操作</h2><p>1）查看当前Hbase中有哪些namespace</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:002:0<span class="token operator">></span> list_namespace<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>NAMESPACE</p><p>default(创建表时未指定命名空间的话默认在default下)</p><p>hbase(系统使用的，用来存放系统相关的元数据信息等，勿随便操作)</p><p>2）创建namespace</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:010:0<span class="token operator">></span> create_namespace <span class="token string">"test"</span>
hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:010:0<span class="token operator">></span> create_namespace <span class="token string">"test01"</span>, <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"author"=>"wyh", "create_time"=>"2020-03-10 08:08:08"&amp;#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>3）查看namespace</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:010:0<span class="token operator">></span>  describe_namespace <span class="token string">"test01"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）修改namespace的信息（添加或者修改属性）</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:010:0<span class="token operator">></span> alter_namespace <span class="token string">"test01"</span>, <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;METHOD => 'set', 'author' => 'weiyunhui'&amp;#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加或者修改属性:</p><pre class="line-numbers language-bash"><code class="language-bash">alter_namespace <span class="token string">'ns1'</span>, <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;METHOD => 'set', 'PROPERTY_NAME' => 'PROPERTY_VALUE'&amp;#125; </span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>删除属性:</p><pre class="line-numbers language-bash"><code class="language-bash">alter_namespace <span class="token string">'ns1'</span>, <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;METHOD => 'unset', NAME => ' PROPERTY_NAME '&amp;#125; </span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）删除namespace</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:010:0<span class="token operator">></span> drop_namespace <span class="token string">"test01"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意: 要删除的namespace必须是空的，其下没有表。</p><h2 id="3-3-表的操作"><a href="#3-3-表的操作" class="headerlink" title="3.3 表的操作"></a>3.3 表的操作</h2><p>0）查看当前数据库中有哪些表</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:002:0<span class="token operator">></span> list<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>1）创建表</p><p>创建student表，列族info</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:002:0<span class="token operator">></span> create <span class="token string">'student'</span>,<span class="token string">'info'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）插入数据到表</p><p>表明，rowkey,列，value</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:003:0<span class="token operator">></span> put <span class="token string">'student'</span>,<span class="token string">'1001'</span>,<span class="token string">'info:sex'</span>,<span class="token string">'male'</span>
hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:004:0<span class="token operator">></span> put <span class="token string">'student'</span>,<span class="token string">'1001'</span>,<span class="token string">'info:age'</span>,<span class="token string">'18'</span>
hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:005:0<span class="token operator">></span> put <span class="token string">'student'</span>,<span class="token string">'1002'</span>,<span class="token string">'info:name'</span>,<span class="token string">'Janna'</span>
hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:006:0<span class="token operator">></span> put <span class="token string">'student'</span>,<span class="token string">'1002'</span>,<span class="token string">'info:sex'</span>,<span class="token string">'female'</span>
hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:007:0<span class="token operator">></span> put <span class="token string">'student'</span>,<span class="token string">'1002'</span>,<span class="token string">'info:age'</span>,<span class="token string">'20'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）扫描查看表数据</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:008:0<span class="token operator">></span> scan <span class="token string">'student'</span>
hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:009:0<span class="token operator">></span> scan <span class="token string">'student'</span>,<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;STARTROW => '1001', STOPROW => '1001'&amp;#125;</span>
hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:010:0<span class="token operator">></span> scan <span class="token string">'student'</span>,<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;STARTROW => '1001'&amp;#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>4）查看表结构</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:011:0<span class="token operator">></span> describe <span class="token string">'student'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）更新指定字段的数据</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:012:0<span class="token operator">></span> put <span class="token string">'student'</span>,<span class="token string">'1001'</span>,<span class="token string">'info:name'</span>,<span class="token string">'Nick'</span>
hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:013:0<span class="token operator">></span> put <span class="token string">'student'</span>,<span class="token string">'1001'</span>,<span class="token string">'info:age'</span>,<span class="token string">'100'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>6）查看“指定行”或“指定列族:列”的数据必须：<strong>指定rowkey</strong></p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:014:0<span class="token operator">></span> get <span class="token string">'student'</span>,<span class="token string">'1001'</span>
hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:015:0<span class="token operator">></span> get <span class="token string">'student'</span>,<span class="token string">'1001'</span>,<span class="token string">'info:name'</span>
hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:015:0<span class="token operator">></span> get <span class="token string">'student'</span>,<span class="token string">'1001'</span>,<span class="token string">'info:name'</span>,<span class="token string">'info:age'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>7）统计表数据行数</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:021:0<span class="token operator">></span> count <span class="token string">'student'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>8）删除数据</p><p>删除某rowkey的全部数据：</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:016:0<span class="token operator">></span> deleteall <span class="token string">'student'</span>,<span class="token string">'1001'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>删除某rowkey的某一列数据：</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:017:0<span class="token operator">></span> delete <span class="token string">'student'</span>,<span class="token string">'1002'</span>,<span class="token string">'info:sex'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>9）清空表数据</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:018:0<span class="token operator">></span> truncate <span class="token string">'student'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>提示：清空表的操作顺序为先disable，然后再truncate。</p><p>10）删除表</p><p>首先需要先让该表为disable状态：</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:019:0<span class="token operator">></span> disable <span class="token string">'student'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后才能drop这个表：</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:020:0<span class="token operator">></span> drop <span class="token string">'student'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>提示：如果直接drop表，会报错：ERROR: Table student is enabled. Disable it first.</p><p>11）变更表信息</p><p>将info列族中的数据存放3个版本：</p><pre class="line-numbers language-bash"><code class="language-bash">hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:022:0<span class="token operator">></span> alter <span class="token string">'student'</span>,<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;NAME=>'info',VERSIONS=>3&amp;#125;</span>
hbase<span class="token punctuation">(</span>main<span class="token punctuation">)</span>:022:0<span class="token operator">></span> get <span class="token string">'student'</span>,<span class="token string">'1001'</span>,<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;COLUMN=>'info:name',VERSIONS=>3&amp;#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li></ul></div><span class="post-count">总字数2.3k</span> <span class="post-count">预计阅读10分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-datacollect1-userbehavior" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/23/bigdata-datacollect1-userbehavior/" class="article-date"><time class="published" datetime="2020-11-23T07:10:21.000Z" itemprop="datePublished">2020-11-23 发布</time> <time class="updated" datetime="2021-11-25T02:30:35.149Z" itemprop="dateUpdated">2021-11-25 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/23/bigdata-datacollect1-userbehavior/">大数据实践（一）数仓采集项目</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="1-数仓概念"><a href="#1-数仓概念" class="headerlink" title="1 数仓概念"></a>1 数仓概念</h1><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637651564709.png" alt="1637651564709"></p><h1 id="2-项目需求及架构设计"><a href="#2-项目需求及架构设计" class="headerlink" title="2 项目需求及架构设计"></a>2 项目需求及架构设计</h1><h2 id="2-1-项目需求分析"><a href="#2-1-项目需求分析" class="headerlink" title="2.1 项目需求分析"></a>2.1 项目需求分析</h2><p>1）用户行为数据采集平台搭建（存在日志服务器-用flume采集到数据仓库（hdfs）中）<br>2）业务数据采集平台搭建（存在mysql,oracle）：通过sqoop采集到数据仓库（hdfs）中<br>3）数据仓库维度建模：建库建表<br>4）分析，设备 会员 商品 地区 活动等电商核心主题，统计的报表指标近100个。<br>5）采用即席查询工具，随时进行指标分析<br>6）对集群性能进行监控，发送异常需要报警<br>7）元数据管理：管理Hive的元数据(数仓的核心就是hive)<br>8）质量监控：数仓分析的质量 有没有丢数据等</p><h2 id="2-2-项目框架"><a href="#2-2-项目框架" class="headerlink" title="2.2 项目框架"></a>2.2 项目框架</h2><h3 id="2-2-1-技术选型"><a href="#2-2-1-技术选型" class="headerlink" title="2.2.1 技术选型"></a>2.2.1 技术选型</h3><p>技术选型主要考虑因素：数据量大小，业务需求，行业内经验，技术成熟度、开发维护成本、总成本预算。下面加粗的是本次选择的。</p><ul><li>数据采集传输：<strong>Flume</strong>,Logstash(elk系列),<strong>Kafka</strong>,<strong>Sqoop</strong>（开源的）,DataX（阿里技术，比Sqoop强大）</li><li>数据存储：<strong>Mysq</strong>,<strong>HDFS</strong>,HBase,Redis,MongoDB</li><li>数据计算：<strong>Hive</strong>,Tez,<strong>Spark</strong>,Flink,Storm(Hive底层用Tex或者Spark)，Storm较早 不用</li><li>数据查询：<strong>Presto</strong>,<strong>Kylin</strong>（Apache）,Impala,Druid</li><li>数据可视化：Echarts（百度的，现在已经是Apache的了）,<strong>Superset</strong>（开源）,QuickBI,DataV（后两个都是阿里）</li><li>任务调度：<strong>Azkaban</strong>(Apache的，可视化界面),Oozie（CDH生态，HUE，功能比azkaban强大）</li><li>集群监控：<strong>Zabbix</strong></li><li>元数据管理：<strong>Atlas</strong></li></ul><h3 id="2-2-2-系统数据流程设计"><a href="#2-2-2-系统数据流程设计" class="headerlink" title="2.2.2 系统数据流程设计"></a>2.2.2 系统数据流程设计</h3><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637652372079.png" alt="1637652372079"></p><p>多个Flume往HDFS中写数据，会有压力，因此中间部署Kafka，来缓冲。</p><p>Hive通过写sql对数据进行处理，主要有5层处理，分别是ods,dwd,dws,dwt,ads。</p><p>对整个仓库的任务：利用什么时候导入数据等用Azkaban来调度。</p><h3 id="2-2-3-框架版本选型"><a href="#2-2-3-框架版本选型" class="headerlink" title="2.2.3 框架版本选型"></a>2.2.3 框架版本选型</h3><p>1）如何选择Apache/CDH/HDP版本？</p><p>Apache：运维麻烦；组件间兼容性需要自己调研（一般大厂使用，技术实力雄厚，有专业的运维人员）</p><p>CDH：国内使用最多的版本，但是CM不开源。2021年开始收费，一个节点1万美金</p><p>HDP：开源，可以进行二次开发，但是没有CDH稳定，国内使用较少</p><p>目前CDH和HDP已经合并了。</p><p>2）框架版本参考</p><table><thead><tr><th>产品</th><th>版本</th></tr></thead><tbody><tr><td>Hadoop</td><td>3.1.3</td></tr><tr><td>Flume</td><td>1.9.0</td></tr><tr><td>Kafka</td><td>2.4.1</td></tr><tr><td>Hive</td><td>3.1.2</td></tr><tr><td>Sqoop</td><td>1.4.6</td></tr><tr><td>Java</td><td>1.8</td></tr><tr><td>Zookeeper</td><td>3.5.7</td></tr><tr><td>Presto</td><td>0.189</td></tr></tbody></table><h3 id="2-2-4-服务器选型"><a href="#2-2-4-服务器选型" class="headerlink" title="2.2.4   服务器选型"></a>2.2.4 服务器选型</h3><p>服务器选择物理机还是云主机？</p><p>1）物理机：</p><p>以128G内存，20核物理CPU，40线程，戴尔品牌单台报价4W出头，一般物理机寿命5年左右。</p><p>需要有专业的运维任意，平均一个月1万。电费也是不少的开销。</p><p>2）云主机：</p><p>以阿里云为例，差不多配置，每年5W。运维工作都有阿里云完成，运维相对较轻松。</p><p>3）企业选择</p><p>金融有限公司和阿里没有直接冲突的公司选择阿里云。</p><p>中小公司，为了融资上市，选择阿里云，拉到融资后买物理机。</p><p>有长期打算，资金比较足，选择物理机。</p><h3 id="2-2-5-集群资源规划设计"><a href="#2-2-5-集群资源规划设计" class="headerlink" title="2.2.5 集群资源规划设计"></a>2.2.5 集群资源规划设计</h3><p>1）如何确认集群规模？（假设服务器8T磁盘，128G内存）</p><p>（1）每天日活跃用户100万，每人一天平均100条：100万*100条=1亿条</p><p>（2）每条日志1k左右，每天1亿条：100000000/1024/1024=约100G</p><p>（3）半年内不扩容服务器来算：100G*180天=约18T</p><p>（4）保留3副本：18T*3=54T</p><p>（5）保留20%-30%buf=54T/0.7=77T</p><p>（6）算到这里：约8T*10台服务器</p><p>2）测试集群服务器规划</p><table><thead><tr><th>服务名称</th><th>子服务</th><th>服务器 hadoop102</th><th>服务器 hadoop103</th><th>服务器 hadoop104</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td>√</td><td></td><td></td></tr><tr><td></td><td>DataNode</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>SecondaryNameNode</td><td></td><td></td><td>√</td></tr><tr><td>Yarn</td><td>NodeManager</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>Resourcemanager</td><td></td><td>√</td><td></td></tr><tr><td>Zookeeper</td><td>Zookeeper Server</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Flume(采集日志)</td><td>Flume</td><td>√</td><td>√</td><td></td></tr><tr><td>Kafka</td><td>Kafka</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Flume（消费Kafka）</td><td>Flume</td><td></td><td></td><td>√</td></tr><tr><td>Hive</td><td>Hive</td><td>√</td><td></td><td></td></tr><tr><td>MySQL</td><td>MySQL</td><td>√</td><td></td><td></td></tr><tr><td>Sqoop</td><td>Sqoop</td><td>√</td><td></td><td></td></tr><tr><td>Presto</td><td>Coordinator</td><td>√</td><td></td><td></td></tr><tr><td></td><td>Worker</td><td></td><td>√</td><td>√</td></tr><tr><td>Azkaban</td><td>AzkabanWebServer</td><td>√</td><td></td><td></td></tr><tr><td></td><td>AzkabanExecutorServer</td><td>√</td><td></td><td></td></tr><tr><td>Druid</td><td>Druid</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Kylin</td><td></td><td>√</td><td></td><td></td></tr><tr><td>Hbase</td><td>HMaster</td><td>√</td><td></td><td></td></tr><tr><td></td><td>HRegionServer</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Superset</td><td></td><td>√</td><td></td><td></td></tr><tr><td>Atlas</td><td></td><td>√</td><td></td><td></td></tr><tr><td>Solr</td><td>Jar</td><td>√</td><td></td><td></td></tr><tr><td>服务数总计</td><td></td><td>18</td><td>9</td><td>9</td></tr></tbody></table><h1 id="3-数据生成模块"><a href="#3-数据生成模块" class="headerlink" title="3 数据生成模块"></a>3 数据生成模块</h1><h2 id="3-1-目标数据"><a href="#3-1-目标数据" class="headerlink" title="3.1 目标数据"></a>3.1 目标数据</h2><p>我们要收集和分析的数据主要包括页面数据、事件数据、曝光数据、启动数据和错误数据。</p><h3 id="3-1-1-页面"><a href="#3-1-1-页面" class="headerlink" title="3.1.1 页面"></a>3.1.1 页面</h3><p>页面数据主要记录一个页面的用户访问情况，包括访问时间、停留时间、页面路径等信息。</p><p>​</p><p>1）所有页面id如下</p><pre class="line-numbers language-bash"><code class="language-bash">home<span class="token punctuation">(</span><span class="token string">"首页"</span><span class="token punctuation">)</span>,
category<span class="token punctuation">(</span><span class="token string">"分类页"</span><span class="token punctuation">)</span>,
discovery<span class="token punctuation">(</span><span class="token string">"发现页"</span><span class="token punctuation">)</span>,
top_n<span class="token punctuation">(</span><span class="token string">"热门排行"</span><span class="token punctuation">)</span>,
favor<span class="token punctuation">(</span><span class="token string">"收藏页"</span><span class="token punctuation">)</span>,
search<span class="token punctuation">(</span><span class="token string">"搜索页"</span><span class="token punctuation">)</span>,
good_list<span class="token punctuation">(</span><span class="token string">"商品列表页"</span><span class="token punctuation">)</span>,
good_detail<span class="token punctuation">(</span><span class="token string">"商品详情"</span><span class="token punctuation">)</span>,
good_spec<span class="token punctuation">(</span><span class="token string">"商品规格"</span><span class="token punctuation">)</span>,
comment<span class="token punctuation">(</span><span class="token string">"评价"</span><span class="token punctuation">)</span>,
comment_done<span class="token punctuation">(</span><span class="token string">"评价完成"</span><span class="token punctuation">)</span>,
comment_list<span class="token punctuation">(</span><span class="token string">"评价列表"</span><span class="token punctuation">)</span>,
cart<span class="token punctuation">(</span><span class="token string">"购物车"</span><span class="token punctuation">)</span>,
trade<span class="token punctuation">(</span><span class="token string">"下单结算"</span><span class="token punctuation">)</span>,
payment<span class="token punctuation">(</span><span class="token string">"支付页面"</span><span class="token punctuation">)</span>,
payment_done<span class="token punctuation">(</span><span class="token string">"支付完成"</span><span class="token punctuation">)</span>,
orders_all<span class="token punctuation">(</span><span class="token string">"全部订单"</span><span class="token punctuation">)</span>,
orders_unpaid<span class="token punctuation">(</span><span class="token string">"订单待支付"</span><span class="token punctuation">)</span>,
orders_undelivered<span class="token punctuation">(</span><span class="token string">"订单待发货"</span><span class="token punctuation">)</span>,
orders_unreceipted<span class="token punctuation">(</span><span class="token string">"订单待收货"</span><span class="token punctuation">)</span>,
orders_wait_comment<span class="token punctuation">(</span><span class="token string">"订单待评价"</span><span class="token punctuation">)</span>,
mine<span class="token punctuation">(</span><span class="token string">"我的"</span><span class="token punctuation">)</span>,
activity<span class="token punctuation">(</span><span class="token string">"活动"</span><span class="token punctuation">)</span>,
login<span class="token punctuation">(</span><span class="token string">"登录"</span><span class="token punctuation">)</span>,
register<span class="token punctuation">(</span><span class="token string">"注册"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）所有页面对象类型如下：</p><pre class="line-numbers language-bash"><code class="language-bash">sku_id<span class="token punctuation">(</span><span class="token string">"商品skuId"</span><span class="token punctuation">)</span>,
keyword<span class="token punctuation">(</span><span class="token string">"搜索关键词"</span><span class="token punctuation">)</span>,
sku_ids<span class="token punctuation">(</span><span class="token string">"多个商品skuId"</span><span class="token punctuation">)</span>,
activity_id<span class="token punctuation">(</span><span class="token string">"活动id"</span><span class="token punctuation">)</span>,
coupon_id<span class="token punctuation">(</span><span class="token string">"购物券id"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）所有来源类型如下：</p><pre class="line-numbers language-bash"><code class="language-bash">promotion<span class="token punctuation">(</span><span class="token string">"商品推广"</span><span class="token punctuation">)</span>,
recommend<span class="token punctuation">(</span><span class="token string">"算法推荐商品"</span><span class="token punctuation">)</span>,
query<span class="token punctuation">(</span><span class="token string">"查询结果商品"</span><span class="token punctuation">)</span>,
activity<span class="token punctuation">(</span><span class="token string">"促销活动"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-1-2-事件"><a href="#3-1-2-事件" class="headerlink" title="3.1.2 事件"></a>3.1.2 事件</h3><p>事件数据主要记录应用内一个具体操作行为，包括操作类型、操作对象、操作对象描述等信息。</p><p>1）所有动作类型如下：</p><pre class="line-numbers language-bash"><code class="language-bash">favor_add<span class="token punctuation">(</span><span class="token string">"添加收藏"</span><span class="token punctuation">)</span>,
favor_canel<span class="token punctuation">(</span><span class="token string">"取消收藏"</span><span class="token punctuation">)</span>,
cart_add<span class="token punctuation">(</span><span class="token string">"添加购物车"</span><span class="token punctuation">)</span>,
cart_remove<span class="token punctuation">(</span><span class="token string">"删除购物车"</span><span class="token punctuation">)</span>,
cart_add_num<span class="token punctuation">(</span><span class="token string">"增加购物车商品数量"</span><span class="token punctuation">)</span>,
cart_minus_num<span class="token punctuation">(</span><span class="token string">"减少购物车商品数量"</span><span class="token punctuation">)</span>,
trade_add_address<span class="token punctuation">(</span><span class="token string">"增加收货地址"</span><span class="token punctuation">)</span>,
get_coupon<span class="token punctuation">(</span><span class="token string">"领取优惠券"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注：对于下单、支付等业务数据，可从业务数据库获取。</p><p>2）所有动作目标类型如下：</p><p>sku_id(“商品”),<br>coupon_id(“购物券”);</p><h3 id="3-1-3-曝光"><a href="#3-1-3-曝光" class="headerlink" title="3.1.3 曝光"></a>3.1.3 曝光</h3><p>曝光数据主要记录页面所曝光的内容，包括曝光对象，曝光类型等信息。</p><p>1）所有曝光类型如下：</p><pre class="line-numbers language-bash"><code class="language-bash">promotion<span class="token punctuation">(</span><span class="token string">"商品推广"</span><span class="token punctuation">)</span>,
recommend<span class="token punctuation">(</span><span class="token string">"算法推荐商品"</span><span class="token punctuation">)</span>,
query<span class="token punctuation">(</span><span class="token string">"查询结果商品"</span><span class="token punctuation">)</span>,
activity<span class="token punctuation">(</span><span class="token string">"促销活动"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>2）所有曝光对象类型如下：</p><pre class="line-numbers language-bash"><code class="language-bash">sku_id<span class="token punctuation">(</span><span class="token string">"商品skuId"</span><span class="token punctuation">)</span>,
activity_id<span class="token punctuation">(</span><span class="token string">"活动id"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="3-1-4-启动"><a href="#3-1-4-启动" class="headerlink" title="3.1.4 启动"></a>3.1.4 启动</h3><p>启动数据记录应用的启动信息。</p><p>1）所有启动入口类型如下：</p><pre class="line-numbers language-bash"><code class="language-bash">icon<span class="token punctuation">(</span><span class="token string">"图标"</span><span class="token punctuation">)</span>,
notification<span class="token punctuation">(</span><span class="token string">"通知"</span><span class="token punctuation">)</span>,
install<span class="token punctuation">(</span><span class="token string">"安装后启动"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="3-1-5-错误"><a href="#3-1-5-错误" class="headerlink" title="3.1.5 错误"></a>3.1.5 错误</h3><p>错误数据记录应用使用过程中的错误信息，包括错误编号及错误信息。</p><h2 id="3-2-数据埋点"><a href="#3-2-数据埋点" class="headerlink" title="3.2 数据埋点"></a>3.2 数据埋点</h2><h3 id="3-2-1-主流埋点方式（了解）"><a href="#3-2-1-主流埋点方式（了解）" class="headerlink" title="3.2.1 主流埋点方式（了解）"></a>3.2.1 主流埋点方式（了解）</h3><p>目前主流的埋点方式，有代码埋点（前端/后端）、可视化埋点、全埋点三种。</p><p>代码埋点是通过调用埋点SDK函数，在需要埋点的业务逻辑功能位置调用接口，上报埋点数据。例如，我们对页面中的某个按钮埋点后，当这个按钮被点击时，可以在这个按钮对应的 OnClick 函数里面调用SDK提供的数据发送接口，来发送数据。</p><p>可视化埋点只需要研发人员集成采集 SDK，不需要写埋点代码，业务人员就可以通过访问分析平台的“圈选”功能，来“圈”出需要对用户行为进行捕捉的控件，并对该事件进行命名。圈选完毕后，这些配置会同步到各个用户的终端上，由采集 SDK 按照圈选的配置自动进行用户行为数据的采集和发送。（三方埋点技术：神策大数据，GrowingIO）</p><p>全埋点是通过在产品中嵌入SDK，前端自动采集页面上的全部用户行为事件，上报埋点数据，相当于做了一个统一的埋点。然后再通过界面配置哪些数据需要在系统里面进行分析。</p><h3 id="3-2-2-埋点数据日志结构"><a href="#3-2-2-埋点数据日志结构" class="headerlink" title="3.2.2 埋点数据日志结构"></a>3.2.2 埋点数据日志结构</h3><p>我们的日志结构大致可分为两类，一是普通页面埋点日志，二是启动日志。</p><p>普通页面日志结构如下，每条日志包含了，当前页面的页面信息，所有事件（动作）、所有曝光信息以及错误信息。除此之外，还包含了一系列<strong>公共信息</strong>，包括设备信息，地理位置，应用信息等，即下边的<strong>common</strong>字段。</p><p>1）普通页面埋点日志格式</p><pre class="line-numbers language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>
  <span class="token property">"common"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>                  -- 公共信息
    <span class="token property">"ar"</span><span class="token operator">:</span> <span class="token string">"230000"</span><span class="token punctuation">,</span>              -- 地区编码
    <span class="token property">"ba"</span><span class="token operator">:</span> <span class="token string">"iPhone"</span><span class="token punctuation">,</span>              -- 手机品牌
    <span class="token property">"ch"</span><span class="token operator">:</span> <span class="token string">"Appstore"</span><span class="token punctuation">,</span>            -- 渠道
    <span class="token property">"is_new"</span><span class="token operator">:</span> <span class="token string">"1"</span><span class="token punctuation">,</span>--是否首日使用，首次使用的当日，该字段值为<span class="token number">1</span>，过了<span class="token number">24</span><span class="token operator">:</span><span class="token number">00</span>，该字段置为<span class="token number">0</span>。
    <span class="token property">"md"</span><span class="token operator">:</span> <span class="token string">"iPhone 8"</span><span class="token punctuation">,</span>            -- 手机型号
    <span class="token property">"mid"</span><span class="token operator">:</span> <span class="token string">"YXfhjAYH6As2z9Iq"</span><span class="token punctuation">,</span> -- 设备id
    <span class="token property">"os"</span><span class="token operator">:</span> <span class="token string">"iOS 13.2.9"</span><span class="token punctuation">,</span>          -- 操作系统
    <span class="token property">"uid"</span><span class="token operator">:</span> <span class="token string">"485"</span><span class="token punctuation">,</span>                 -- 会员id
    <span class="token property">"vc"</span><span class="token operator">:</span> <span class="token string">"v2.1.134"</span>             -- app版本号
  &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
<span class="token property">"actions"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                     --动作(事件<span class="token punctuation">)</span>  
    &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token property">"action_id"</span><span class="token operator">:</span> <span class="token string">"favor_add"</span><span class="token punctuation">,</span>   --动作id
      <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"3"</span><span class="token punctuation">,</span>                   --目标id
      <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>       --目标类型
      <span class="token property">"ts"</span><span class="token operator">:</span> <span class="token number">1585744376605</span>           --动作时间戳
    &amp;#<span class="token number">125</span><span class="token punctuation">;</span>
  <span class="token punctuation">]</span><span class="token punctuation">,</span>
  <span class="token property">"displays"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
    &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token property">"displayType"</span><span class="token operator">:</span> <span class="token string">"query"</span><span class="token punctuation">,</span>        -- 曝光类型
      <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"3"</span><span class="token punctuation">,</span>                     -- 曝光对象id
      <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>         -- 曝光对象类型
      <span class="token property">"order"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>                      --出现顺序
      <span class="token property">"pos_id"</span><span class="token operator">:</span> <span class="token number">2</span>                      --曝光位置
    &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
    &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token property">"displayType"</span><span class="token operator">:</span> <span class="token string">"promotion"</span><span class="token punctuation">,</span>
      <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"6"</span><span class="token punctuation">,</span>
      <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>
      <span class="token property">"order"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span> 
      <span class="token property">"pos_id"</span><span class="token operator">:</span> <span class="token number">1</span>
    &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
    &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token property">"displayType"</span><span class="token operator">:</span> <span class="token string">"promotion"</span><span class="token punctuation">,</span>
      <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"9"</span><span class="token punctuation">,</span>
      <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>
      <span class="token property">"order"</span><span class="token operator">:</span> <span class="token number">3</span><span class="token punctuation">,</span> 
      <span class="token property">"pos_id"</span><span class="token operator">:</span> <span class="token number">3</span>
    &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
    &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token property">"displayType"</span><span class="token operator">:</span> <span class="token string">"recommend"</span><span class="token punctuation">,</span>
      <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"6"</span><span class="token punctuation">,</span>
      <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>
      <span class="token property">"order"</span><span class="token operator">:</span> <span class="token number">4</span><span class="token punctuation">,</span> 
      <span class="token property">"pos_id"</span><span class="token operator">:</span> <span class="token number">2</span>
    &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
    &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token property">"displayType"</span><span class="token operator">:</span> <span class="token string">"query "</span><span class="token punctuation">,</span>
      <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"6"</span><span class="token punctuation">,</span>
      <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>
      <span class="token property">"order"</span><span class="token operator">:</span> <span class="token number">5</span><span class="token punctuation">,</span> 
      <span class="token property">"pos_id"</span><span class="token operator">:</span> <span class="token number">1</span>
    &amp;#<span class="token number">125</span><span class="token punctuation">;</span>
  <span class="token punctuation">]</span><span class="token punctuation">,</span>
  <span class="token property">"page"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>                       --页面信息
    <span class="token property">"during_time"</span><span class="token operator">:</span> <span class="token number">7648</span><span class="token punctuation">,</span>        -- 持续时间毫秒
    <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"3"</span><span class="token punctuation">,</span>                  -- 目标id
    <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>      -- 目标类型
    <span class="token property">"last_page_id"</span><span class="token operator">:</span> <span class="token string">"login"</span><span class="token punctuation">,</span>    -- 上页类型
    <span class="token property">"page_id"</span><span class="token operator">:</span> <span class="token string">"good_detail"</span><span class="token punctuation">,</span>   -- 页面ID
    <span class="token property">"sourceType"</span><span class="token operator">:</span> <span class="token string">"promotion"</span>   -- 来源类型
  &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
<span class="token property">"err"</span><span class="token operator">:</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span>                     --错误
<span class="token property">"error_code"</span><span class="token operator">:</span> <span class="token string">"1234"</span><span class="token punctuation">,</span>      --错误码
    <span class="token property">"msg"</span><span class="token operator">:</span> <span class="token string">"***********"</span>       --错误信息
&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  <span class="token property">"ts"</span><span class="token operator">:</span> <span class="token number">1585744374423</span>  --跳入时间戳
&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）启动日志格式</p><p>启动日志结构相对简单，主要包含公共信息，启动信息和错误信息。</p><pre class="line-numbers language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>
  <span class="token property">"common"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token property">"ar"</span><span class="token operator">:</span> <span class="token string">"370000"</span><span class="token punctuation">,</span>
    <span class="token property">"ba"</span><span class="token operator">:</span> <span class="token string">"Honor"</span><span class="token punctuation">,</span>
    <span class="token property">"ch"</span><span class="token operator">:</span> <span class="token string">"wandoujia"</span><span class="token punctuation">,</span>
    <span class="token property">"is_new"</span><span class="token operator">:</span> <span class="token string">"1"</span><span class="token punctuation">,</span>
    <span class="token property">"md"</span><span class="token operator">:</span> <span class="token string">"Honor 20s"</span><span class="token punctuation">,</span>
    <span class="token property">"mid"</span><span class="token operator">:</span> <span class="token string">"eQF5boERMJFOujcp"</span><span class="token punctuation">,</span>
    <span class="token property">"os"</span><span class="token operator">:</span> <span class="token string">"Android 11.0"</span><span class="token punctuation">,</span>
    <span class="token property">"uid"</span><span class="token operator">:</span> <span class="token string">"76"</span><span class="token punctuation">,</span>
    <span class="token property">"vc"</span><span class="token operator">:</span> <span class="token string">"v2.1.134"</span>
  &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  <span class="token property">"start"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>   
    <span class="token property">"entry"</span><span class="token operator">:</span> <span class="token string">"icon"</span><span class="token punctuation">,</span>         --icon手机图标  notice 通知   install 安装后启动
    <span class="token property">"loading_time"</span><span class="token operator">:</span> <span class="token number">18803</span><span class="token punctuation">,</span>  --启动加载时间
    <span class="token property">"open_ad_id"</span><span class="token operator">:</span> <span class="token number">7</span><span class="token punctuation">,</span>        --广告页ID
    <span class="token property">"open_ad_ms"</span><span class="token operator">:</span> <span class="token number">3449</span><span class="token punctuation">,</span>    -- 广告总共播放时间
    <span class="token property">"open_ad_skip_ms"</span><span class="token operator">:</span> <span class="token number">1989</span>   --  用户跳过广告时点
  &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
<span class="token property">"err"</span><span class="token operator">:</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span>                     --错误
<span class="token property">"error_code"</span><span class="token operator">:</span> <span class="token string">"1234"</span><span class="token punctuation">,</span>      --错误码
    <span class="token property">"msg"</span><span class="token operator">:</span> <span class="token string">"***********"</span>       --错误信息
&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  <span class="token property">"ts"</span><span class="token operator">:</span> <span class="token number">1585744304000</span>
&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-2-3-埋点数据上报时机"><a href="#3-2-3-埋点数据上报时机" class="headerlink" title="3.2.3 埋点数据上报时机"></a>3.2.3 埋点数据上报时机</h3><p>埋点数据上报时机包括两种方式。</p><p>方式一，在离开该页面时，上传在这个页面产生的所有数据（页面、事件、曝光、错误等）。优点，批处理，减少了服务器接收数据压力。缺点，不是特别及时。</p><p>方式二，每个事件、动作、错误等，产生后，立即发送。优点，响应及时。缺点，对服务器接收数据压力比较大。</p><h2 id="3-3-服务器和JDK准备"><a href="#3-3-服务器和JDK准备" class="headerlink" title="3.3 服务器和JDK准备"></a>3.3 服务器和JDK准备</h2><h3 id="3-3-1-服务器准备"><a href="#3-3-1-服务器准备" class="headerlink" title="3.3.1 服务器准备"></a>3.3.1 服务器准备</h3><p>安装hadoop集群，分别安装hadoop102、hadoop103、hadoop104三台主机。</p><h3 id="3-3-2-阿里云服务器准备（可选）"><a href="#3-3-2-阿里云服务器准备（可选）" class="headerlink" title="3.3.2 阿里云服务器准备（可选）"></a>3.3.2 阿里云服务器准备（可选）</h3><h3 id="3-3-3-JDK准备"><a href="#3-3-3-JDK准备" class="headerlink" title="3.3.3 JDK准备"></a>3.3.3 JDK准备</h3><p>1）卸载现有JDK（3台节点）</p><pre><code>[molly@hadoop102 opt]# sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps
[molly@hadoop103 opt]# sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps
[molly@hadoop104 opt]# sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps</code></pre><p>2）用SecureCRT工具将JDK导入到hadoop102的/opt/software文件夹下面</p><p>3） “alt+p”进入sftp模式</p><p>4）选择jdk1.8拖入工具</p><p>5）在Linux系统下的opt目录中查看软件包是否导入成功</p><pre><code>[molly@hadoop102 software]# ls /opt/software/</code></pre><p>看到如下结果：</p><pre><code>jdk-8u212-linux-x64.tar.gz</code></pre><p>6）解压JDK到/opt/module目录下</p><pre><code>[molly@hadoop102 software]# tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/</code></pre><p>7）配置JDK环境变量</p><p>​ （1）新建/etc/profile.d/my_env.sh文件</p><pre><code>[molly@hadoop102 module]# sudo vim /etc/profile.d/my_env.sh</code></pre><p>添加如下内容，然后保存（:wq）退出</p><pre class="line-numbers language-sh"><code class="language-sh">#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>​ （2）让环境变量生效</p><pre><code>[molly@hadoop102 software]$ source /etc/profile.d/my_env.sh</code></pre><p>8）测试JDK是否安装成功</p><pre><code>[molly@hadoop102 module]# java -version
#如果能看到以下结果、则Java正常安装
java version &quot;1.8.0_212&quot;</code></pre><p>9）分发JDK</p><pre><code>[molly@hadoop102 module]$ xsync /opt/module/jdk1.8.0_212/</code></pre><p>10）分发环境变量配置文件</p><pre><code>[molly@hadoop102 module]$ sudo /home/molly/bin/xsync /etc/profile.d/my_env.sh</code></pre><p>11）分别在hadoop103、hadoop104上执行source</p><pre><code>[molly@hadoop103 module]$ source /etc/profile.d/my_env.sh
[molly@hadoop104 module]$ source /etc/profile.d/my_env.sh</code></pre><h3 id="3-3-4-环境变量配置说明"><a href="#3-3-4-环境变量配置说明" class="headerlink" title="3.3.4 环境变量配置说明"></a>3.3.4 环境变量配置说明</h3><p>Linux的环境变量可在多个文件中配置，如/etc/profile，/etc/profile.d/*.sh，<del>/.bashrc，</del>/.bash_profile等，下面说明上述几个文件之间的关系和区别。</p><p>bash的运行模式可分为login shell和non-login shell。</p><p>例如，我们通过终端，输入用户名、密码，登录系统之后，得到就是一个login shell，而当我们执行以下命令ssh hadoop103 command，在hadoop103执行command的就是一个non-login shell。</p><p>这两种shell的主要区别在于，它们启动时会加载不同的配置文件，login shell启动时会加载/etc/profile，<del>/.bash_profile，</del>/.bashrc，non-login shell启动时会加载~/.bashrc。</p><p>而在加载<del>/.bashrc（实际是</del>/.bashrc中加载的/etc/bashrc）或/etc/profile时，都会执行如下代码片段，</p><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637657318225.png" alt="1637657318225"></p><p>因此不管是login shell还是non-login shell，启动时都会加载/etc/profile.d/*.sh中的环境变量。</p><h2 id="3-4-模拟数据"><a href="#3-4-模拟数据" class="headerlink" title="3.4 模拟数据"></a>3.4 模拟数据</h2><h3 id="3-4-1-使用说明"><a href="#3-4-1-使用说明" class="headerlink" title="3.4.1 使用说明"></a>3.4.1 使用说明</h3><p>1）将application.yml、gmall2020-mock-log-2021-01-22.jar、path.json、logback.xml上传到hadoop102的/opt/module/applog目录下</p><p>（1）创建applog路径</p><pre><code>[molly@hadoop102 module]$ mkdir /opt/module/applog</code></pre><p>（2）上传文件</p><p>2）配置文件</p><p>（1）application.yml文件</p><p>可以根据需求生成对应日期的用户行为日志。</p><pre><code>[molly@hadoop102 applog]$ vim application.yml</code></pre><p>修改如下内容</p><pre class="line-numbers language-yml"><code class="language-yml"># 外部配置打开
# 外部配置打开
logging.config: "./logback.xml"
#业务日期
mock.date: "2020-06-14"

#模拟数据发送模式
#mock.type: "http"
#mock.type: "kafka"
mock.type: "log"

#http模式下，发送的地址
mock.url: "http://hdp1/applog"

#kafka模式下，发送的地址
mock:
  kafka-server: "hdp1:9092,hdp2:9092,hdp3:9092"
  kafka-topic: "ODS_BASE_LOG"

#启动次数
mock.startup.count: 200
#设备最大值
mock.max.mid: 500000
#会员最大值
mock.max.uid: 100
#商品最大值
mock.max.sku-id: 35
#页面平均访问时间
mock.page.during-time-ms: 20000
#错误概率 百分比
mock.error.rate: 3
#每条日志发送延迟 ms
mock.log.sleep: 10
#商品详情来源  用户查询，商品推广，智能推荐, 促销活动
mock.detail.source-type-rate: "40:25:15:20"
#领取购物券概率
mock.if_get_coupon_rate: 75
#购物券最大id
mock.max.coupon-id: 3
#搜索关键词  
mock.search.keyword: "图书,小米,iphone11,电视,口红,ps5,苹果手机,小米盒子"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）path.json，该文件用来配置访问路径</p><p>根据需求，可以灵活配置用户点击路径。</p><p>来到主页-搜索-上篇-下单-。。</p><pre class="line-numbers language-json"><code class="language-json"><span class="token punctuation">[</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span><span class="token punctuation">,</span><span class="token string">"good_list"</span><span class="token punctuation">,</span><span class="token string">"good_detail"</span><span class="token punctuation">,</span><span class="token string">"cart"</span><span class="token punctuation">,</span><span class="token string">"trade"</span><span class="token punctuation">,</span><span class="token string">"payment"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">20</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span><span class="token punctuation">,</span><span class="token string">"search"</span><span class="token punctuation">,</span><span class="token string">"good_list"</span><span class="token punctuation">,</span><span class="token string">"good_detail"</span><span class="token punctuation">,</span><span class="token string">"login"</span><span class="token punctuation">,</span><span class="token string">"good_detail"</span><span class="token punctuation">,</span><span class="token string">"cart"</span><span class="token punctuation">,</span><span class="token string">"trade"</span><span class="token punctuation">,</span><span class="token string">"payment"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">40</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span><span class="token punctuation">,</span><span class="token string">"mine"</span><span class="token punctuation">,</span><span class="token string">"orders_unpaid"</span><span class="token punctuation">,</span><span class="token string">"trade"</span><span class="token punctuation">,</span><span class="token string">"payment"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">10</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span><span class="token punctuation">,</span><span class="token string">"mine"</span><span class="token punctuation">,</span><span class="token string">"orders_unpaid"</span><span class="token punctuation">,</span><span class="token string">"good_detail"</span><span class="token punctuation">,</span><span class="token string">"good_spec"</span><span class="token punctuation">,</span><span class="token string">"comment"</span><span class="token punctuation">,</span><span class="token string">"trade"</span><span class="token punctuation">,</span><span class="token string">"payment"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">5</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span><span class="token punctuation">,</span><span class="token string">"mine"</span><span class="token punctuation">,</span><span class="token string">"orders_unpaid"</span><span class="token punctuation">,</span><span class="token string">"good_detail"</span><span class="token punctuation">,</span><span class="token string">"good_spec"</span><span class="token punctuation">,</span><span class="token string">"comment"</span><span class="token punctuation">,</span><span class="token string">"home"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">5</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span><span class="token punctuation">,</span><span class="token string">"good_detail"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">10</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span>  <span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">10</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）logback配置文件</p><p>可配置日志生成路径，修改内容如下</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>LOG_HOME<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>/opt/module/applog/log<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>console<span class="token punctuation">"</span></span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ch.qos.logback.core.ConsoleAppender<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>encoder</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pattern</span><span class="token punctuation">></span></span>%msg%n<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pattern</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>encoder</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>appender</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>rollingFile<span class="token punctuation">"</span></span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ch.qos.logback.core.rolling.RollingFileAppender<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>rollingPolicy</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ch.qos.logback.core.rolling.TimeBasedRollingPolicy<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>fileNamePattern</span><span class="token punctuation">></span></span>$<span class="token entity" title="&#123;">&amp;#123;</span>LOG_HOME<span class="token entity" title="&#125;">&amp;#125;</span>/app.%d<span class="token entity" title="&#123;">&amp;#123;</span>yyyy-MM-dd<span class="token entity" title="&#125;">&amp;#125;</span>.log<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>fileNamePattern</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>rollingPolicy</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>encoder</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pattern</span><span class="token punctuation">></span></span>%msg%n<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pattern</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>encoder</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>appender</span><span class="token punctuation">></span></span>

    <span class="token comment" spellcheck="true">&lt;!-- 将某一个包下日志单独打印日志 --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>logger</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>com.atgugu.gmall2020.mock.log.util.LogUtil<span class="token punctuation">"</span></span>
            <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>INFO<span class="token punctuation">"</span></span> <span class="token attr-name">additivity</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>false<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>rollingFile<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>console<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>logger</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>root</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>error<span class="token punctuation">"</span></span>  <span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>console<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>root</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）生成日志</p><p>（1）进入到/opt/module/applog路径，执行以下命令</p><pre><code>[molly@hadoop102 applog]$ java -jar gmall2020-mock-log-2021-01-22.jar</code></pre><p>（2）在/opt/module/applog/log目录下查看生成日志</p><pre><code>[molly@hadoop102 log]$ ll</code></pre><h3 id="3-4-2-集群日志生成脚本"><a href="#3-4-2-集群日志生成脚本" class="headerlink" title="3.4.2 集群日志生成脚本"></a>3.4.2 集群日志生成脚本</h3><p>在生成日志的时候模拟多台服务器生产的日志。在hadoop102的/home/molly目录下创建bin目录，这样脚本可以在服务器的任何目录执行。</p><pre><code>[molly@hadoop102 ~]$ echo $PATH
/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/molly/.local/bin:/home/molly/bin</code></pre><p>​ 1）在/home/molly/bin目录下创建脚本lg.sh</p><pre><code>[molly@hadoop102 bin]$ vim lg.sh</code></pre><p>​ 2）在脚本中编写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#!/bin/bash
for i in hadoop102 hadoop103; do
    echo "========== $i =========="
    ssh $i "cd /opt/module/applog/; java -jar gmall2020-mock-log-2021-01-22.jar >/dev/null 2>&1 &"
done <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注：</p><p>（1）/opt/module/applog/为jar包及配置文件所在路径</p><p>（2）/dev/null代表linux的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞”。</p><p>标准输入0：从键盘获得输入 /proc/self/fd/0</p><p>标准输出1：输出到屏幕（即控制台） /proc/self/fd/1</p><p>错误输出2：输出到屏幕（即控制台） /proc/self/fd/2</p><p>3）修改脚本执行权限</p><pre><code>[molly@hadoop102 bin]$ chmod u+x lg.sh</code></pre><p>4）将jar包及配置文件s上传至hadoop103的/opt/module/applog/路径</p><p>5）启动脚本</p><pre><code>[molly@hadoop102 module]$ lg.sh </code></pre><p>6）分别在hadoop102、hadoop103的/opt/module/applog/log目录上查看生成的数据</p><pre><code>[molly@hadoop102 logs]$ ls
app.2020-06-14.log
[molly@hadoop103 logs]$ ls
app.2020-06-14.log</code></pre><h1 id="4-数据采集模块"><a href="#4-数据采集模块" class="headerlink" title="4 数据采集模块"></a>4 数据采集模块</h1><h2 id="4-1-集群所有进程查看脚本"><a href="#4-1-集群所有进程查看脚本" class="headerlink" title="4.1 集群所有进程查看脚本"></a>4.1 集群所有进程查看脚本</h2><p>1）在/home/molly/bin目录下创建脚本xcall.sh</p><pre><code>[molly@hadoop102 bin]$ vim xcall.sh</code></pre><p>2）在脚本中编写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#! /bin/bash

for i in hadoop102 hadoop103 hadoop104
do
    echo --------- $i ----------
    ssh $i "$*"
done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）修改脚本执行权限</p><pre><code>[molly@hadoop102 bin]$ chmod 777 xcall.sh</code></pre><p>4）启动脚本</p><pre><code>[molly@hadoop102 bin]$ xcall.sh jps</code></pre><h2 id="4-2-Hadoop安装"><a href="#4-2-Hadoop安装" class="headerlink" title="4.2 Hadoop安装"></a>4.2 Hadoop安装</h2><p>详见：<a href="https://m01ly.github.io/2020/11/12/bigdata-hdfs1/">Hadoop 教程（二）安装hadoop集群-完全分布式部署</a></p><p>1）集群规划：</p><table><thead><tr><th></th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode DataNode</td><td>DataNode</td><td>DataNode SecondaryNameNode</td></tr><tr><td>Yarn</td><td>NodeManager</td><td>Resourcemanager NodeManager</td><td>NodeManager</td></tr></tbody></table><p>注意：尽量使用离线方式安装。第一次启动需要格式化namenode.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>hdfs namenode -format<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-2-1-项目经验之HDFS存储多目录"><a href="#4-2-1-项目经验之HDFS存储多目录" class="headerlink" title="4.2.1 项目经验之HDFS存储多目录"></a>4.2.1 项目经验之HDFS存储多目录</h3><p>datanode和namenode都可以多目录存储。namenode不同目录的数据都是一样的，所以这里我们不配namenode的多目录。但是datanode的多目录中每个目录存储的数据是不一样的，可以多目录（磁盘挂载到的对应目录。因此用不同磁盘来存储Datanode，实现方式就是：datanode配置多目录）</p><p>1）生产环境服务器磁盘情况</p><p>从当前服务器可以看到：4个磁盘挂载在不同的目录。</p><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637661734977.png" alt="1637661734977"></p><p>2）在hdfs-site.xml文件中配置多目录，注意新挂载磁盘的访问权限问题。</p><p>HDFS的DataNode节点保存数据的路径由dfs.datanode.data.dir参数决定，其默认值为file://${hadoop.tmp.dir}/dfs/data，若服务器有多个磁盘，必须对该参数进行修改。如服务器磁盘如上图所示，则该参数应修改为如下的值。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.datanode.data.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>注意：每台服务器挂载的磁盘不一样，所以每个节点的多目录配置可以不一致。单独配置即可。</p><h3 id="4-2-2-集群数据均衡"><a href="#4-2-2-集群数据均衡" class="headerlink" title="4.2.2 集群数据均衡"></a>4.2.2 集群数据均衡</h3><p><strong>1）节点间数据均衡</strong>:就是102 103 104之间的</p><p>下面由于人工操作会导致三台节点的存储数据不均衡，如下图所示：<img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637662122810.png" alt="1637662122810"></p><p>解决办法：开启数据均衡命令：</p><pre class="line-numbers language-bash"><code class="language-bash">start-balancer.sh -threshold 10<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。</p><p>停止数据均衡命令：</p><pre class="line-numbers language-bash"><code class="language-bash">stop-balancer.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>2）磁盘间数据均衡</strong></p><p>hadoop3.x的新特性：针对磁盘间的数据均衡</p><p>（1）生成均衡计划（我们只有一块磁盘，不会生成计划）</p><pre class="line-numbers language-bash"><code class="language-bash">hdfs diskbalancer -plan hadoop103<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）执行均衡计划</p><pre class="line-numbers language-bash"><code class="language-bash">hdfs diskbalancer -execute hadoop103.plan.json<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）查看当前均衡任务的执行情况</p><pre class="line-numbers language-bash"><code class="language-bash">hdfs diskbalancer -query hadoop103<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）取消均衡任务</p><pre class="line-numbers language-bash"><code class="language-bash">hdfs diskbalancer -cancel hadoop103.plan.json<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-2-3-项目经验之支持LZO压缩配置"><a href="#4-2-3-项目经验之支持LZO压缩配置" class="headerlink" title="4.2.3 项目经验之支持LZO压缩配置"></a>4.2.3 项目经验之支持LZO压缩配置</h3><p>1）hadoop本身并不支持lzo压缩，故需要使用twitter提供的<a target="_blank" rel="noopener" href="https://github.com/twitter/hadoop-lzo">hadoop-lzo</a>开源组件。hadoop-lzo需依赖hadoop和lzo进行编译。</p><p>2）将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-3.1.3/share/hadoop/common/</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 common<span class="token punctuation">]</span>$ <span class="token function">pwd</span>
/opt/module/hadoop-3.1.3/share/hadoop/common
<span class="token punctuation">[</span>molly@hadoop102 common<span class="token punctuation">]</span>$ <span class="token function">ls</span>
hadoop-lzo-0.4.20.jar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>3）同步hadoop-lzo-0.4.20.jar到hadoop103、hadoop104</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 common<span class="token punctuation">]</span>$ xsync hadoop-lzo-0.4.20.jar<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）core-site.xml增加配置支持LZO压缩</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codecs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>
​      org.apache.hadoop.io.compress.GzipCodec,
​      org.apache.hadoop.io.compress.DefaultCodec,
​      org.apache.hadoop.io.compress.BZip2Codec,
​      org.apache.hadoop.io.compress.SnappyCodec,
​      com.hadoop.compression.lzo.LzoCodec,
​      com.hadoop.compression.lzo.LzopCodec
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codec.lzo.class<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.hadoop.compression.lzo.LzoCodec<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5）同步core-site.xml到hadoop103、hadoop104</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop<span class="token punctuation">]</span>$ xsync core-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>6）启动及查看集群</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-dfs.sh
<span class="token punctuation">[</span>molly@hadoop103 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-yarn.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="4-2-4-测试LZO"><a href="#4-2-4-测试LZO" class="headerlink" title="4.2.4 测试LZO"></a>4.2.4 测试LZO</h3><p>（1）执行wordcount程序</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class<span class="token operator">=</span>com.hadoop.mapreduce.LzoTextInputFormat /input /lzo-output<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637662603236.png" alt="1637662603236"></p><h3 id="4-2-4-项目经验之LZO创建索引"><a href="#4-2-4-项目经验之LZO创建索引" class="headerlink" title="4.2.4 项目经验之LZO创建索引"></a>4.2.4 项目经验之LZO创建索引</h3><p>1）创建LZO文件的索引，LZO压缩文件的可切片特性依赖于其索引，故我们需要手动为LZO压缩文件创建索引。若无索引，则LZO文件的切片只有一个。</p><pre class="line-numbers language-bash"><code class="language-bash">hadoop jar /path/to/your/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer big_file.lzo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）测试</p><p>​ （1）将bigtable.lzo（200M）上传到集群的根目录</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ hadoop fs -mkdir /input
<span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ hadoop fs -put bigtable.lzo /input<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637662802344.png" alt="1637662802344"></p><p>（2）执行wordcount程序</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class<span class="token operator">=</span>com.hadoop.mapreduce.LzoTextInputFormat /input /output1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>允许发现切片只有一个，因此想起原理，Lzo切片是依赖于索引的，因此我们需要建索引</p><p>（3）对上传的LZO文件建索引</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /input/bigtable.lzo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637662962636.png" alt="1637662962636"></p><p>（4）再次执行WordCount程序</p><p>这里注意需要指定inputformat类为LzoText对应的类com.hadoop.mapreduce.LzoTextInputFormat</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class<span class="token operator">=</span>com.hadoop.mapreduce.LzoTextInputFormat /input /output2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>发现切片数为2。查看历史服务器去查看具体切片信息。<img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637662988690.png" alt="1637662988690"></p><p>map执行过程中切片信息。<img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637663231181.png" alt="1637663231181"></p><p>3）注意：如果以上任务，在运行过程中报如下异常</p><pre class="line-numbers language-bash"><code class="language-bash">Container <span class="token punctuation">[</span>pid<span class="token operator">=</span>8468,containerID<span class="token operator">=</span>container_1594198338753_0001_01_000002<span class="token punctuation">]</span> is running 318740992B beyond the <span class="token string">'VIRTUAL'</span> memory limit. Current usage: 111.5 MB of 1 GB physical memory used<span class="token punctuation">;</span> 2.4 GB of 2.1 GB virtual memory used. Killing container.
Dump of the process-tree <span class="token keyword">for</span> container_1594198338753_0001_01_000002 <span class="token keyword">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>解决办法：在hadoop102的/opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml文件中增加如下配置，然后分发到hadoop103、hadoop104服务器上，并重新启动集群。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token comment" spellcheck="true">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.pmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.vmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-2-5-项目经验之基准测试"><a href="#4-2-5-项目经验之基准测试" class="headerlink" title="4.2.5 项目经验之基准测试"></a>4.2.5 项目经验之基准测试</h3><p><strong>1） 测试HDFS写性能</strong></p><p>​ 测试内容：向HDFS集群写10个128M的文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mapreduce<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 6 -fileSize 128MB

2020-04-16 13:41:24,724 INFO fs.TestDFSIO: ----- TestDFSIO ----- <span class="token keyword">:</span> <span class="token function">write</span>
2020-04-16 13:41:24,724 INFO fs.TestDFSIO:       Date <span class="token operator">&amp;</span> time: Thu Apr 16 13:41:24 CST 2020
2020-04-16 13:41:24,724 INFO fs.TestDFSIO:     Number of files: 10
2020-04-16 13:41:24,725 INFO fs.TestDFSIO: Total MBytes processed: 1280
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:    Throughput mb/sec: 8.88
2020-04-16 13:41:24,725 INFO fs.TestDFSIO: Average IO rate mb/sec: 8.96
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:  IO rate std deviation: 0.87
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:   Test <span class="token function">exec</span> <span class="token function">time</span> sec: 67.61<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）测试HDFS读性能</p><p>测试内容：读取HDFS集群10个128M的文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mapreduce<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB
2020-04-16 13:43:38,857 INFO fs.TestDFSIO: ----- TestDFSIO ----- <span class="token keyword">:</span> <span class="token function">read</span>
2020-04-16 13:43:38,858 INFO fs.TestDFSIO:   Date <span class="token operator">&amp;</span> time: Thu Apr 16 13:43:38 CST 2020
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:         Number of files: 10
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:  Total MBytes processed: 1280
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:       Throughput mb/sec: 85.54
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:  Average IO rate mb/sec: 100.21
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:   IO rate std deviation: 44.37
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:      Test <span class="token function">exec</span> <span class="token function">time</span> sec: 53.61<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）删除测试生成数据</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mapreduce<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）使用Sort程序评测MapReduce（要求性能特别好，普通性能不要去跑）</p><p>（1）使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mapreduce<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）执行Sort程序</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mapreduce<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar <span class="token function">sort</span> random-data sorted-data<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）验证数据是否真正排好序了</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mapreduce<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-2-6-项目经验之Hadoop参数调优"><a href="#4-2-6-项目经验之Hadoop参数调优" class="headerlink" title="4.2.6 项目经验之Hadoop参数调优"></a>4.2.6 项目经验之Hadoop参数调优</h3><p><strong>1）HDFS参数调优hdfs-site.xml</strong></p><p><strong>NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。</strong></p><p>对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.handler.count<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>10<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>dfs.namenode.handler.count=<img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637663793390.png" alt="1637663793390"> ，比如集群规模为8台时，此参数设置为41。</p><p><strong>2）YARN参数调优yarn-site.xml</strong></p><p>（1）情景描述：总共7台机器，每天几亿条数据，数据源-&gt;Flume-&gt;Kafka-&gt;HDFS-&gt;Hive</p><p>面临问题：数据统计主要用HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启的JVM重用，而且IO没有阻塞，内存用了不到50%。但是还是跑的非常慢，而且数据量洪峰过来时，整个集群都会宕掉。基于这种情况有没有优化方案。</p><p>（2）解决办法：</p><p>内存利用率不够。这个一般是Yarn的2个配置造成的，单个任务可以申请的最大内存大小，和Hadoop单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。</p><p>（a）yarn.nodemanager.resource.memory-mb</p><p>表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。</p><p>（b）yarn.scheduler.maximum-allocation-mb</p><p>单个任务可申请的最多物理内存量，默认是8192（MB）。</p><h2 id="4-3-Zookeeper安装"><a href="#4-3-Zookeeper安装" class="headerlink" title="4.3 Zookeeper安装"></a>4.3 Zookeeper安装</h2><h3 id="4-3-1-安装ZK"><a href="#4-3-1-安装ZK" class="headerlink" title="4.3.1 安装ZK"></a>4.3.1 安装ZK</h3><p>详见：</p><p>集群规划</p><table><thead><tr><th></th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>Zookeeper</td><td>Zookeeper</td><td>Zookeeper</td><td>Zookeeper</td></tr></tbody></table><h3 id="4-3-2-ZK集群启动停止脚本"><a href="#4-3-2-ZK集群启动停止脚本" class="headerlink" title="4.3.2 ZK集群启动停止脚本"></a>4.3.2 ZK集群启动停止脚本</h3><p>1）在hadoop102的/home/molly/bin目录下创建脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ vim zk.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>#在脚本中编写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#!/bin/bash

case $1 in
"start")&#123;
    for i in hadoop102 hadoop103 hadoop104
    do
        echo ---------- zookeeper $i 启动 ------------
        ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
    done
&#125;;;
"stop")&#123;
    for i in hadoop102 hadoop103 hadoop104
    do
        echo ---------- zookeeper $i 停止 ------------    
        ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
    done
&#125;;;
"status")&#123;
    for i in hadoop102 hadoop103 hadoop104
    do
        echo ---------- zookeeper $i 状态 ------------    
        ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
    done
&#125;;;
esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）增加脚本执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> u+x zk.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）Zookeeper集群启动脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ zk.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）Zookeeper集群停止脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ zk.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="4-4-Kafka安装"><a href="#4-4-Kafka安装" class="headerlink" title="4.4 Kafka安装"></a>4.4 Kafka安装</h2><h3 id="4-4-1-Kafka集群安装"><a href="#4-4-1-Kafka集群安装" class="headerlink" title="4.4.1 Kafka集群安装"></a>4.4.1 Kafka集群安装</h3><p>详见：<a href="https://m01ly.github.io/2020/11/15/bigdata-kafka1-setup/">kafka学习笔记（一） kafka搭建</a></p><p>配置文件：</p><pre class="line-numbers language-bash"><code class="language-bash">log.dirs<span class="token operator">=</span>/opt/moudule/kafka 2.11-2.4.1/datas
zookeeper.connect<span class="token operator">=</span>hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>集群规划：</p><table><thead><tr><th></th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>Kafka</td><td>Kafka</td><td>Kafka</td><td>Kafka</td></tr></tbody></table><h3 id="4-4-2-Kafka集群启动停止脚本"><a href="#4-4-2-Kafka集群启动停止脚本" class="headerlink" title="4.4.2 Kafka集群启动停止脚本"></a>4.4.2 Kafka集群启动停止脚本</h3><p>1）在/home/molly/bin目录下创建脚本kf.sh</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ vim kf.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在脚本中填写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#! /bin/bash

case $1 in
"start")&#123;
    for i in hadoop102 hadoop103 hadoop104
    do
        echo " --------启动 $i Kafka-------"
        ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
    done
&#125;;;
"stop")&#123;
    for i in hadoop102 hadoop103 hadoop104
    do
        echo " --------停止 $i Kafka-------"
        ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop"
    done
&#125;;;
esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）增加脚本执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> u+x kf.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）kf集群启动脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ kf.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查看zookeeper信息<img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637664623705.png" alt="1637664623705"></p><p>4）kf集群停止脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ kf.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-4-3-Kafka常用命令"><a href="#4-4-3-Kafka常用命令" class="headerlink" title="4.4.3 Kafka常用命令"></a>4.4.3 Kafka常用命令</h3><p>1）查看Kafka Topic列表</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka --list
<span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list
<span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>2）创建Kafka Topic</p><p>进入到/opt/module/kafka/目录下创建日志主题</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka --create --replication-factor 1 --partitions 1 --topic topic_log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）删除Kafka Topic</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --delete --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka --topic topic_log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）Kafka生产消息</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-console-producer.sh \
--broker-list hadoop102:9092 --topic topic_log
\<span class="token operator">></span>hello world
\<span class="token operator">></span>molly molly<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>5）Kafka消费消息</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-console-consumer.sh \
--bootstrap-server hadoop102:9092 --from-beginning --topic topic_log<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>–from-beginning：会把主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。</p><p>6）查看Kafka Topic详情</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka \
--describe --topic topic_log<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="4-4-4-项目经验之Kafka压力测试"><a href="#4-4-4-项目经验之Kafka压力测试" class="headerlink" title="4.4.4 项目经验之Kafka压力测试"></a>4.4.4 项目经验之Kafka压力测试</h3><p><strong>1）Kafka压测</strong></p><p>用Kafka官方自带的脚本，对Kafka进行压测。Kafka压测时，可以查看到哪个地方出现了瓶颈（<strong>CPU，内存，网络IO</strong>）。一般都是网络IO达到瓶颈。</p><p>kafka-consumer-perf-test.sh<br>kafka-producer-perf-test.sh</p><p><strong>2）Kafka Producer压力测试</strong></p><p>（1）在/opt/module/kafka/bin目录下面有这两个文件。我们来测试一下</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-producer-perf-test.sh --topic <span class="token function">test</span> --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers<span class="token operator">=</span>hadoop102:9092,hadoop103:9092,hadoop104:9092<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>说明：</p><p>record-size是一条信息有多大，单位是字节。<br>num-records是总共发送多少条信息。<br>throughput 是每秒多少条信息，设成-1，表示不限流，可测出生产者最大吞吐量。</p><p><strong>（2）Kafka会打印下面的信息</strong></p><pre class="line-numbers language-bash"><code class="language-bash">100000 records sent, 95877.277085 records/sec <span class="token punctuation">(</span>9.14 MB/sec<span class="token punctuation">)</span>, 187.68 ms avg latency, 424.00 ms max latency, 155 ms 50th, 411 ms 95th, 423 ms 99th, 424 ms 99.9th.<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>参数解析：本例中一共写入10w条消息，吞吐量为9.14 MB/sec，每次写入的平均延迟为187.68毫秒，最大的延迟为424.00毫秒。</p><p><strong>3）Kafka Consumer压力测试</strong></p><p>Consumer的测试，如果这四个指标（IO，CPU，内存，网络）都不能改变，考虑增加分区数来提升性能。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-consumer-perf-test.sh --broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic <span class="token function">test</span> --fetch-size 10000 --messages 10000000 --threads 1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>参数说明：</p><p>–zookeeper 指定zookeeper的链接信息<br>–topic 指定topic的名称<br>–fetch-size 指定每次fetch的数据的大小<br>–messages 总共要消费的消息个数</p><p>测试结果说明：</p><pre class="line-numbers language-bash"><code class="language-bash">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
2019-02-19 20:29:07:566, 2019-02-19 20:29:12:170, 9.5368, 2.0714, 100010, 21722.4153<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>开始测试时间，测试结束数据，共消费数据9.5368MB，吞吐量2.0714MB/s，共消费100010条，平均每秒消费21722.4153条。</p><h3 id="4-4-5-项目经验之Kafka机器数量计算"><a href="#4-4-5-项目经验之Kafka机器数量计算" class="headerlink" title="4.4.5 项目经验之Kafka机器数量计算"></a>4.4.5 项目经验之Kafka机器数量计算</h3><p>Kafka机器数量（经验公式）=2x（峰值生产速度x副本数/100）+1<br>先拿到峰值生产速度，再根据设定的副本数，就能预估出需要部署Kafka的数量。<br>比如我们的峰值生产速度是50M/s。副本数为2。<br>Kafka机器数量=2<em>（50</em>2/100）+ 1=3台</p><h3 id="4-4-6-项目经验值Kafka分区数计算"><a href="#4-4-6-项目经验值Kafka分区数计算" class="headerlink" title="4.4.6 项目经验值Kafka分区数计算"></a>4.4.6 项目经验值Kafka分区数计算</h3><p>1）创建一个只有1个分区的topic</p><p>2）测试这个topic的producer吞吐量和consumer吞吐量。</p><p>3）假设他们的值分别是Tp和Tc，单位可以是MB/s。</p><p>4）然后假设总的目标吞吐量是Tt，那么分区数=Tt / min（Tp，Tc）</p><p>例如：producer吞吐量=20m/s；consumer吞吐量=50m/s，期望吞吐量100m/s；</p><p>分区数=100 / 20 =5分区</p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42641909/article/details/89294698">https://blog.csdn.net/weixin_42641909/article/details/89294698</a></p><p>分区数一般设置为：3-10个</p><h1 id="5-项目1-采集用户行为数据"><a href="#5-项目1-采集用户行为数据" class="headerlink" title="5 项目1 采集用户行为数据"></a>5 项目1 采集用户行为数据</h1><p>如下图所示，用户行为经过埋点进行收集然后存放到logserver上，这个时候利用Flume（第一层）从日志服务器logserver上采集数据送到kafka中，再通过一个flume（第二层）接收，最后存储到HDFS中。</p><p>再看flume架构：</p><p>第一层flume，我们source选择为taildirSource,channel选Kafka Channel（这里不需要sink，因为KafkaChanel直接将数据存到Kafka中了）。</p><p>第二层flume：我们source选择为KafkaSource,channel选fileChannel,sink选择HDFS Sink。</p><p>其中flume安装详见：<a href="https://m01ly.github.io/2020/11/15/bigdata-flume1-setup/">flume学习笔记（一） flume搭建</a></p><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637724868076.png" alt="1637724868076"></p><p><strong>日志采集Flume集群规划</strong>：</p><p>我们将第一层flume安装在hadoop102和hadoop103上，第二层flume安装在hadoop104.</p><table><thead><tr><th></th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>Flume(采集日志)</td><td>（第一层）Flume</td><td>（第一层） Flume</td><td>（第二层 Flum</td></tr></tbody></table><h2 id="5-1-第一层flume采集"><a href="#5-1-第一层flume采集" class="headerlink" title="5.1 第一层flume采集"></a>5.1 第一层flume采集</h2><h3 id="5-1-1-项目经验之Flume组件选型"><a href="#5-1-1-项目经验之Flume组件选型" class="headerlink" title="5.1.1 项目经验之Flume组件选型"></a>5.1.1 项目经验之Flume组件选型</h3><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637722930078.png" alt="1637722930078"></p><p>Flume直接读log日志的数据，log日志的格式是app.yyyy-mm-dd.log。注意其中logInterceptor主要对原始日志进行初步数据处理，删除空数据。</p><p><strong>1）Source</strong></p><p>（1）Taildir Source相比Exec Source、Spooling Directory Source的优势</p><p><strong>TailDir Source：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。</strong></p><p><strong>Exec Source</strong>可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失。</p><p><strong>Spooling Directory Source</strong>监控目录，支持断点续传。</p><p>（2）batchSize大小如何设置？</p><p>+答：Event 1K左右时，500-1000合适（默认为100）</p><p><strong>2）Channel</strong></p><p><strong>采用Kafka Channel，省去了Sink，提高了效率。KafkaChannel数据存储在Kafka里面，所以数据是存储在磁盘中。</strong></p><p>注意在Flume1.7以前，Kafka Channel很少有人使用，因为发现parseAsFlumeEvent这个配置起不了作用。也就是无论parseAsFlumeEvent配置为true还是false，都会转为Flume Event。这样的话，造成的结果是，会始终都把Flume的headers中的信息混合着内容一起写入Kafka的消息中，这显然不是我所需要的，我只是需要把内容写入即可。</p><h3 id="5-1-2-日志采集Flume配置"><a href="#5-1-2-日志采集Flume配置" class="headerlink" title="5.1.2 日志采集Flume配置"></a>5.1.2 日志采集Flume配置</h3><p>1）Flume的具体配置如下：</p><p>​ （1）在/opt/module/flume/conf目录下创建file-flume-kafka.conf文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ vim file-flume-kafka.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在文件配置如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#为各组件命名
a1.sources = r1
a1.channels = c1

#描述source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*
a1.sources.r1.positionFile = /opt/module/flume/taildir_position.json
a1.sources.r1.interceptors =  i1
a1.sources.r1.interceptors.i1.type = com.molly.flume.interceptor.ETLInterceptor$Builder
#描述channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092
a1.channels.c1.kafka.topic = topic_log
a1.channels.c1.parseAsFlumeEvent = false
#绑定source和channel以及sink和channel的关系
a1.sources.r1.channels = c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意：com.molly.flume.interceptor.ETLInterceptor是自定义的拦截器的全类名。需要根据用户自定义的拦截器做相应修改。</p><h3 id="5-1-3-Flume拦截器"><a href="#5-1-3-Flume拦截器" class="headerlink" title="5.1.3 Flume拦截器"></a>5.1.3 Flume拦截器</h3><p>在第一层flume中对原始数据进行清洗</p><p>1）创建Maven工程flume-interceptor</p><p>2）创建包名：com.molly.flume.interceptor</p><p>3）在pom.xml文件中添加如下配置</p><p>maven-compiler-plugin是打包插件。com.alibaba注意加<scope>compile</scope>，把该组件打到包中。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flume<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flume-ng-core<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.9.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">></span></span>provided<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.alibaba<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>fastjson<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.2.62<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">></span></span>compile<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-compiler-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.3.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>source</span><span class="token punctuation">></span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>source</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>target</span><span class="token punctuation">></span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>target</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-assembly-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRefs</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRef</span><span class="token punctuation">></span></span>jar-with-dependencies<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRef</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRefs</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>make-assembly<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>
                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>single<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4）在com.molly.flume.interceptor包下创建JSONUtils类</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>molly<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>interceptor<span class="token punctuation">;</span>
<span class="token keyword">import</span> com<span class="token punctuation">.</span>alibaba<span class="token punctuation">.</span>fastjson<span class="token punctuation">.</span>JSON<span class="token punctuation">;</span>
<span class="token keyword">import</span> com<span class="token punctuation">.</span>alibaba<span class="token punctuation">.</span>fastjson<span class="token punctuation">.</span>JSONException<span class="token punctuation">;</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">JSONUtils</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">boolean</span> <span class="token function">isJSONValidate</span><span class="token punctuation">(</span>String log<span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token keyword">try</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            JSON<span class="token punctuation">.</span><span class="token function">parse</span><span class="token punctuation">(</span>log<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">JSONException</span> e<span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> <span class="token boolean">false</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5）在com.molly.flume.interceptor包下创建LogInterceptor类</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>molly<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>interceptor<span class="token punctuation">;</span>
<span class="token keyword">import</span> com<span class="token punctuation">.</span>alibaba<span class="token punctuation">.</span>fastjson<span class="token punctuation">.</span>JSON<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>Context<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>Event<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>interceptor<span class="token punctuation">.</span>Interceptor<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>nio<span class="token punctuation">.</span>charset<span class="token punctuation">.</span>StandardCharsets<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Iterator<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>List<span class="token punctuation">;</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">ETLInterceptor</span> <span class="token keyword">implements</span> <span class="token class-name">Interceptor</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">initialize</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> Event <span class="token function">intercept</span><span class="token punctuation">(</span>Event event<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span> body <span class="token operator">=</span> event<span class="token punctuation">.</span><span class="token function">getBody</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        String log <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">String</span><span class="token punctuation">(</span>body<span class="token punctuation">,</span> StandardCharsets<span class="token punctuation">.</span>UTF_8<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>JSONUtils<span class="token punctuation">.</span><span class="token function">isJSONValidate</span><span class="token punctuation">(</span>log<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> event<span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> null<span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> List<span class="token operator">&lt;</span>Event<span class="token operator">></span> <span class="token function">intercept</span><span class="token punctuation">(</span>List<span class="token operator">&lt;</span>Event<span class="token operator">></span> list<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        Iterator<span class="token operator">&lt;</span>Event<span class="token operator">></span> iterator <span class="token operator">=</span> list<span class="token punctuation">.</span><span class="token function">iterator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">while</span> <span class="token punctuation">(</span>iterator<span class="token punctuation">.</span><span class="token function">hasNext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            Event next <span class="token operator">=</span> iterator<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">intercept</span><span class="token punctuation">(</span>next<span class="token punctuation">)</span><span class="token operator">==</span>null<span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
                iterator<span class="token punctuation">.</span><span class="token function">remove</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> list<span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">Builder</span> <span class="token keyword">implements</span> <span class="token class-name">Interceptor<span class="token punctuation">.</span>Builder</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> Interceptor <span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">ETLInterceptor</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">configure</span><span class="token punctuation">(</span>Context context<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
       <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>6）打包</p><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637725114820.png" alt="1637725114820"></p><p>7）需要先将打好的包放入到hadoop102的/opt/module/flume/lib文件夹下面。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 lib<span class="token punctuation">]</span>$ <span class="token function">ls</span> <span class="token operator">|</span> <span class="token function">grep</span> interceptor
flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>8）分发Flume到hadoop103、hadoop104</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ xsync flume/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>9）hadoop102消费Flume数据</strong></p><p>为了查看第一次flume是否起作用，我们开启一个kafka消费端来消费kafkaChannel中的数据。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102~<span class="token punctuation">]</span>kafka-console-consumer.sh --topic topic_log --bootstrap-server hadoop102:9092<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>10）分别在hadoop102、hadoop103上启动Flume</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 flume<span class="token punctuation">]</span>$ bin/flume-ng agent --name a1 --conf-file conf/file-flume-kafka.conf <span class="token operator">&amp;</span>
<span class="token punctuation">[</span>molly@hadoop103 flume<span class="token punctuation">]</span>$ bin/flume-ng agent --name a1 --conf-file conf/file-flume-kafka.conf <span class="token operator">&amp;</span>
<span class="token punctuation">[</span>molly@hadoop102 flume<span class="token punctuation">]</span>$ bin/flume-ng agent --name a1 --conf-file conf/file-flume-kafka.conf  -n a1 -Dflume.root.logger<span class="token operator">=</span>INFO,console<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>11）观看9)中的kafka消费端有数据在消费。</p><h3 id="5-1-4-日志采集Flume启动停止脚本"><a href="#5-1-4-日志采集Flume启动停止脚本" class="headerlink" title="5.1.4 日志采集Flume启动停止脚本"></a>5.1.4 日志采集Flume启动停止脚本</h3><p>1）在/home/molly/bin目录下创建脚本f1.sh</p><p>[molly@hadoop102 bin]$ vim f1.sh</p><p>​ 在脚本中填写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#! /bin/bash

case $1 in
"start")&#123;
        for i in hadoop102 hadoop103
        do
                echo " --------启动 $i 采集flume-------"
                ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log1.txt 2>&1  &"
        done
&#125;;;    
"stop")&#123;
        for i in hadoop102 hadoop103
        do
                echo " --------停止 $i 采集flume-------"
                ssh $i "ps -ef | grep file-flume-kafka | grep -v grep |awk  '&#123;print \$2&#125;' | xargs -n1 kill -9 "
        done

&#125;;;
esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>说明1：nohup，该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思，不挂断地运行命令。</p><p>说明2：awk 默认分隔符为空格</p><p>说明3：xargs 表示取出前面命令运行的结果，作为后面命令的输入参数。</p><p>2）增加脚本执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> u+x f1.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）f1集群启动脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ f1.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）f1集群停止脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ f1.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>集群规划</p><table><thead><tr><th></th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>Flume（消费Kafka）</td><td></td><td></td><td>Flume</td></tr></tbody></table><h2 id="5-2-第二层flume采集"><a href="#5-2-第二层flume采集" class="headerlink" title="5.2 第二层flume采集"></a>5.2 第二层flume采集</h2><p>集群规划，第二层flume是消费Kafka数据的Flume，部署在hadoop104上。</p><table><thead><tr><th></th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>Flume（消费Kafka）</td><td></td><td></td><td>Flume</td></tr></tbody></table><h3 id="5-2-1-项目经验之Flume组件选型"><a href="#5-2-1-项目经验之Flume组件选型" class="headerlink" title="5.2.1 项目经验之Flume组件选型"></a>5.2.1 项目经验之Flume组件选型</h3><p>第二次Flume主要作用是消费Kafka中的数据，然后存储到HDFS中，因此Source选择KafkaSource,sink选择HDFSsink。同时在source端使用一个拦截器：拦截器作用是获取日志中的实际时间。</p><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637738855416.png" alt="1637738855416"></p><p>1）FileChannel和MemoryChannel区别</p><p>MemoryChannel传输数据速度更快，但因为数据保存在JVM的堆内存中，Agent进程挂掉会导致数据丢失，适用于对数据质量要求不高的需求。</p><p>FileChannel传输速度相对于Memory慢，但数据安全保障高，Agent进程挂掉也可以从失败中恢复数据。</p><p>选型：</p><p>金融类公司、对钱要求非常准确的公司通常会选择FileChannel</p><p>传输的是普通日志信息（京东内部一天丢100万-200万条，这是非常正常的），通常选择MemoryChannel。</p><p><strong>2）FileChannel优化</strong></p><p>通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。</p><p>官方说明如下：</p><p>checkpointDir和backupCheckpointDir也尽量配置在不同硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据</p><p><strong>3）Sink：HDFS Sink</strong></p><p>（1）HDFS存入大量小文件，有什么影响？</p><p><strong>元数据层面：</strong>每个小文件都有一份元数据，其中包括文件路径，文件名，所有者，所属组，权限，创建时间等，这些信息都保存在Namenode内存中。所以小文件过多，会占用Namenode服务器大量内存，影响Namenode性能和使用寿命</p><p><strong>计算层面：</strong>默认情况下MR会对每个小文件启用一个Map任务计算，非常影响计算性能。同时也影响磁盘寻址时间。</p><p>（2）HDFS小文件处理</p><p>官方默认的这三个参数配置写入HDFS后会产生小文件，hdfs.rollInterval、hdfs.rollSize、hdfs.rollCount</p><p>基于以上hdfs.rollInterval=3600，hdfs.rollSize=134217728，hdfs.rollCount =0几个参数综合作用，效果如下：</p><p>（1）文件在达到128M时会滚动生成新文件</p><p>（2）文件创建超3600秒时会滚动生成新文件</p><h3 id="5-2-2-Flume拦截器"><a href="#5-2-2-Flume拦截器" class="headerlink" title="5.2.2 Flume拦截器"></a>5.2.2 Flume拦截器</h3><p>由于flume默认会用linux系统时间，作为输出到HDFS路径的时间。如果数据是23:59分产生的。Flume消费kafka里面的数据时，有可能已经是第二天了，那么这部门数据会被发往第二天的HDFS路径。我们希望的是根据日志里面的实际时间，发往HDFS的路径，<strong>所以下面拦截器作用是获取日志中的实际时间</strong>。</p><p>1）在com.molly.flume.interceptor包下创建TimeStampInterceptor类</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>molly<span class="token punctuation">.</span>interceptor<span class="token punctuation">;</span>

<span class="token keyword">import</span> com<span class="token punctuation">.</span>alibaba<span class="token punctuation">.</span>fastjson<span class="token punctuation">.</span>JSONObject<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>Context<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>Event<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>interceptor<span class="token punctuation">.</span>Interceptor<span class="token punctuation">;</span>

<span class="token keyword">import</span> java<span class="token punctuation">.</span>nio<span class="token punctuation">.</span>charset<span class="token punctuation">.</span>StandardCharsets<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>ArrayList<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>List<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Map<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">TimeStampInterceptor</span> <span class="token keyword">implements</span> <span class="token class-name">Interceptor</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token keyword">private</span> ArrayList<span class="token operator">&lt;</span>Event<span class="token operator">></span> events <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ArrayList</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">initialize</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> Event <span class="token function">intercept</span><span class="token punctuation">(</span>Event event<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        Map<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> String<span class="token operator">></span> headers <span class="token operator">=</span> event<span class="token punctuation">.</span><span class="token function">getHeaders</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        String log <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">String</span><span class="token punctuation">(</span>event<span class="token punctuation">.</span><span class="token function">getBody</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> StandardCharsets<span class="token punctuation">.</span>UTF_8<span class="token punctuation">)</span><span class="token punctuation">;</span>

        JSONObject jsonObject <span class="token operator">=</span> JSONObject<span class="token punctuation">.</span><span class="token function">parseObject</span><span class="token punctuation">(</span>log<span class="token punctuation">)</span><span class="token punctuation">;</span>

        String ts <span class="token operator">=</span> jsonObject<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"ts"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        headers<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">"timestamp"</span><span class="token punctuation">,</span> ts<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token keyword">return</span> event<span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> List<span class="token operator">&lt;</span>Event<span class="token operator">></span> <span class="token function">intercept</span><span class="token punctuation">(</span>List<span class="token operator">&lt;</span>Event<span class="token operator">></span> list<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        events<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>Event event <span class="token operator">:</span> list<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            events<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span><span class="token function">intercept</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token keyword">return</span> events<span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">Builder</span> <span class="token keyword">implements</span> <span class="token class-name">Interceptor<span class="token punctuation">.</span>Builder</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> Interceptor <span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">TimeStampInterceptor</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">configure</span><span class="token punctuation">(</span>Context context<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）重新打包</p><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637739135017.png" alt="1637739135017"></p><p>3）需要先将打好的包放入到hadoop102的/opt/module/flume/lib文件夹下面。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 lib<span class="token punctuation">]</span>$ <span class="token function">ls</span> <span class="token operator">|</span> <span class="token function">grep</span> interceptor
flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>4）分发Flume到hadoop103、hadoop104</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ xsync flume/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="5-2-3-日志消费Flume配置"><a href="#5-2-3-日志消费Flume配置" class="headerlink" title="5.2.3 日志消费Flume配置"></a>5.2.3 日志消费Flume配置</h3><p>1）Flume的具体配置如下：</p><p>​ （1）在hadoop104的/opt/module/flume/conf目录下创建kafka-flume-hdfs.conf文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop104 conf<span class="token punctuation">]</span>$ vim kafka-flume-hdfs.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在文件配置如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">## 组件
a1.sources=r1
a1.channels=c1
a1.sinks=k1

## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics=topic_log
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.molly.flume.interceptor.TimeStampInterceptor$Builder

## channel1
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1/
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 6
## sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log-
a1.sinks.k1.hdfs.round = false

a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0
## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop
## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="5-2-4-日志消费Flume启动停止脚本"><a href="#5-2-4-日志消费Flume启动停止脚本" class="headerlink" title="5.2.4 日志消费Flume启动停止脚本"></a>5.2.4 日志消费Flume启动停止脚本</h3><p>1）在/home/molly/bin目录下创建脚本f2.sh</p><p>[molly@hadoop102 bin]$ vim f2.sh</p><p>​ 在脚本中填写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#! /bin/bash

case $1 in
"start")&#123;
        for i in hadoop104
        do
                echo " --------启动 $i 消费flume-------"
                ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log2.txt   2>&1 &"
        done
&#125;;;
"stop")&#123;
        for i in hadoop104
        do
                echo " --------停止 $i 消费flume-------"
                ssh $i "ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '&#123;print \$2&#125;' | xargs -n1 kill"
        done

&#125;;;
esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）增加脚本执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> u+x f2.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）f2集群启动脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ f2.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）f2集群停止脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ f2.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="5-3-项目经验之Flume内存优化"><a href="#5-3-项目经验之Flume内存优化" class="headerlink" title="5.3 项目经验之Flume内存优化"></a>5.3 项目经验之Flume内存优化</h2><p>1）问题描述：如果启动消费Flume抛出如下异常</p><p>ERROR hdfs.HDFSEventSink: process failed</p><p>java.lang.OutOfMemoryError: GC overhead limit exceeded</p><p>2）解决方案步骤：</p><p>（1）在hadoop102服务器的/opt/module/flume/conf/flume-env.sh文件中增加如下配置</p><p>export JAVA_OPTS=”-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote”</p><p>（2）同步配置到hadoop103、hadoop104服务器</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ xsync flume-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）Flume内存参数设置及优化</p><p>JVM heap一般设置为4G或更高</p><p>-Xmx与-Xms最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导致频繁fullgc。</p><p>-Xms表示JVM Heap(堆内存)最小尺寸，初始分配；-Xmx 表示JVM Heap(堆内存)最大允许的尺寸，按需分配。如果不设置一致，容易在初始化时，由于内存不够，频繁触发fullgc。</p><h2 id="5-4-采集通道启动-停止脚本"><a href="#5-4-采集通道启动-停止脚本" class="headerlink" title="5.4 采集通道启动/停止脚本"></a>5.4 采集通道启动/停止脚本</h2><h3 id="5-4-1-数据通道测试"><a href="#5-4-1-数据通道测试" class="headerlink" title="5.4.1 数据通道测试"></a>5.4.1 数据通道测试</h3><p>根据需求分别生成2020-06-14和2020-06-15日期的数据</p><p>1）修改/opt/module/applog/application.yml中业务日期为2020-06-14</p><p>#业务日期</p><p>mock.date=2020-06-14</p><p>2）执行脚本，生成2020-06-14日志数据</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ lg.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）再次修改/opt/module/applog/application.yml中业务日期2020-06-15</p><p>#业务日期</p><p>mock.date=2020-06-15</p><p>4）执行脚本，生成2020-06-15日志数据</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ lg.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）在这个期间，不断观察Hadoop的HDFS路径上是否有数据</p><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637740975327.png" alt="1637740975327"></p><h3 id="5-4-2-采集通道启动-停止脚本"><a href="#5-4-2-采集通道启动-停止脚本" class="headerlink" title="5.4.2 采集通道启动/停止脚本"></a>5.4.2 采集通道启动/停止脚本</h3><p>1）在/home/molly/bin目录下创建脚本cluster.sh</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ vim cluster.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在脚本中填写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#!/bin/bash
case $1 in
"start")&#123;
        echo ================== 启动 集群 ==================
        #启动 Zookeeper集群
        zk.sh start
        #启动 Hadoop集群
        hdp.sh start
        #启动 Kafka采集集群
        kf.sh start
        #启动 Flume采集集群
        f1.sh start
        #启动 Flume消费集群
        f2.sh start
        &#125;;;
"stop")&#123;
        echo ================== 停止 集群 ==================

        #停止 Flume消费集群
        f2.sh stop
        #停止 Flume采集集群
        f1.sh stop
        #停止 Kafka采集集群
        kf.sh stop
        #停止 Hadoop集群
        hdp.sh stop
        #停止 Zookeeper集群
        zk.sh stop
&#125;;;
esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）增加脚本执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> u+x cluster.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）cluster集群启动脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ cluster.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）cluster集群停止脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ cluster.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="6-项目2-采集业务数据"><a href="#6-项目2-采集业务数据" class="headerlink" title="6 项目2 采集业务数据"></a>6 项目2 采集业务数据</h1><h2 id="6-1-电商业务流程"><a href="#6-1-电商业务流程" class="headerlink" title="6.1 电商业务流程"></a>6.1 电商业务流程</h2><p>电商的业务流程可以以一个普通用户的浏览足迹为例进行说明，用户点开电商首页开始浏览，可能会通过分类查询也可能通过全文搜索寻找自己中意的商品，这些商品无疑都是存储在后台的管理系统中的。</p><p>当用户寻找到自己中意的商品，可能会想要购买，将商品添加到购物车后发现需要登录，登录后对商品进行结算，这时候购物车的管理和商品订单信息的生成都会对业务数据库产生影响，会生成相应的订单数据和支付数据。</p><p>订单正式生成之后，还会对订单进行跟踪处理，直到订单全部完成。</p><p>电商的主要业务流程包括用户前台浏览商品时的商品详情的管理，用户商品加入购物车进行支付时用户个人中心&amp;支付服务的管理，用户支付完成后订单后台服务的管理，这些流程涉及到了十几个甚至几十个业务数据表，甚至更多。</p><h2 id="6-2-电商常识"><a href="#6-2-电商常识" class="headerlink" title="6.2 电商常识"></a>6.2 电商常识</h2><h3 id="6-2-1-SKU和SPU"><a href="#6-2-1-SKU和SPU" class="headerlink" title="6.2.1 SKU和SPU"></a>6.2.1 SKU和SPU</h3><p>SKU=Stock Keeping Unit（库存量基本单位）。现在已经被引申为产品统一编号的简称，每种产品均对应有唯一的SKU号。</p><p>SPU（Standard Product Unit）：是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息集合。</p><p>例如：iPhoneX手机就是SPU。一台银色、128G内存的、支持联通网络的iPhoneX，就是SKU。</p><p>SPU表示一类商品。同一SPU的商品可以共用商品图片、海报、销售属性等。</p><h3 id="6-2-2-平台属性和销售属性"><a href="#6-2-2-平台属性和销售属性" class="headerlink" title="6.2.2 平台属性和销售属性"></a>6.2.2 平台属性和销售属性</h3><p><strong>1.平台属性</strong></p><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637745317255.png" alt="1637745317255"></p><p><strong>2.销售属性</strong></p><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637745332754.png" alt="1637745332754"></p><h2 id="6-3-业务数据采集架构"><a href="#6-3-业务数据采集架构" class="headerlink" title="6.3 业务数据采集架构"></a>6.3 业务数据采集架构</h2><p>从2.2.2可以看到整体架构；对于需求2 采集业务数据的结构图如下所示：</p><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637746045465.png" alt="1637746045465"></p><p>业务数据库是直接存储到mysql数据库的，我们可以通过sqoop组件使用JDBC将数据传输到HDFS上存储。</p><p>sqoop安装教程见：</p><p>Hive安装教程见：</p><h2 id="6-4-同步策略"><a href="#6-4-同步策略" class="headerlink" title="6.4 同步策略"></a>6.4 同步策略</h2><p>数据同步策略的类型包括：全量同步、增量同步、新增及变化同步、特殊情况</p><p>Ø 全量表：存储完整的数据。</p><p>Ø 增量表：存储新增加的数据。</p><p>Ø 新增及变化表：存储新增加的数据和变化的数据。</p><p>Ø 特殊表：只需要存储一次。</p><h3 id="6-4-1-全量同步策略"><a href="#6-4-1-全量同步策略" class="headerlink" title="6.4.1 全量同步策略"></a>6.4.1 全量同步策略</h3><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637746273106.png" alt="1637746273106"></p><h3 id="6-4-2-增量同步策略"><a href="#6-4-2-增量同步策略" class="headerlink" title="6.4.2 增量同步策略"></a>6.4.2 增量同步策略</h3><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637746298966.png" alt="1637746298966"></p><h3 id="6-4-3-新增及变化策略"><a href="#6-4-3-新增及变化策略" class="headerlink" title="6.4.3 新增及变化策略"></a>6.4.3 新增及变化策略</h3><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637746311874.png" alt="1637746311874"></p><h3 id="6-4-4-特殊策略"><a href="#6-4-4-特殊策略" class="headerlink" title="6.4.4 特殊策略"></a>6.4.4 特殊策略</h3><p>某些特殊的表，可不必遵循上述同步策略。</p><p><strong>例如</strong>没变化的客观世界的数据（比如性别，地区，民族，政治成分，鞋子尺码）可以只存一份。</p><h2 id="6-5-业务数据导入HDFS"><a href="#6-5-业务数据导入HDFS" class="headerlink" title="6.5 业务数据导入HDFS"></a>6.5 业务数据导入HDFS</h2><h3 id="6-5-1-分析表同步策略"><a href="#6-5-1-分析表同步策略" class="headerlink" title="6.5.1 分析表同步策略"></a>6.5.1 分析表同步策略</h3><p>在生产环境，个别小公司，为了简单处理，所有表全量导入。</p><p>中大型公司，由于数据量比较大，还是严格按照同步策略导入数据。</p><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637746489890.png" alt="1637746489890"></p><h3 id="6-5-2-业务数据首日同步脚本"><a href="#6-5-2-业务数据首日同步脚本" class="headerlink" title="6.5.2 业务数据首日同步脚本"></a>6.5.2 业务数据首日同步脚本</h3><p><strong>1）脚本编写</strong></p><p>（1）在/home/atguigu/bin目录下创建</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ vim mysql_to_hdfs_init.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加如下内容：</p><pre class="line-numbers language-sh"><code class="language-sh">#! /bin/bash
APP=gmall
sqoop=/opt/module/sqoop/bin/sqoop
# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$2" ] ;then
   do_date=$2
else 
   echo "请传入日期参数"
   exit
fi 
import_data()&#123;
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/$APP \
--username root \
--password 123456 \
--target-dir /origin_data/$APP/db/$1/$do_date \
--delete-target-dir \
--query "$2 where \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \
--compression-codec lzop \
--null-string '\\N' \
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
&#125;
import_order_info()&#123;
  import_data order_info "select
                            id, 
                            total_amount, 
                            order_status, 
                            user_id, 
                            payment_way,
                            delivery_address,
                            out_trade_no, 
                            create_time, 
                            operate_time,
                            expire_time,
                            tracking_no,
                            province_id,
                            activity_reduce_amount,
                            coupon_reduce_amount,                            
                            original_total_amount,
                            feight_fee,
                            feight_fee_reduce      
                        from order_info"
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="6-5-3-项目经验"><a href="#6-5-3-项目经验" class="headerlink" title="6.5.3 项目经验"></a>6.5.3 项目经验</h3><p>Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用–input-null-string和–input-null-non-string两个参数。导入数据时采用–null-string和–null-non-string。</p><h2 id="6-6-Hive进行业务数据访问"><a href="#6-6-Hive进行业务数据访问" class="headerlink" title="6.6 Hive进行业务数据访问"></a>6.6 Hive进行业务数据访问</h2><p>Hive安装教程详见：<a href="https://m01ly.github.io/2020/11/14/bigdata-hive1/">https://m01ly.github.io/2020/11/14/bigdata-hive1/</a></p><p>后面就是通过Hive去对数据进行初步的分析。</p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%95%B0%E4%BB%93%E9%87%87%E9%9B%86%E9%A1%B9%E7%9B%AE/" rel="tag">数仓采集项目</a></li></ul></div><span class="post-count">总字数12.8k</span> <span class="post-count">预计阅读57分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-sqoop" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/22/bigdata-sqoop/" class="article-date"><time class="published" datetime="2020-11-22T07:10:21.000Z" itemprop="datePublished">2020-11-22 发布</time> <time class="updated" datetime="2021-11-24T08:48:34.244Z" itemprop="dateUpdated">2021-11-24 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/22/bigdata-sqoop/">sqoop安装教程</a></h1></header><div class="article-entry" itemprop="articleBody"><h3 id="1-下载并解压"><a href="#1-下载并解压" class="headerlink" title="1 下载并解压"></a>1 下载并解压</h3><p>1）下载地址：<a target="_blank" rel="noopener" href="http://mirrors.hust.edu.cn/apache/sqoop/1.4.6/">http://mirrors.hust.edu.cn/apache/sqoop/1.4.6/</a></p><p>2）上传安装包sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz到hadoop102的/opt/software路径中</p><p>3）解压sqoop安装包到指定目录，如：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）解压sqoop安装包到指定目录，如：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ <span class="token function">mv</span> sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-修改配置文件"><a href="#2-修改配置文件" class="headerlink" title="2 修改配置文件"></a>2 修改配置文件</h3><ol><li>进入到/opt/module/sqoop/conf目录，重命名配置文件</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ <span class="token function">mv</span> sqoop-env-template.sh sqoop-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="2"><li>修改配置文件</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ vim sqoop-env.sh <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>增加如下内容</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">export</span> HADOOP_COMMON_HOME<span class="token operator">=</span>/opt/module/hadoop-3.1.3
<span class="token function">export</span> HADOOP_MAPRED_HOME<span class="token operator">=</span>/opt/module/hadoop-3.1.3
<span class="token function">export</span> HIVE_HOME<span class="token operator">=</span>/opt/module/hive
<span class="token function">export</span> ZOOKEEPER_HOME<span class="token operator">=</span>/opt/module/zookeeper-3.5.7
<span class="token function">export</span> ZOOCFGDIR<span class="token operator">=</span>/opt/module/zookeeper-3.5.7/conf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-拷贝JDBC驱动"><a href="#3-拷贝JDBC驱动" class="headerlink" title="3 拷贝JDBC驱动"></a>3 拷贝JDBC驱动</h3><p>1）将mysql-connector-java-5.1.48.jar 上传到/opt/software路径</p><p>2）进入到/opt/software/路径，拷贝jdbc驱动到sqoop的lib目录下。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">cp</span> mysql-connector-java-5.1.48.jar /opt/module/sqoop/lib/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-验证Sqoop"><a href="#4-验证Sqoop" class="headerlink" title="4 验证Sqoop"></a>4 验证Sqoop</h3><p>我们可以通过某一个command来验证sqoop配置是否正确：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 sqoop<span class="token punctuation">]</span>$ bin/sqoop <span class="token function">help</span>
Available commands:
  codegen            Generate code to interact with database records
  create-hive-table     Import a table definition into Hive
  <span class="token function">eval</span>               Evaluate a SQL statement and display the results
  <span class="token function">export</span>             Export an HDFS directory to a database table
  <span class="token function">help</span>               List available commands
  <span class="token function">import</span>             Import a table from a database to HDFS
  import-all-tables     Import tables from a database to HDFS
  import-mainframe    Import datasets from a mainframe server to HDFS
  job                Work with saved <span class="token function">jobs</span>
  list-databases        List available databases on a server
  list-tables           List available tables <span class="token keyword">in</span> a database
  merge              Merge results of incremental imports
  metastore           Run a standalone Sqoop metastore
  version            Display version information<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="5-测试Sqoop是否能够成功连接数据库"><a href="#5-测试Sqoop是否能够成功连接数据库" class="headerlink" title="5 测试Sqoop是否能够成功连接数据库"></a>5 测试Sqoop是否能够成功连接数据库</h3><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 sqoop<span class="token punctuation">]</span>$ bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password 000000
information_schema
metastore
mysql
oozie
performance_schema<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/sqoop/" rel="tag">sqoop</a></li></ul></div><span class="post-count">总字数420</span> <span class="post-count">预计阅读2分钟</span></div><div class="clearfix"></div></div></div></article><nav id="page-nav"><a class="extend prev" rel="prev" href="/page/2/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/4/">Next &amp;raquo;</a></nav></div><footer id="footer"><div class="outer"><div id="footer-info"><div class="footer-left"><i class="fa fa-copyright"></i> 2017-2022 冀-18010769-1</div><div class="visit"><span id="busuanzi_container_site_pv" style="display:none"><span id="site-visit" title="本站到访人数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span> </span></span><span>| </span><span id="busuanzi_container_page_pv" style="display:none"><span id="page-visit" title="本站总访问量"><i class="fa fa-eye" aria-hidden="true"></i><span id="busuanzi_value_site_pv"></span></span></span></div><div class="footer-right"><i class="fa fa-heart"></i><a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架"> Hexo</a> Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a></div></div></div></footer></div><script type="application/javascript">var leftWidth,hide=!1;$(".hide-left-col").click(function(){hide=hide?($(".left-col").css("width",leftWidth),$(".left-col .intrude-less").fadeIn(200),$("#tocButton").fadeIn(200),"block"===$("#switch-btn").css("display")&&"block"===$("#switch-area").css("display")||$("#toc").fadeIn(200),$(".hide-left-col").css("left",leftWidth).html('<i class="fa fa-angle-double-left"></i>'),$(".mid-col").css("left",leftWidth),$("#post-nav-button").css("left",leftWidth),$("#post-nav-button > a:nth-child(2)").css("display","block"),!1):(leftWidth=$(".left-col")[0].style.width,$(".left-col").css("width",0),$(".left-col .intrude-less").fadeOut(200),$("#toc").fadeOut(100),$("#tocButton").fadeOut(100),$(".hide-left-col").css("left",0).html('<i class="fa fa-angle-double-right"></i>'),$(".mid-col").css("left",0),$("#post-nav-button").css("left",0),$("#post-nav-button > a:nth-child(2)").css("display","none"),$(".post-list").is(":visible")&&($("#post-nav-button .fa-bars,#post-nav-button .fa-times").toggle(),$(".post-list").toggle()),!0)})</script><script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.3.5/require.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[["$$","$$"],["$","$"],["\\(","\\)"]],processEscapes:!0,skipTags:["script","noscript","style","textarea","pre","code"]}}),MathJax.Hub.Queue(function(){var a,e=MathJax.Hub.getAllJax();for(a=0;a<e.length;a+=1)e[a].SourceElement().parentNode.className+=" has-jax"})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script><div class="scroll" id="scroll"><a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a> <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a> <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a></div><script>var oOpenInNew={post:".copyright a[href]",friends:"#js-friends a",socail:".social a"};for(var x in oOpenInNew)$(oOpenInNew[x]).attr("target","_blank")</script><script>var titleTime,originTitle=document.title;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="(つェ⊂)"+originTitle,clearTimeout(titleTime)):(document.title="(*´∇｀*)~ "+originTitle,titleTime=setTimeout(function(){document.title=originTitle},2e3))})</script><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><link href="//cdn.bootcss.com/aos/2.2.0/aos.css" rel="stylesheet"><script type="text/javascript">AOS.init({easing:"ease-out-back",once:!0})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"live2d_models/live2d-widget-model-izumi"},"display":{"position":"right","width":100,"height":200,"hOffset":-50,"vOffset":-85},"mobile":{"show":false},"react":{"opacityDefault":0.9,"opacityOnHover":0.3},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false});</script></body></html>