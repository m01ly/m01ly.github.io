<!DOCTYPE html><html lang="zh-Hans"><head><!--[if IE]><style>body{display:none;}</style><script>alert('IE浏览器下无法展示效果，请更换浏览器！');var headNode=document.getElementsByTagName('head')[0];var refresh=document.createElement('meta');refresh.setAttribute('http-equiv','Refresh');refresh.setAttribute('Content','0; url=http://outdatedbrowser.com/');headNode.appendChild(refresh);</script><![endif]--><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="m01ly"><meta name="baidu-site-verification" content="yvSXOjM1ag"><meta name="google-site-verification" content="riwnp5QCq6dRKbhBa3d3aDZrfGLQVhMFN9d4fMGYuoU"><meta name="description" content="description123456"><meta property="og:type" content="website"><meta property="og:title" content="Hexo"><meta property="og:url" content="https://m01ly.github.io/page/4/index.html"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="description123456"><meta property="og:locale"><meta property="article:author" content="m01ly"><meta property="article:tag" content="description123456"><meta name="twitter:card" content="summary"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="shortcut icon" href="/favicon.ico"><link href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css" rel="stylesheet"><link rel="stylesheet" href="/css/style.css"><title>Hexo</title><script src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"><script src="//cdn.bootcss.com/aos/2.2.0/aos.js"></script><script>var yiliaConfig={fancybox:!0,isHome:!0,isPost:!1,isArchive:!1,isTag:!1,isCategory:!1,fancybox_js:"//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js",search:!0}</script><script>yiliaConfig.jquery_ui=[!1]</script><script>yiliaConfig.rootUrl="/"</script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/rss+xml">
<link rel="stylesheet" href="/css/prism-a11y-dark.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="container"><div class="left-col"><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><form id="search-form"><input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false"> <i class="fa fa-times" onclick="resetSearch()"></i></form><div id="local-search-result"></div><p class="no-result">No results found <i class="fa fa-spinner fa-pulse"></i></p><div id="switch-btn" class="switch-btn"><div class="icon"><div class="icon-ctn"><div class="icon-wrap icon-house" data-idx="0"><div class="birdhouse"></div><div class="birdhouse_holes"></div></div><div class="icon-wrap icon-ribbon hide" data-idx="1"><div class="ribbon"></div></div><div class="icon-wrap icon-link hide" data-idx="2"><div class="loopback_l"></div><div class="loopback_r"></div></div><div class="icon-wrap icon-me hide" data-idx="3"><div class="user"></div><div class="shoulder"></div></div></div></div><div class="tips-box hide"><div class="tips-arrow"></div><ul class="tips-inner"><li>菜单</li><li>标签</li><li>友情链接</li><li>目标</li></ul></div></div><div id="switch-area" class="switch-area"><div class="switch-wrap"><section class="switch-part switch-part1"><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" rel="noopener" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" rel="noopener" href="https://music.163.com/" title="网易云音乐"></a></ul><ul class="social"><div class="donateIcon-position"><p style="display:block"><a class="donateIcon" href="javascript:void(0)" onmouseout='var qr=document.getElementById("donate");qr.style.display="none"' onmouseenter='var qr=document.getElementById("donate");qr.style.display="block"'>赏</a></p><div id="donate"><img id="multipay" src="/img/multipay.png" width="250px" alt="m01ly Multipay"><div class="triangle"></div></div></div></ul></nav></section><section class="switch-part switch-part2"><div class="widget tagcloud" id="js-tagcloud"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TLS/" rel="tag">TLS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/" rel="tag">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/" rel="tag">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BC%81%E4%B8%9A%E5%AE%89%E5%85%A8%E5%BB%BA%E8%AE%BE/" rel="tag">企业安全建设</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E5%85%A8%E6%89%AB%E6%8F%8F/" rel="tag">安全扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" rel="tag">安装教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8%E6%95%99%E7%A8%8B/" rel="tag">容器教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/" rel="tag">密码学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" rel="tag">插件开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E4%BB%93%E9%87%87%E9%9B%86%E9%A1%B9%E7%9B%AE/" rel="tag">数仓采集项目</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/" rel="tag">日志管理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/" rel="tag">流量分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" rel="tag">渗透测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%8F%E6%B4%9E%E6%89%AB%E6%8F%8F/" rel="tag">漏洞扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%BB%E5%8A%A8%E5%AE%89%E5%85%A8/" rel="tag">移动安全</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%B6%E5%9C%BAwriteup/" rel="tag">靶场writeup</a></li></ul></div></section><section class="switch-part switch-part3"><div id="js-friends"><a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/TechCatsLab">TechCatsLab</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://yangchenglong11.github.io">YangChengLong</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://jsharkc.github.io">LiuJiaChang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://blog.yusank.space">YusanKurban</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.lizebang.top">Lizebang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/sunanxiang">SunAnXiang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/DoubleWoodH">LinHao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.littlechao.top">ShiChao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/Txiaozhe">TangXiaoJi</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/LLLeon">JiaChenHui</a></div></section><section class="switch-part switch-part4"><div id="js-aboutme">不悲不喜，不卑不亢，努力成为一个更好的程序猿！</div></section></div></div></header></div></div><div class="hide-left-col" title="隐藏侧栏"><i class="fa fa-angle-double-left"></i></div><div class="mid-col"><nav id="mobile-nav"><div class="overlay"><div class="slider-trigger"></div><h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">m01ly</a></h1></div><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/" title="回到主页">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li><div class="clearfix"></div></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" href="https://music.163.com/" title="网易云音乐"></a></ul></nav></header></div><link class="menu-list" tags="标签" friends="友情链接" about="目标"></nav><div class="body-wrap"><article id="post-bigdata-hive5-example" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/15/bigdata-hive5-example/" class="article-date"><time class="published" datetime="2020-11-15T07:45:51.000Z" itemprop="datePublished">2020-11-15 发布</time> <time class="updated" datetime="2021-11-16T09:07:44.849Z" itemprop="dateUpdated">2021-11-16 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/15/bigdata-hive5-example/">Hive学习笔记（五） Hive实战</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="1-需求描述"><a href="#1-需求描述" class="headerlink" title="1 需求描述"></a>1 需求描述</h2><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p><p>– 统计视频观看数Top10</p><p>– 统计视频类别热度Top10</p><p>– 统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</p><p>– 统计视频观看数Top50所关联视频的所属类别Rank</p><p>– 统计每个类别中的视频热度Top10,以Music为例</p><p>– 统计每个类别视频观看数Top10</p><p>– 统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频</p><h2 id="2-数据结构"><a href="#2-数据结构" class="headerlink" title="2 数据结构"></a>2 数据结构</h2><p>1）视频表</p><p>视频表</p><table><thead><tr><th>字段</th><th>备注</th><th>详细描述</th></tr></thead><tbody><tr><td>videoId</td><td>视频唯一id（String）</td><td>11位字符串</td></tr><tr><td>uploader</td><td>视频上传者（String）</td><td>上传视频的用户名String</td></tr><tr><td>age</td><td>视频年龄（int）</td><td>视频在平台上的整数天</td></tr><tr><td>category</td><td>视频类别（Array<string>）</string></td><td>上传视频指定的视频分类</td></tr><tr><td>length</td><td>视频长度（Int）</td><td>整形数字标识的视频长度</td></tr><tr><td>views</td><td>观看次数（Int）</td><td>视频被浏览的次数</td></tr><tr><td>rate</td><td>视频评分（Double）</td><td>满分5分</td></tr><tr><td>Ratings</td><td>流量（Int）</td><td>视频的流量，整型数字</td></tr><tr><td>conments</td><td>评论数（Int）</td><td>一个视频的整数评论数</td></tr><tr><td>relatedId</td><td>相关视频id（Array<string>）</string></td><td>相关视频的id，最多20个</td></tr></tbody></table><p>2）用户表</p><p>用户表</p><table><thead><tr><th>字段</th><th>备注</th><th>字段类型</th></tr></thead><tbody><tr><td>uploader</td><td>上传者用户名</td><td>string</td></tr><tr><td>videos</td><td>上传视频数</td><td>int</td></tr><tr><td>friends</td><td>朋友数量</td><td>int</td></tr></tbody></table><h2 id="3-准备工作"><a href="#3-准备工作" class="headerlink" title="3 准备工作"></a>3 准备工作</h2><h3 id="3-1-ETL"><a href="#3-1-ETL" class="headerlink" title="3.1 ETL"></a>3.1 ETL</h3><p><strong>ETL即数据预处理。</strong></p><p>原始数据一行展示：</p><pre class="line-numbers language-bash"><code class="language-bash">LKh7zAJ4nwo    TheReceptionist    653    People <span class="token operator">&amp;</span> Blogs    424    13021    4.34    1305    744    DjdA-5oKYFQ    NxTDlnOuybo    c-8VuICzXtU    DH56yrIO5nI    W1Uo5DQTtzc    E-3zXq_r4w0    1TCeoRPg5dE    yAr26YhuYNY    2ZgXx72XmoE    -7ClGo-YgZ0    vmdPOOd6cxI    KRHfMQqSHpk    pIMpORZthYw    1tUDzOp10pk    heqocRij5P0    _XIuvoH6rUg    LGVU5DsezE0    uO2kj6_D8B4    xiDqywcDQRM    uX81lMev6_o<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用“\t”进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用“&amp;”分割，同时去掉两边空格，多个相关视频id也使用“&amp;”进行分割。</p><p>1）ETL之封装工具类:用于具体处理数据的工具</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">ETLUtil</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
  <span class="token comment" spellcheck="true">/**
   \* 数据清洗方法
   */</span>
  <span class="token keyword">public</span> <span class="token keyword">static</span> String <span class="token function">etlData</span><span class="token punctuation">(</span>String srcData<span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
​    StringBuffer resultData <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">StringBuffer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​    <span class="token comment" spellcheck="true">//1. 先将数据通过\t 切割</span>
​    String<span class="token punctuation">[</span><span class="token punctuation">]</span> datas <span class="token operator">=</span> srcData<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">"\t"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​    <span class="token comment" spellcheck="true">//2. 判断长度是否小于9</span>
​    <span class="token keyword">if</span><span class="token punctuation">(</span>datas<span class="token punctuation">.</span>length <span class="token operator">&lt;</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
​      <span class="token keyword">return</span> null <span class="token punctuation">;</span>
​    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
​    <span class="token comment" spellcheck="true">//3. 将数据中的视频类别的空格去掉</span>
​    datas<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">=</span>datas<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">replaceAll</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span><span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​     <span class="token comment" spellcheck="true">//4. 将数据中的关联视频id通过&amp;拼接</span>
​    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> datas<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
​      <span class="token keyword">if</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
​        <span class="token comment" spellcheck="true">//4.1 没有关联视频的情况</span>
​        <span class="token keyword">if</span><span class="token punctuation">(</span>i <span class="token operator">==</span> datas<span class="token punctuation">.</span>length<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
​          resultData<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>datas<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token keyword">else</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
​          resultData<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>datas<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span><span class="token string">"\t"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
​      <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token keyword">else</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
​        <span class="token comment" spellcheck="true">//4.2 有关联视频的情况</span>
​        <span class="token keyword">if</span><span class="token punctuation">(</span>i <span class="token operator">==</span> datas<span class="token punctuation">.</span>length<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
​          resultData<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>datas<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token keyword">else</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
​          resultData<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>datas<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span><span class="token string">"&amp;"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
​      <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
​    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
​    <span class="token keyword">return</span> resultData<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）ETL之Mapper</p><pre class="line-numbers language-java"><code class="language-java">  <span class="token comment" spellcheck="true">/**
 \* 清洗谷粒影音的原始数据
 \* 清洗规则
 \* 1. 将数据长度小于9的清洗掉
 \*  2. 将数据中的视频类别中间的空格去掉  People &amp; Blogs
 \* 3. 将数据中的关联视频id通过&amp;符号拼接
 */</span>
 <span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">EtlMapper</span> <span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token operator">&lt;</span>LongWritable<span class="token punctuation">,</span> Text<span class="token punctuation">,</span>Text<span class="token punctuation">,</span> NullWritable<span class="token operator">></span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
   <span class="token keyword">private</span> Text k <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
   <span class="token annotation punctuation">@Override</span>
   <span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span>LongWritable key<span class="token punctuation">,</span> Text value<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token comment" spellcheck="true">//获取一行</span>
     String line <span class="token operator">=</span> value<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     <span class="token comment" spellcheck="true">//清洗</span>
     String resultData <span class="token operator">=</span> ETLUtil<span class="token punctuation">.</span><span class="token function">etlData</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">;</span>
​     <span class="token keyword">if</span><span class="token punctuation">(</span>resultData <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
​       <span class="token comment" spellcheck="true">//写出</span>
​       k<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>resultData<span class="token punctuation">)</span><span class="token punctuation">;</span>
​       context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span>NullWritable<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​     <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
   <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）ETL之Driver</p><pre class="line-numbers language-java"><code class="language-java"> <span class="token keyword">package</span> com<span class="token punctuation">.</span>molly<span class="token punctuation">.</span>gulivideo<span class="token punctuation">.</span>etl<span class="token punctuation">;</span>
 <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>conf<span class="token punctuation">.</span>Configuration<span class="token punctuation">;</span>
 <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>fs<span class="token punctuation">.</span>Path<span class="token punctuation">;</span>
 <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>NullWritable<span class="token punctuation">;</span>
 <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span>
 <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Job<span class="token punctuation">;</span>
 <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>input<span class="token punctuation">.</span>FileInputFormat<span class="token punctuation">;</span>
 <span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>output<span class="token punctuation">.</span>FileOutputFormat<span class="token punctuation">;</span>
 <span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">EtlDriver</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
   <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
     Configuration conf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     Job job <span class="token operator">=</span> Job<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span>conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
     job<span class="token punctuation">.</span><span class="token function">setJarByClass</span><span class="token punctuation">(</span>EtlDriver<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>EtlMapper<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     job<span class="token punctuation">.</span><span class="token function">setMapOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span>NullWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     job<span class="token punctuation">.</span><span class="token function">setOutputValueClass</span><span class="token punctuation">(</span>NullWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     job<span class="token punctuation">.</span><span class="token function">setNumReduceTasks</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     FileInputFormat<span class="token punctuation">.</span><span class="token function">setInputPaths</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     FileOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
   <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
 <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4）将ETL程序打包为etl.jar 并上传到Linux的 /opt/module/hive/datas 目录下</p><p>5）上传原始数据到HDFS</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 datas<span class="token punctuation">]</span> <span class="token function">pwd</span>
/opt/module/hive/datas
<span class="token punctuation">[</span>molly@hadoop102 datas<span class="token punctuation">]</span> hadoop fs -mkdir -p /gulivideo/video
<span class="token punctuation">[</span>molly@hadoop102 datas<span class="token punctuation">]</span> hadoop fs -mkdir -p /gulivideo/user
<span class="token punctuation">[</span>molly@hadoop102 datas<span class="token punctuation">]</span> hadoop fs -put gulivideo/user/user.txt  /gulivideo/user
<span class="token punctuation">[</span>molly@hadoop102 datas<span class="token punctuation">]</span> hadoop fs -put gulivideo/video/*.txt  /gulivideo/video<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>6）ETL数据</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 datas<span class="token punctuation">]</span> hadoop jar  etl.jar  com.molly.hive.etl.EtlDriver /gulivideo/video /gulivideo/video/output<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-2-准备表"><a href="#3-2-准备表" class="headerlink" title="3.2 准备表"></a>3.2 准备表</h3><p>1）需要准备的表</p><p><strong>因为我们想要创建压缩表，但是压缩表不能直接导入数据，因此我们采用两个中转表（原始表），然后从原始表导入压缩表（最终表）。</strong></p><p>创建原始数据表：gulivideo_ori，gulivideo_user_ori，</p><p>创建最终表：gulivideo_orc，gulivideo_user_orc</p><p>2）创建原始数据表：</p><p>（1）gulivideo_ori</p><pre class="line-numbers language-bash"><code class="language-bash">create table gulivideo_ori<span class="token punctuation">(</span>
  videoId string, 
  uploader string, 
  age int, 
  category array<span class="token operator">&lt;</span>string<span class="token operator">></span>, 
  length int, 
  views int, 
  rate float, 
  ratings int, 
  comments int,
  relatedId array<span class="token operator">&lt;</span>string<span class="token operator">></span><span class="token punctuation">)</span>
row <span class="token function">format</span> delimited fields terminated by <span class="token string">"\t"</span>
collection items terminated by <span class="token string">"&amp;"</span>
stored as textfile<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）创建原始数据表: gulivideo_user_ori</p><pre class="line-numbers language-bash"><code class="language-bash">create table gulivideo_user_ori<span class="token punctuation">(</span>
  uploader string,
  videos int,
  friends int<span class="token punctuation">)</span>
row <span class="token function">format</span> delimited 
fields terminated by <span class="token string">"\t"</span> 
stored as textfile<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>1） 创建orc存储格式带snappy压缩的表：</p><p>（1）gulivideo_orc</p><pre class="line-numbers language-bash"><code class="language-bash">create table gulivideo_orc<span class="token punctuation">(</span>
  videoId string, 
  uploader string, 
  age int, 
  category array<span class="token operator">&lt;</span>string<span class="token operator">></span>, 
  length int, 
  views int, 
  rate float, 
  ratings int, 
  comments int,
  relatedId array<span class="token operator">&lt;</span>string<span class="token operator">></span><span class="token punctuation">)</span>
stored as orc
tblproperties<span class="token punctuation">(</span><span class="token string">"orc.compress"</span><span class="token operator">=</span><span class="token string">"SNAPPY"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）gulivideo_user_orc</p><pre class="line-numbers language-bash"><code class="language-bash">create table gulivideo_user_orc<span class="token punctuation">(</span>
  uploader string,
  videos int,
  friends int<span class="token punctuation">)</span>
row <span class="token function">format</span> delimited 
fields terminated by <span class="token string">"\t"</span> 
stored as orc
tblproperties<span class="token punctuation">(</span><span class="token string">"orc.compress"</span><span class="token operator">=</span><span class="token string">"SNAPPY"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）向ori表插入数据</p><pre class="line-numbers language-bash"><code class="language-bash">load data inpath <span class="token string">"/gulivideo/video/output"</span> into table gulivideo_ori<span class="token punctuation">;</span>
load data inpath <span class="token string">"/gulivideo/user"</span> into table gulivideo_user_ori<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（4）向orc表插入数据</p><pre class="line-numbers language-bash"><code class="language-bash">insert into table gulivideo_orc <span class="token keyword">select</span> * from gulivideo_ori<span class="token punctuation">;</span>
insert into table gulivideo_user_orc <span class="token keyword">select</span> * from gulivideo_user_ori<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="4-业务分析"><a href="#4-业务分析" class="headerlink" title="4 业务分析"></a>4 业务分析</h2><h3 id="4-1-统计视频观看数Top10"><a href="#4-1-统计视频观看数Top10" class="headerlink" title="4.1 统计视频观看数Top10"></a>4.1 统计视频观看数Top10</h3><p>思路：使用order by按照views字段做一个全局排序即可，同时我们设置只显示前10条。</p><p>最终代码：</p><pre class="line-numbers language-bash"><code class="language-bash">SELECT videoId,views 
FROM gulivideo_orc
ORDER BY views DESC 
LIMIT 10<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-2-统计视频类别热度Top10"><a href="#4-2-统计视频类别热度Top10" class="headerlink" title="4.2 统计视频类别热度Top10"></a>4.2 统计视频类别热度Top10</h3><p>思路：</p><p>（1）即统计每个类别有多少个视频，显示出包含视频最多的前10个类别。</p><p>（2）我们需要按照类别group by聚合，然后count组内的videoId个数即可。</p><p>（3）因为当前表结构为：一个视频对应一个或多个类别。所以如果要group by类别，需要先将类别进行列转行(展开)，然后再进行count即可。</p><p>（4）最后按照热度排序，显示前10条。</p><p>最终代码：</p><pre class="line-numbers language-bash"><code class="language-bash">SELECT 
  t1.category_name , COUNT<span class="token punctuation">(</span>t1.videoId<span class="token punctuation">)</span> hot
FROM 
<span class="token punctuation">(</span>
SELECT videoId, category_name 
FROM gulivideo_orc 
lateral VIEW explode<span class="token punctuation">(</span>category<span class="token punctuation">)</span> gulivideo_orc_tmp AS category_name
<span class="token punctuation">)</span> t1
GROUP BY t1.category_name 
ORDER BY hot DESC 
LIMIT 10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-3-统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数"><a href="#4-3-统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数" class="headerlink" title="4.3 统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数"></a>4.3 统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</h3><p>思路：</p><p>（1）先找到观看数最高的20个视频所属条目的所有信息，降序排列</p><p>（2）把这20条信息中的category分裂出来(列转行)</p><p>（3）最后查询视频分类名称和该分类下有多少个Top20的视频</p><p>最终代码：</p><pre class="line-numbers language-bash"><code class="language-bash">SELECT  t2.category_name, COUNT<span class="token punctuation">(</span>t2.videoId<span class="token punctuation">)</span> video_sum
 FROM 
 <span class="token punctuation">(</span>
 SELECT  t1.videoId,  category_name
 FROM 
 <span class="token punctuation">(</span>
 SELECT  videoId,  views , category 
 FROM  gulivideo_orc
 ORDER BY  views  DESC 
 LIMIT 20 
<span class="token punctuation">)</span> t1
lateral VIEW explode<span class="token punctuation">(</span>t1.category<span class="token punctuation">)</span> t1_tmp AS category_name
<span class="token punctuation">)</span> t2
GROUP BY t2.category_name<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-4-统计视频观看数Top50所关联视频的所属类别排序"><a href="#4-4-统计视频观看数Top50所关联视频的所属类别排序" class="headerlink" title="4.4 统计视频观看数Top50所关联视频的所属类别排序"></a>4.4 统计视频观看数Top50所关联视频的所属类别排序</h3><p>代码：</p><pre class="line-numbers language-bash"><code class="language-bash">SELECT t6.category_name, t6.video_sum, rank<span class="token punctuation">(</span><span class="token punctuation">)</span> over<span class="token punctuation">(</span>ORDER BY t6.video_sum DESC <span class="token punctuation">)</span> rk
FROM
 <span class="token punctuation">(</span>
 SELECT t5.category_name, COUNT<span class="token punctuation">(</span>t5.relatedid_id<span class="token punctuation">)</span> video_sum
 FROM
 <span class="token punctuation">(</span>
 SELECT t4.relatedid_id, category_name
 FROM
 <span class="token punctuation">(</span>
 SELECT   t2.relatedid_id , t3.category  
 FROM 
 <span class="token punctuation">(</span>
 SELECT   relatedid_id
 FROM 
 <span class="token punctuation">(</span>
 SELECT  videoId,   views, relatedid 
 FROM  gulivideo_orc
 ORDER BY views  DESC  LIMIT 50
 <span class="token punctuation">)</span>t1
 lateral VIEW explode<span class="token punctuation">(</span>t1.relatedid<span class="token punctuation">)</span> t1_tmp AS relatedid_id
 <span class="token punctuation">)</span>t2 
 JOIN 
  gulivideo_orc t3 
 ON 
  t2.relatedid_id <span class="token operator">=</span> t3.videoId 
<span class="token punctuation">)</span> t4 
lateral VIEW explode<span class="token punctuation">(</span>t4.category<span class="token punctuation">)</span> t4_tmp AS category_name
<span class="token punctuation">)</span> t5
GROUP BY t5.category_name
ORDER BY  video_sum DESC 
<span class="token punctuation">)</span> t6<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-5-统计每个类别中的视频热度Top10，以Music为例"><a href="#4-5-统计每个类别中的视频热度Top10，以Music为例" class="headerlink" title="4.5 统计每个类别中的视频热度Top10，以Music为例"></a>4.5 统计每个类别中的视频热度Top10，以Music为例</h3><p>思路：</p><p>（1）要想统计Music类别中的视频热度Top10，需要先找到Music类别，那么就需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。</p><p>（2）向category展开的表中插入数据。</p><p>（3）统计对应类别（Music）中的视频热度。</p><p>统计Music类别的Top10（也可以统计其他）</p><pre class="line-numbers language-bash"><code class="language-bash">SELECT t1.videoId,  t1.views, t1.category_name
 FROM 
 <span class="token punctuation">(</span>
 SELECT  videoId, views, category_name
 FROM gulivideo_orc
 lateral VIEW explode<span class="token punctuation">(</span>category<span class="token punctuation">)</span> gulivideo_orc_tmp AS category_name 
<span class="token punctuation">)</span>t1  
 WHERE   t1.category_name <span class="token operator">=</span> <span class="token string">"Music"</span> 
 ORDER BY   t1.views DESC 
LIMIT 10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-6-统计每个类别视频观看数Top10"><a href="#4-6-统计每个类别视频观看数Top10" class="headerlink" title="4.6 统计每个类别视频观看数Top10"></a>4.6 统计每个类别视频观看数Top10</h3><p>最终代码：</p><pre class="line-numbers language-bash"><code class="language-bash">SELECT   t2.videoId, t2.views,  t2.category_name,  t2.rk
FROM 
<span class="token punctuation">(</span>
SELECT  t1.videoId,  t1.views, t1.category_name,
rank<span class="token punctuation">(</span><span class="token punctuation">)</span> over<span class="token punctuation">(</span>PARTITION BY t1.category_name ORDER BY t1.views DESC <span class="token punctuation">)</span> rk
FROM  
<span class="token punctuation">(</span>
SELECT videoId, views,  category_name
FROM gulivideo_orc
lateral VIEW explode<span class="token punctuation">(</span>category<span class="token punctuation">)</span> gulivideo_orc_tmp AS category_name
<span class="token punctuation">)</span>t1
<span class="token punctuation">)</span>t2
WHERE t2.rk <span class="token operator">&lt;=</span>10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-7-统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频"><a href="#4-7-统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频" class="headerlink" title="4.7 统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频"></a>4.7 统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频</h3><p>思路：</p><p>（1）求出上传视频最多的10个用户</p><p>（2）关联gulivideo_orc表，求出这10个用户上传的所有的视频，按照观看数取前20</p><p>最终代码:</p><pre class="line-numbers language-bash"><code class="language-bash">SELECT  t2.videoId,  t2.views,  t2.uploader
FROM
<span class="token punctuation">(</span>
 SELECT  uploader, videos
 FROM gulivideo_user_orc 
 ORDER BY  videos DESC LIMIT 10  
 <span class="token punctuation">)</span> t1
 JOIN gulivideo_orc t2 
 ON t1.uploader <span class="token operator">=</span> t2.uploader
 ORDER BY   t2.views  DESC
 LIMIT 20<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li></ul></div><span class="post-count">总字数2.2k</span> <span class="post-count">预计阅读10分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-hive4-optimize" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/15/bigdata-hive4-optimize/" class="article-date"><time class="published" datetime="2020-11-15T07:45:51.000Z" itemprop="datePublished">2020-11-15 发布</time> <time class="updated" datetime="2021-11-16T07:31:40.892Z" itemprop="dateUpdated">2021-11-16 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/15/bigdata-hive4-optimize/">Hive学习笔记（四） Hive的企业级调优</a></h1></header><div class="article-entry" itemprop="articleBody"><p>对于Hive的操作是面对大数据层面，因此对于查询效率是有要求的，这篇主要从以下几个方面进行调优。</p><p>总结:优化：<br>1 设置抓取为more:set hive.fetch.task.conversion=more;<br>2 开启本地模式:配置小数据量放到本地跑<br>set hive.exec.mode.local.auto=true; //开启本地mr<br>//设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128M<br>set hive.exec.mode.local.auto.inputbytes.max=50000000;<br>//设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4<br>set hive.exec.mode.local.auto.input.files.max=10;</p><h1 id="1-执行计划（Explain）"><a href="#1-执行计划（Explain）" class="headerlink" title="1 执行计划（Explain）"></a>1 执行计划（Explain）</h1><p>explain很详细的看到语句执行过程中发生的事情。</p><h2 id="1-1基本语法"><a href="#1-1基本语法" class="headerlink" title="1.1基本语法"></a>1.1基本语法</h2><p>EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query</p><p>Explain主要是分析一下sql的执行过程。</p><h2 id="1-2-案例实操"><a href="#1-2-案例实操" class="headerlink" title="1.2 案例实操"></a>1.2 案例实操</h2><p>（1）查看下面这条语句的执行计划</p><p>没有生成MR任务的</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> explain <span class="token keyword">select</span> * from emp<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>有生成MR任务的</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> explain <span class="token keyword">select</span> deptno, avg<span class="token punctuation">(</span>sal<span class="token punctuation">)</span> avg_sal from emp group by deptno<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查看详细执行计划</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> explain extended <span class="token keyword">select</span> * from emp<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> explain extended <span class="token keyword">select</span> deptno, avg<span class="token punctuation">(</span>sal<span class="token punctuation">)</span> avg_sal from emp group by deptno<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h1 id="2-Fetch抓取"><a href="#2-Fetch抓取" class="headerlink" title="2 Fetch抓取"></a>2 Fetch抓取</h1><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。</p><p>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>property<span class="token operator">></span>
  <span class="token operator">&lt;</span>name<span class="token operator">></span>hive.fetch.task.conversion<span class="token operator">&lt;</span>/name<span class="token operator">></span>
  <span class="token operator">&lt;</span>value<span class="token operator">></span>more<span class="token operator">&lt;</span>/value<span class="token operator">></span>
  <span class="token operator">&lt;</span>description<span class="token operator">></span>
   Expects one of <span class="token punctuation">[</span>none, minimal, more<span class="token punctuation">]</span>.
   Some <span class="token keyword">select</span> queries can be converted to single FETCH task minimizing latency.
   Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts <span class="token punctuation">(</span>which incurs RS<span class="token punctuation">)</span>, lateral views and joins.
   \0. none <span class="token keyword">:</span> disable hive.fetch.task.conversion
   \1. minimal <span class="token keyword">:</span> SELECT STAR, FILTER on partition columns, LIMIT only
   \2. <span class="token function">more</span> <span class="token keyword">:</span> SELECT, FILTER, LIMIT only <span class="token punctuation">(</span>support TABLESAMPLE and virtual columns<span class="token punctuation">)</span>
  <span class="token operator">&lt;</span>/description<span class="token operator">></span>
<span class="token operator">&lt;</span>/property<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-1-案例实操："><a href="#2-1-案例实操：" class="headerlink" title="2.1 案例实操："></a>2.1 案例实操：</h2><p>（1）把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> hive.fetch.task.conversion<span class="token operator">=</span>none<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> ename from emp<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> ename from emp limit 3<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>（2）把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> hive.fetch.task.conversion<span class="token operator">=</span>more<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> ename from emp<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> ename from emp limit 3<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-本地模式"><a href="#3-本地模式" class="headerlink" title="3 本地模式"></a>3 本地模式</h1><p>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</p><p>用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。</p><p>set hive.exec.mode.local.auto=true; //开启本地mr</p><p>//设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128M</p><p>set hive.exec.mode.local.auto.inputbytes.max=50000000;</p><p>//设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4</p><p>set hive.exec.mode.local.auto.input.files.max=10;</p><h2 id="3-1-案例实操："><a href="#3-1-案例实操：" class="headerlink" title="3.1 案例实操："></a>3.1 案例实操：</h2><p>（1）开启本地模式，并执行查询语句</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> hive.exec.mode.local.auto<span class="token operator">=</span>true<span class="token punctuation">;</span> 
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp cluster by deptno<span class="token punctuation">;</span>
Time taken: 1.328 seconds, Fetched: 14 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（2）关闭本地模式，并执行查询语句</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> hive.exec.mode.local.auto<span class="token operator">=</span>false<span class="token punctuation">;</span> 
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp cluster by deptno<span class="token punctuation">;</span>
Time taken: 20.09 seconds, Fetched: 14 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h1 id="4-表的优化"><a href="#4-表的优化" class="headerlink" title="4 表的优化"></a>4 表的优化</h1><h2 id="4-1-小表大表Join-MapJoin"><a href="#4-1-小表大表Join-MapJoin" class="headerlink" title="4.1 小表大表Join(MapJoin)"></a>4.1 小表大表Join(MapJoin)</h2><p>将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成join。</p><p>实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</p><p>案例实操</p><p>1）需求</p><p>测试大表JOIN小表和小表JOIN大表的效率</p><p>2）开启MapJoin参数设置</p><p>（1）设置自动选择Mapjoin</p><p>set hive.auto.convert.join = true; 默认为true</p><p>（2）大表小表的阈值设置（默认25M以下认为是小表）：</p><p>set hive.mapjoin.smalltable.filesize = 25000000;</p><p>3）MapJoin工作机制</p><p>​ <img src="/2020/11/15/bigdata-hive4-optimize/1637047033227.png" alt="1637047033227"></p><p>4）建大表、小表和JOIN后表的语句</p><pre class="line-numbers language-bash"><code class="language-bash">// 创建大表
create table bigtable<span class="token punctuation">(</span>id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
 // 创建小表
create table smalltable<span class="token punctuation">(</span>id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
// 创建join后表的语句
create table jointable<span class="token punctuation">(</span>id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5）分别向大表和小表中导入数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> load data local inpath <span class="token string">'/opt/module/hive/datas/bigtable'</span> into table bigtable<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>load data local inpath <span class="token string">'/opt/module/hive/datas/smalltable'</span> into table smalltable<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>6）小表JOIN大表语句</p><pre class="line-numbers language-bash"><code class="language-bash">insert overwrite table jointable
<span class="token keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
from smalltable s
<span class="token function">join</span> bigtable b
on b.id <span class="token operator">=</span> s.id<span class="token punctuation">;</span>
Time taken: 35.921 seconds
No rows affected <span class="token punctuation">(</span>44.456 seconds<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>7）执行大表JOIN小表语句</p><pre class="line-numbers language-bash"><code class="language-bash">insert overwrite table jointable
<span class="token keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
from bigtable b
<span class="token function">join</span> smalltable s
on s.id <span class="token operator">=</span> b.id<span class="token punctuation">;</span>
Time taken: 34.196 seconds
No rows affected <span class="token punctuation">(</span>26.287 seconds<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-2-大表Join大表"><a href="#4-2-大表Join大表" class="headerlink" title="4.2 大表Join大表"></a>4.2 大表Join大表</h2><h3 id="4-2-1-空KEY过滤"><a href="#4-2-1-空KEY过滤" class="headerlink" title="4.2.1 空KEY过滤"></a>4.2.1 空KEY过滤</h3><p>有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：</p><p>案例实操</p><p>（1）配置历史服务器</p><p>配置mapred-site.xml</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>property<span class="token operator">></span>
<span class="token operator">&lt;</span>name<span class="token operator">></span>mapreduce.jobhistory.address<span class="token operator">&lt;</span>/name<span class="token operator">></span>
<span class="token operator">&lt;</span>value<span class="token operator">></span>hadoop102:10020<span class="token operator">&lt;</span>/value<span class="token operator">></span>
<span class="token operator">&lt;</span>/property<span class="token operator">></span>
<span class="token operator">&lt;</span>property<span class="token operator">></span>
  <span class="token operator">&lt;</span>name<span class="token operator">></span>mapreduce.jobhistory.webapp.address<span class="token operator">&lt;</span>/name<span class="token operator">></span>
  <span class="token operator">&lt;</span>value<span class="token operator">></span>hadoop102:19888<span class="token operator">&lt;</span>/value<span class="token operator">></span>
<span class="token operator">&lt;</span>/property<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>启动历史服务器</p><pre class="line-numbers language-bash"><code class="language-bash">sbin/mr-jobhistory-daemon.sh start historyserver<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查看jobhistory</p><p><a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p><p>（2）创建原始数据表、空id表、合并后数据表</p><p>// 创建空id表</p><pre class="line-numbers language-bash"><code class="language-bash">create table nullidtable<span class="token punctuation">(</span>id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）分别加载原始数据和空id数据到对应表中</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> load data local inpath <span class="token string">'/opt/module/hive/datas/nullid'</span> into table nullidtable<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）测试不过滤空id</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite table jointable <span class="token keyword">select</span> n.* from nullidtable n
left <span class="token function">join</span> bigtable o on n.id <span class="token operator">=</span> o.id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（5）测试过滤空id</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite table jointable <span class="token keyword">select</span> n.* from <span class="token punctuation">(</span>select * from nullidtable where <span class="token function">id</span> is not null <span class="token punctuation">)</span> n left <span class="token function">join</span> bigtable o on n.id <span class="token operator">=</span> o.id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-2-2-空key转换"><a href="#4-2-2-空key转换" class="headerlink" title="4.2.2 空key转换"></a>4.2.2 空key转换</h3><p>有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如：</p><p>案例实操：</p><p>不随机分布空null值：</p><p>（1）设置5个reduce个数</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token keyword">set</span> mapreduce.job.reduces <span class="token operator">=</span> 5<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）JOIN两张表</p><pre class="line-numbers language-bash"><code class="language-bash">insert overwrite table jointable

<span class="token keyword">select</span> n.* from nullidtable n left <span class="token function">join</span> bigtable b on n.id <span class="token operator">=</span> b.id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>结果：如下图所示，可以看出来，出现了数据倾斜，某些reducer的资源消耗远大于其他reducer。</p><p>​ <img src="/2020/11/15/bigdata-hive4-optimize/1637047154377.png" alt="1637047154377"></p><p>随机分布空null值</p><p>（1）设置5个reduce个数</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token keyword">set</span> mapreduce.job.reduces <span class="token operator">=</span> 5<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）JOIN两张表</p><pre class="line-numbers language-bash"><code class="language-bash">insert overwrite table jointable
<span class="token keyword">select</span> n.* from nullidtable n full <span class="token function">join</span> bigtable o on 
nvl<span class="token punctuation">(</span>n.id,rand<span class="token punctuation">(</span><span class="token punctuation">))</span> <span class="token operator">=</span> o.id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>结果：如下图所示，可以看出来，消除了数据倾斜，负载均衡reducer的资源消耗</p><p>​ <img src="/2020/11/15/bigdata-hive4-optimize/1637047177458.png" alt="1637047177458"></p><h3 id="4-2-3-SMB-Sort-Merge-Bucket-join"><a href="#4-2-3-SMB-Sort-Merge-Bucket-join" class="headerlink" title="4.2.3 SMB(Sort Merge Bucket join)"></a>4.2.3 SMB(Sort Merge Bucket join)</h3><p>（1）创建第二张大表</p><pre class="line-numbers language-bash"><code class="language-bash">create table bigtable2<span class="token punctuation">(</span>
  <span class="token function">id</span> bigint,
  t bigint,
  uid string,
  keyword string,
  url_rank int,
  click_num int,
  click_url string<span class="token punctuation">)</span>
row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
load data local inpath <span class="token string">'/opt/module/data/bigtable'</span> into table bigtable2<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>测试大表直接JOIN</p><pre class="line-numbers language-bash"><code class="language-bash">insert overwrite table jointable
<span class="token keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
from bigtable s
<span class="token function">join</span> bigtable2 b
on b.id <span class="token operator">=</span> s.id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）创建分桶表1,桶的个数不要超过可用CPU的核数</p><pre class="line-numbers language-bash"><code class="language-bash">create table bigtable_buck1<span class="token punctuation">(</span>
  <span class="token function">id</span> bigint,
  t bigint,
  uid string,
  keyword string,
  url_rank int,
  click_num int,
  click_url string<span class="token punctuation">)</span>
clustered by<span class="token punctuation">(</span>id<span class="token punctuation">)</span> 
sorted by<span class="token punctuation">(</span>id<span class="token punctuation">)</span>
into 6 buckets
row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
insert into bigtable_buck1 <span class="token keyword">select</span> * from bigtable<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）创建分通表2,桶的个数不要超过可用CPU的核数</p><pre class="line-numbers language-bash"><code class="language-bash">create table bigtable_buck2<span class="token punctuation">(</span>
  <span class="token function">id</span> bigint,
  t bigint,
  uid string,
  keyword string,
  url_rank int,
  click_num int,
  click_url string<span class="token punctuation">)</span>
clustered by<span class="token punctuation">(</span>id<span class="token punctuation">)</span>
sorted by<span class="token punctuation">(</span>id<span class="token punctuation">)</span> 
into 6 buckets
row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
insert into bigtable_buck2 <span class="token keyword">select</span> * from bigtable<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（4）设置参数</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token keyword">set</span> hive.optimize.bucketmapjoin <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive.input.format<span class="token operator">=</span>org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（5）测试</p><pre class="line-numbers language-bash"><code class="language-bash">insert overwrite table jointable
<span class="token keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
from bigtable_buck1 s
<span class="token function">join</span> bigtable_buck2 b
on b.id <span class="token operator">=</span> s.id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-3-Group-By"><a href="#4-3-Group-By" class="headerlink" title="4.3 Group By"></a>4.3 Group By</h2><p>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。</p><p>​ <img src="/2020/11/15/bigdata-hive4-optimize/1637047313467.png" alt="1637047313467"></p><p>并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p><h3 id="4-3-1-开启Map端聚合参数设置"><a href="#4-3-1-开启Map端聚合参数设置" class="headerlink" title="4.3.1 开启Map端聚合参数设置"></a>4.3.1 开启Map端聚合参数设置</h3><p>（1）是否在Map端进行聚合，默认为True</p><p>set hive.map.aggr = true</p><p>（2）在Map端进行聚合操作的条目数目</p><p>set hive.groupby.mapaggr.checkinterval = 100000</p><p>（3）有数据倾斜的时候进行负载均衡（默认是false）</p><p>set hive.groupby.skewindata = true</p><p>当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> deptno from emp group by deptno<span class="token punctuation">;</span>
Stage-Stage-1: Map: 1 Reduce: 5  Cumulative CPU: 23.68 sec  HDFS Read: 19987 HDFS Write: 9 SUCCESS
Total MapReduce CPU Time Spent: 23 seconds 680 msec
OK
deptno
10
20
30<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>优化以后</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> hive.groupby.skewindata <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> deptno from emp group by deptno<span class="token punctuation">;</span>
Stage-Stage-1: Map: 1 Reduce: 5  Cumulative CPU: 28.53 sec  HDFS Read: 18209 HDFS Write: 534 SUCCESS
Stage-Stage-2: Map: 1 Reduce: 5  Cumulative CPU: 38.32 sec  HDFS Read: 15014 HDFS Write: 9 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 6 seconds 850 msec
OK
deptno
10
20
30<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-4-Count-Distinct-去重统计"><a href="#4-4-Count-Distinct-去重统计" class="headerlink" title="4.4 Count(Distinct) 去重统计"></a>4.4 Count(Distinct) 去重统计</h2><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换,但是需要注意group by造成的数据倾斜问题.</p><p>1） 案例实操</p><p>（1）创建一张大表</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create table bigtable<span class="token punctuation">(</span>id bigint, <span class="token function">time</span> bigint, uid string, keyword
string, url_rank int, click_num int, click_url string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited
fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（2）加载数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> load data local inpath <span class="token string">'/opt/module/datas/bigtable'</span> into table bigtable<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）设置5个reduce个数</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token keyword">set</span> mapreduce.job.reduces <span class="token operator">=</span> 5<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）执行去重id查询</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> count<span class="token punctuation">(</span>distinct id<span class="token punctuation">)</span> from bigtable<span class="token punctuation">;</span>
Stage-Stage-1: Map: 1 Reduce: 1  Cumulative CPU: 7.12 sec  HDFS Read: 120741990 HDFS Write: 7 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 120 msec
OK
c0
100001
Time taken: 23.607 seconds, Fetched: 1 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（5）采用GROUP by去重id</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> count<span class="token punctuation">(</span>id<span class="token punctuation">)</span> from <span class="token punctuation">(</span>select <span class="token function">id</span> from bigtable group by id<span class="token punctuation">)</span> a<span class="token punctuation">;</span>
Stage-Stage-1: Map: 1 Reduce: 5  Cumulative CPU: 17.53 sec  HDFS Read: 120752703 HDFS Write: 580 SUCCESS
Stage-Stage-2: Map: 1 Reduce: 1  Cumulative CPU: 4.29 sec2  HDFS Read: 9409 HDFS Write: 7 SUCCESS
Total MapReduce CPU Time Spent: 21 seconds 820 msec
OK
_c0
100001
Time taken: 50.795 seconds, Fetched: 1 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。</p><h2 id="4-5-笛卡尔积"><a href="#4-5-笛卡尔积" class="headerlink" title="4.5 笛卡尔积"></a>4.5 笛卡尔积</h2><p>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p><h2 id="4-6-行列过滤"><a href="#4-6-行列过滤" class="headerlink" title="4.6 行列过滤"></a>4.6 行列过滤</h2><p>列处理：在SELECT中，只拿需要的列，如果有分区，尽量使用分区过滤，少用SELECT *。</p><p>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤，比如：</p><p>案例实操：</p><p>1）测试先关联两张表，再用where条件过滤</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> o.id from bigtable b
<span class="token function">join</span> bigtable  o.id <span class="token operator">=</span> b.id
where o.id <span class="token operator">&lt;=</span> 10<span class="token punctuation">;</span>
Time taken: 34.406 seconds, Fetched: 100 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>2）通过子查询后，再关联表</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> b.id from bigtable b
<span class="token function">join</span> <span class="token punctuation">(</span>select <span class="token function">id</span> from bigtable where <span class="token function">id</span> <span class="token operator">&lt;=</span> 10 <span class="token punctuation">)</span> o on b.id <span class="token operator">=</span> o.id<span class="token punctuation">;</span>
Time taken: 30.058 seconds, Fetched: 100 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="4-7-分区分桶"><a href="#4-7-分区分桶" class="headerlink" title="4.7 分区分桶"></a>4.7 分区分桶</h2><p>在涉及存储结构时候，设置分区分桶，查找时候效率就会更高。</p><h1 id="5-合理设置Map及Reduce数"><a href="#5-合理设置Map及Reduce数" class="headerlink" title="5 合理设置Map及Reduce数"></a>5 合理设置Map及Reduce数</h1><p>1）通常情况下，作业会通过input的目录产生一个或者多个map任务。</p><p>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</p><p>2）是不是map数越多越好？</p><p>答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</p><p>3）是不是保证每个map处理接近128m的文件块，就高枕无忧了？</p><p>答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</p><p>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；</p><h2 id="5-1-复杂文件增加Map数"><a href="#5-1-复杂文件增加Map数" class="headerlink" title="5.1 复杂文件增加Map数"></a>5.1 复杂文件增加Map数</h2><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p><p>增加map的方法为：根据</p><p>computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</p><p>案例实操：</p><p>1）执行查询</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> count<span class="token punctuation">(</span>*<span class="token punctuation">)</span> from emp<span class="token punctuation">;</span>
Hadoop job information <span class="token keyword">for</span> Stage-1: number of mappers: 1<span class="token punctuation">;</span> number of reducers: 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）设置最大切片值为100个字节</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce.input.fileinputformat.split.maxsize<span class="token operator">=</span>100<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> count<span class="token punctuation">(</span>*<span class="token punctuation">)</span> from emp<span class="token punctuation">;</span>
Hadoop job information <span class="token keyword">for</span> Stage-1: number of mappers: 6<span class="token punctuation">;</span> number of reducers: 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="5-2-小文件进行合并"><a href="#5-2-小文件进行合并" class="headerlink" title="5.2 小文件进行合并"></a>5.2 小文件进行合并</h2><p>1）在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。</p><p>set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</p><p>2）在Map-Reduce的任务结束时合并小文件的设置：</p><p>在map-only任务结束时合并小文件，默认true</p><p>SET hive.merge.mapfiles = true;</p><p>在map-reduce任务结束时合并小文件，默认false</p><p>SET hive.merge.mapredfiles = true;</p><p>合并文件的大小，默认256M</p><p>SET hive.merge.size.per.task = 268435456;</p><p>当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge</p><p>SET hive.merge.smallfiles.avgsize = 16777216;</p><h2 id="5-3-合理设置Reduce数"><a href="#5-3-合理设置Reduce数" class="headerlink" title="5.3 合理设置Reduce数"></a>5.3 合理设置Reduce数</h2><p>1）调整reduce个数方法一</p><p>（1）每个Reduce处理的数据量默认是256MB</p><p>hive.exec.reducers.bytes.per.reducer=256000000</p><p>（2）每个任务最大的reduce数，默认为1009</p><p>hive.exec.reducers.max=1009</p><p>（3）计算reducer数的公式</p><p>N=min(参数2，总输入数据量/参数1)</p><p>2）调整reduce个数方法二</p><p>在hadoop的mapred-default.xml文件中修改</p><p>设置每个job的Reduce个数</p><p>set mapreduce.job.reduces = 15;</p><p>3）reduce个数并不是越多越好</p><p>（1）过多的启动和初始化reduce也会消耗时间和资源；</p><p>（2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</p><p>在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；</p><h1 id="6-并行执行"><a href="#6-并行执行" class="headerlink" title="6 并行执行"></a>6 并行执行</h1><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p><p>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</p><p>set hive.exec.parallel=true; //打开任务并行执行</p><p>set hive.exec.parallel.thread.number=16; //同一个sql允许最大并行度，默认为8。</p><p>当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。</p><h1 id="7-严格模式"><a href="#7-严格模式" class="headerlink" title="7 严格模式"></a>7 严格模式</h1><p>Hive可以通过设置防止一些危险操作：</p><p>1）分区表不使用分区过滤</p><p>将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。<br>2）使用order by没有limit过滤</p><p>将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</p><p>3）笛卡尔积</p><p>将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在 执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p><h1 id="8-压缩"><a href="#8-压缩" class="headerlink" title="8 压缩"></a>8 压缩</h1><p>在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。</p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li></ul></div><span class="post-count">总字数4.9k</span> <span class="post-count">预计阅读21分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-kafka1-setup" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/15/bigdata-kafka1-setup/" class="article-date"><time class="published" datetime="2020-11-14T22:46:51.000Z" itemprop="datePublished">2020-11-15 发布</time> <time class="updated" datetime="2021-11-19T06:15:42.011Z" itemprop="dateUpdated">2021-11-19 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/15/bigdata-kafka1-setup/">kafka学习笔记（一） kafka搭建</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="1-Kafka概述"><a href="#1-Kafka概述" class="headerlink" title="1 Kafka概述"></a>1 Kafka概述</h1><h2 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h2><p>Kafka是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。</p><h2 id="1-2-消息队列"><a href="#1-2-消息队列" class="headerlink" title="1.2 消息队列"></a>1.2 消息队列</h2><h3 id="1-2-1-传统消息队列的应用场景"><a href="#1-2-1-传统消息队列的应用场景" class="headerlink" title="1.2.1 传统消息队列的应用场景"></a>1.2.1 传统消息队列的应用场景</h3><p><img src="/2020/11/15/bigdata-kafka1-setup/1637153923628.png" alt="1637153923628"></p><p>使用消息队列的好处</p><p>1）解耦</p><p>允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p><p>2）可恢复性</p><p>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</p><p>3）缓冲</p><p>有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</p><p>4）灵活性 &amp; 峰值处理能力</p><p>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</p><p>5）异步通信</p><p>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</p><h3 id="1-2-2-消息队列的两种模式"><a href="#1-2-2-消息队列的两种模式" class="headerlink" title="1.2.2 消息队列的两种模式"></a>1.2.2 消息队列的两种模式</h3><p>（1）点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）</p><p>消息生产者生产消息发送到Queue中，然后消息消费者从Queue中<strong>取出</strong>并且消费消息。（<strong>这里注意是消费者主动拉取的</strong>）</p><p>消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。</p><p><img src="/2020/11/15/bigdata-kafka1-setup/1637153951339.png" alt="1637153951339"></p><p>（2）发布/订阅模式（一对多，消费者消费数据之后不会清除消息）</p><p>消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）<strong>消费</strong>该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。（<strong>这里注意数据也是消费者拉取的，因为消费者会一直轮询topic是否有消息</strong>）</p><p><img src="/2020/11/15/bigdata-kafka1-setup/1637153963262.png" alt="1637153963262"></p><h2 id="1-3-Kafka基础架构"><a href="#1-3-Kafka基础架构" class="headerlink" title="1.3 Kafka基础架构"></a>1.3 Kafka基础架构</h2><p><img src="/2020/11/15/bigdata-kafka1-setup/1637154054233.png" alt="1637154054233"></p><p>1）Producer ：消息生产者，就是向kafka broker发消息的客户端；</p><p>2）Consumer ：消息消费者，向kafka broker取消息的客户端；</p><p>3）Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</p><p>4）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</p><p>5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；</p><p>6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；</p><p>7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。</p><p>8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。</p><p>9）follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。</p><p>10）kafka集群依赖于zookeeper管理。</p><h1 id="2-Kafka安装部署"><a href="#2-Kafka安装部署" class="headerlink" title="2 Kafka安装部署"></a>2 Kafka安装部署</h1><h2 id="2-1-集群规划"><a href="#2-1-集群规划" class="headerlink" title="2.1 集群规划"></a>2.1 集群规划</h2><table><thead><tr><th>hadoop102</th><th>hadoop103</th><th>hadoop104</th></tr></thead><tbody><tr><td>zk</td><td>zk</td><td>zk</td></tr><tr><td>kafka</td><td>kafka</td><td>kafka</td></tr></tbody></table><h2 id="2-2-Kafka-下载"><a href="#2-2-Kafka-下载" class="headerlink" title="2.2 Kafka 下载"></a>2.2 Kafka 下载</h2><p><a target="_blank" rel="noopener" href="http://kafka.apache.org/downloads.html">http://kafka.apache.org/downloads.html</a></p><h2 id="2-3-集群部署"><a href="#2-3-集群部署" class="headerlink" title="2.3 集群部署"></a>2.3 集群部署</h2><p>1）解压安装包</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）修改解压后的文件名称</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ <span class="token function">mv</span> kafka_2.11-2.4.1.tgz kafka<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）在/opt/module/kafka目录下创建logs文件夹</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> logs<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）修改配置文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ <span class="token function">cd</span> config/
<span class="token punctuation">[</span>molly@hadoop102 config<span class="token punctuation">]</span>$ <span class="token function">vi</span> server.properties<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>输入以下内容：</p><pre class="line-numbers language-sh"><code class="language-sh">#broker的全局唯一编号，不能重复
broker.id=0
#删除topic功能使能,当前版本此配置默认为true，已从配置文件移除
delete.topic.enable=true
#处理网络请求的线程数量
num.network.threads=3
#用来处理磁盘IO的线程数量
num.io.threads=8
#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400
#接收套接字的缓冲区大小
socket.receive.buffer.bytes=102400
#请求套接字的缓冲区大小
socket.request.max.bytes=104857600
#kafka运行日志存放的路径
log.dirs=/opt/module/kafka/logs
#topic在当前broker上的分区个数
num.partitions=1
#用来恢复和清理data下数据的线程数量
num.recovery.threads.per.data.dir=1
#segment文件保留的最长时间，超时将被删除
log.retention.hours=168
#配置连接Zookeeper集群地址
zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5）配置环境变量</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ <span class="token function">sudo</span> vim /etc/profile.d/my_env.sh
<span class="token comment" spellcheck="true">#KAFKA_HOME</span>
<span class="token function">export</span> KAFKA_HOME<span class="token operator">=</span>/opt/module/kafka
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$KAFKA_HOME</span>/bin
<span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ <span class="token function">source</span> /etc/profile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>6）分发安装包</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ xsync kafka/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​ 注意：分发之后记得配置其他机器的环境变量</p><p>7）分别在hadoop103和hadoop104上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2</p><p>​ 注：broker.id不得重复</p><p>8）启动集群</p><p>​ 先启动Zookeeper集群，然后启动kafaka</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102  kafka<span class="token punctuation">]</span>$ zk.sh start <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>依次在hadoop102、hadoop103、hadoop104节点上启动kafka</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-server-start.sh -daemon config/server.properties

<span class="token punctuation">[</span>molly@hadoop103 kafka<span class="token punctuation">]</span>$ bin/kafka-server-start.sh -daemon  config/server.properties

<span class="token punctuation">[</span>molly@hadoop104 kafka<span class="token punctuation">]</span>$ bin/kafka-server-start.sh -daemon  config/server.properties<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>9）关闭集群</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-server-stop.sh stop
<span class="token punctuation">[</span>molly@hadoop103 kafka<span class="token punctuation">]</span>$ bin/kafka-server-stop.sh stop
<span class="token punctuation">[</span>molly@hadoop104 kafka<span class="token punctuation">]</span>$ bin/kafka-server-stop.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>10）kafka群起脚本</p><pre class="line-numbers language-sh"><code class="language-sh">#!/bin/bash
if [ $# -lt 1 ]
then 
  echo "Input Args Error....."
  exit
fi
for i in hadoop102 hadoop103 hadoop104
do

case $1 in
start)
  echo "==================START $i KAFKA==================="
  ssh $i /opt/module/kafka_2.11-2.4.1/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.11-2.4.1/config/server.properties
;;
stop)
  echo "==================STOP $i KAFKA==================="
  ssh $i /opt/module/kafka_2.11-2.4.1/bin/kafka-server-stop.sh stop
;;

*)
 echo "Input Args Error....."
 exit
;;  
esac
done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-Kafka命令行操作"><a href="#3-Kafka命令行操作" class="headerlink" title="3 Kafka命令行操作"></a>3 Kafka命令行操作</h1><p>kafka提供了测试脚本kafka-topics.sh用来测试。</p><p>1）查看当前服务器中的所有topic</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --list<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）创建topic</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>选项说明：</p><p>–topic 定义topic名<br>–replication-factor 定义副本数<br>–partitions 定义分区数</p><p>3）删除topic</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）发送消息：生产消息– <strong>9092是kafka默认端口</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first
<span class="token operator">></span>hello world
<span class="token operator">></span>molly molly<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>5）消费消息</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-console-consumer.sh \
--bootstrap-server hadoop102:9092 --topic first

<span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-console-consumer.sh \
--bootstrap-server hadoop102:9092 --from-beginning --topic first<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>–from-beginning：会把主题中现有的所有的数据都读取出来</strong>。</p><p>6）查看某个Topic的详情</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe –-topic first<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>7）修改分区数 alter只能修改</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --alter –-topic first --partitions 6<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="4-Kafka监控"><a href="#4-Kafka监控" class="headerlink" title="4 Kafka监控"></a>4 Kafka监控</h1><p>我们知道一个叫kafka manager的kafka管理工具，这个工具管理kafka确实很强大，但是没有安全认证，随便都可以创建，删除，修改topic，而且告警系统，流量波动做的不好。所以，在这里浪尖，再给大家推荐一款kafka 的告警监控管理工具，kafka-eagle。Kafka Eagle是一款开源的Kafka集群监控系统。能够实现broker级常见的JMX监控；能对consumer消费进度进行监控；能在页面上直接对多个集群进行管理；安装方式简单，二进制包解压即用；可以配置告警（钉钉、微信、email均可）。</p><p>kafka-eagle主要是有几个我们关注 但kafkamanager不存在的点，值得一提：</p><ul><li>流量，最长可以查看最近七天的流量波动图</li><li>lag size邮件告警</li><li>可以用kafkasql分析</li></ul><p>相关官方地址：</p><ul><li>源码： <a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://github.com/smartloli/kafka-eagle/">https://github.com/smartloli/kafka-eagle/</a></li><li>官网：<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://www.kafka-eagle.org/">https://www.kafka-eagle.org/</a></li><li>下载： <a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=http://download.kafka-eagle.org/">http://download.kafka-eagle.org/</a></li><li>安装文档： <a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://docs.kafka-eagle.org/2.env-and-install">https://docs.kafka-eagle.org/2.env-and-install</a></li></ul><p><strong>1）修改kafka启动命令</strong></p><p>修改kafka-server-start.sh命令中</p><pre class="line-numbers language-sh"><code class="language-sh">if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
    export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"
fi<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>为</p><pre class="line-numbers language-sh"><code class="language-sh">if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
    export KAFKA_HEAP_OPTS="-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70"
    export JMX_PORT="9999"
    #export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"
fi<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意：修改之后在启动Kafka之前要分发之其他节点</p><p><strong>2）上传压缩包kafka-eagle-bin-1.4.5.tar.gz到集群/opt/software目录</strong></p><p><strong>3）解压到本地</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf kafka-eagle-bin-1.4.5.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>4）进入刚才解压的目录</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka-eagle-bin-1.4.5<span class="token punctuation">]</span>$ ll
总用量 82932
-rw-rw-r--. 1 molly molly 84920710 8月 13 23:00 kafka-eagle-web-1.4.5-bin.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>5）将kafka-eagle-web-1.3.7-bin.tar.gz解压至/opt/module</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka-eagle-bin-1.4.5<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf kafka-eagle-web-1.4.5-bin.tar.gz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>6）修改名称</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ <span class="token function">mv</span> kafka-eagle-web-1.4.5/ eagle<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>7）给启动文件执行权限</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 eagle<span class="token punctuation">]</span>$ <span class="token function">cd</span> bin/
<span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ ll

总用量 12

-rw-r--r--. 1 molly molly 1848 8月 22 2017 ke.bat

-rw-r--r--. 1 molly molly 7190 7月 30 20:12 ke.sh

<span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> 777 ke.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>8）修改配置文件 conf/system-config.properties</strong></p><pre class="line-numbers language-sh"><code class="language-sh">######################################
# multi zookeeper&kafka cluster list
######################################
kafka.eagle.zk.cluster.alias=cluster1
cluster1.zk.list=hadoop102:2181,hadoop103:2181,hadoop104:2181

######################################
# kafka offset storage
######################################
cluster1.kafka.eagle.offset.storage=kafka

######################################
# enable kafka metrics
######################################
kafka.eagle.metrics.charts=true
kafka.eagle.sql.fix.error=false

######################################
# kafka jdbc driver address
######################################
kafka.eagle.driver=com.mysql.jdbc.Driver
kafka.eagle.url=jdbc:mysql://hadoop102:3306/ke?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull
kafka.eagle.username=root
kafka.eagle.password=123456<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>9）添加环境变量</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">export</span> KE_HOME<span class="token operator">=</span>/opt/module/eagle
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$KE_HOME</span>/bin
<span class="token comment" spellcheck="true">#注意：source /etc/profile</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>10）启动</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>atguigu@hadoop102 eagle<span class="token punctuation">]</span>$ bin/ke.sh start
<span class="token punctuation">..</span>. <span class="token punctuation">..</span>.
<span class="token punctuation">..</span>. <span class="token punctuation">..</span>.
*******************************************************************
* Kafka Eagle Service has started success.
* Welcome, Now you can visit <span class="token string">'http://192.168.202.102:8048/ke'</span>
* Account:admin ,Password:123456
*******************************************************************
* <span class="token operator">&lt;</span>Usage<span class="token operator">></span> ke.sh <span class="token punctuation">[</span>start<span class="token operator">|</span>status<span class="token operator">|</span>stop<span class="token operator">|</span>restart<span class="token operator">|</span>stats<span class="token punctuation">]</span> <span class="token operator">&lt;</span>/Usage<span class="token operator">></span>
* <span class="token operator">&lt;</span>Usage<span class="token operator">></span> https://www.kafka-eagle.org/ <span class="token operator">&lt;</span>/Usage<span class="token operator">></span>
*******************************************************************<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>注意：启动之前需要先启动ZK以及KAFKA</strong></p><p><strong>11）登录页面查看监控数据</strong></p><p><a target="_blank" rel="noopener" href="http://192.168.202.102:8048/ke">http://192.168.202.102:8048/ke</a></p><p><img src="/2020/11/15/bigdata-kafka1-setup/1637153214475.png" alt="1637153214475"></p><p>​</p><h1 id="5-Flume对接Kafka"><a href="#5-Flume对接Kafka" class="headerlink" title="5 Flume对接Kafka"></a>5 Flume对接Kafka</h1><h2 id="5-1-简单实现"><a href="#5-1-简单实现" class="headerlink" title="5.1 简单实现"></a>5.1 简单实现</h2><p><strong>1）配置flume</strong></p><pre class="line-numbers language-sh"><code class="language-sh"># define
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F  /opt/module/data/flume.log

# sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sinks.k1.kafka.topic = first
a1.sinks.k1.kafka.flumeBatchSize = 20
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1

# channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# bind
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2） 启动kafka消费者</p><p>3） 进入flume根目录下，启动flume</p><pre class="line-numbers language-bash"><code class="language-bash">$ bin/flume-ng agent -c conf/ -n a1 -f jobs/flume-kafka.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4） 向 /opt/module/data/flume.log里追加数据，查看kafka消费者消费情况</p><pre class="line-numbers language-bash"><code class="language-bash">$ <span class="token keyword">echo</span> hello <span class="token operator">>></span> /opt/module/data/flume.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="5-2-数据分离"><a href="#5-2-数据分离" class="headerlink" title="5.2 数据分离"></a>5.2 数据分离</h2><p>0)需求</p><p>将flume采集的数据按照不同的类型输入到不同的topic中</p><p>​ 将日志数据中带有molly的，输入到Kafka的first主题中，</p><p>​ 将日志数据中带有shangguigu的,输入到Kafka的second主题中，</p><p>​ 其他的数据输入到Kafka的third主题中</p><p>1） 编写Flume的Interceptor</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>atguigu<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>flumeInterceptor<span class="token punctuation">;</span>

<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>Context<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>Event<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>interceptor<span class="token punctuation">.</span>Interceptor<span class="token punctuation">;</span>

<span class="token keyword">import</span> javax<span class="token punctuation">.</span>swing<span class="token punctuation">.</span>text<span class="token punctuation">.</span>html<span class="token punctuation">.</span>HTMLEditorKit<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>List<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Map<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">FlumeKafkaInterceptor</span> <span class="token keyword">implements</span> <span class="token class-name">Interceptor</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">initialize</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">/**
     * 如果包含"atguigu"的数据，发送到first主题
     * 如果包含"sgg"的数据，发送到second主题
     * 其他的数据发送到third主题
     * @param event
     * @return
     */</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> Event <span class="token function">intercept</span><span class="token punctuation">(</span>Event event<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">//1.获取event的header</span>
        Map<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> String<span class="token operator">></span> headers <span class="token operator">=</span> event<span class="token punctuation">.</span><span class="token function">getHeaders</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">//2.获取event的body</span>
        String body <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">String</span><span class="token punctuation">(</span>event<span class="token punctuation">.</span><span class="token function">getBody</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">if</span><span class="token punctuation">(</span>body<span class="token punctuation">.</span><span class="token function">contains</span><span class="token punctuation">(</span><span class="token string">"atguigu"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            headers<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">"topic"</span><span class="token punctuation">,</span><span class="token string">"first"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token keyword">else</span> <span class="token keyword">if</span><span class="token punctuation">(</span>body<span class="token punctuation">.</span><span class="token function">contains</span><span class="token punctuation">(</span><span class="token string">"sgg"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            headers<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">"topic"</span><span class="token punctuation">,</span><span class="token string">"second"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> event<span class="token punctuation">;</span>

    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> List<span class="token operator">&lt;</span>Event<span class="token operator">></span> <span class="token function">intercept</span><span class="token punctuation">(</span>List<span class="token operator">&lt;</span>Event<span class="token operator">></span> events<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>Event event <span class="token operator">:</span> events<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
          <span class="token function">intercept</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> events<span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">MyBuilder</span> <span class="token keyword">implements</span>  <span class="token class-name">Builder</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> Interceptor <span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span>  <span class="token keyword">new</span> <span class="token class-name">FlumeKafkaInterceptor</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">configure</span><span class="token punctuation">(</span>Context context<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）将写好的interceptor打包上传到Flume安装目录的lib目录下</p><p>3）配置flume</p><pre class="line-numbers language-sh"><code class="language-sh"># Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 6666


# Describe the sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.topic = third
a1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sinks.k1.kafka.flumeBatchSize = 20
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1

#Interceptor
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.atguigu.kafka.flumeInterceptor.FlumeKafkaInterceptor$MyBuilder

# # Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4） 启动kafka消费者</p><p>5） 进入flume根目录下，启动flume</p><pre class="line-numbers language-bash"><code class="language-bash">$ bin/flume-ng agent -c conf/ -n a1 -f jobs/flume-kafka.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>6） 向6666端口写数据，查看kafka消费者消费情况</p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li></ul></div><span class="post-count">总字数3.4k</span> <span class="post-count">预计阅读15分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-hive1" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/14/bigdata-hive1/" class="article-date"><time class="published" datetime="2020-11-14T07:08:50.000Z" itemprop="datePublished">2020-11-14 发布</time> <time class="updated" datetime="2021-11-15T07:46:28.407Z" itemprop="dateUpdated">2021-11-15 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/14/bigdata-hive1/">Hive学习笔记（一） Hive安装</a></h1></header><div class="article-entry" itemprop="articleBody"><p>前面学习到利用mapreduce去计算，但是mapreduce写起来麻烦，并且代码重复度高，可以进行封装，所以就出来了Hive，hive工具通过执行类SQL来启动写好的mapreduce,进一步执行hdfs中的资源。</p><h1 id="1-Hive是什么"><a href="#1-Hive是什么" class="headerlink" title="1 Hive是什么"></a>1 Hive是什么</h1><h2 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h2><p>1） hive简介</p><p>Hive：由Facebook开源用于解决海量结构化日志的数据统计工具。</p><p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。</p><p>2） Hive本质：将HQL转化成MapReduce程序如下图所示。需要注意的是以下三点：</p><p>（1）Hive处理的数据存储在HDFS</p><p>（2）Hive分析数据底层的实现是MapReduce</p><p>（3）执行程序运行在Yarn上</p><p><img src="/2020/11/14/bigdata-hive1/1636945549117.png" alt="1636945549117"></p><h2 id="1-2-Hive架构"><a href="#1-2-Hive架构" class="headerlink" title="1.2 Hive架构"></a>1.2 Hive架构</h2><p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。具体架构图如下所示。</p><p><img src="/2020/11/14/bigdata-hive1/1636945658710.png" alt="1636945658710"></p><p>1）用户接口：Client</p><p>CLI（command-line interface）、JDBC/ODBC(jdbc访问hive)、WEBUI（浏览器访问hive）</p><p>2）元数据：Metastore</p><p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</p><p><strong>默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore</strong></p><p>3）Hadoop</p><p><strong>使用HDFS进行存储，使用MapReduce进行计算。</strong></p><p>4）驱动器：Driver</p><p>（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p><p>（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。</p><p>（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。</p><p>（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</p><h1 id="2-Hive-安装"><a href="#2-Hive-安装" class="headerlink" title="2 Hive 安装"></a>2 Hive 安装</h1><h2 id="2-1-Hive安装地址"><a href="#2-1-Hive安装地址" class="headerlink" title="2.1 Hive安装地址"></a>2.1 Hive安装地址</h2><p>1）Hive官网地址</p><p><a target="_blank" rel="noopener" href="http://hive.apache.org/">http://hive.apache.org/</a></p><p>2）文档查看地址</p><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p><p>3）下载地址</p><p><a target="_blank" rel="noopener" href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></p><p>4）github地址</p><p><a target="_blank" rel="noopener" href="https://github.com/apache/hive">https://github.com/apache/hive</a></p><h2 id="2-2-MySql安装"><a href="#2-2-MySql安装" class="headerlink" title="2.2 MySql安装"></a>2.2 MySql安装</h2><p>0）为什么需要Mysql</p><p>原因在于Hive默认使用的元数据库为derby，开启Hive之后就会占用元数据库，且不与其他客户端共享数据，如果想多窗口操作就会报错，操作比较局限。以我们需要将Hive的元数据地址改为MySQL，可支持多窗口操作。</p><p>1）检查当前系统是否安装过Mysql</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ rpm -qa<span class="token operator">|</span><span class="token function">grep</span> -I -E mysql\<span class="token operator">|</span>mariadb
mariadb-libs-5.5.56-2.el7.x86_64 //如果存在通过如下命令卸载
<span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> rpm -e --nodeps mariadb-libs  //用此命令卸载mariadb
<span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ rpm -qa<span class="token operator">|</span><span class="token function">grep</span> -I -E mysql\<span class="token operator">|</span>mariadb <span class="token operator">|</span> <span class="token function">xargs</span> -n1 <span class="token function">sudo</span> rpm -e --nodeps<span class="token comment" spellcheck="true">#卸载所有</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>2）将MySQL安装包拷贝到/opt/software目录下</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># ll</span>
总用量 528384
-rw-r--r--. 1 root root 609556480 3月 21 15:41 mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>3）解压MySQL安装包</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># tar -xf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）在安装目录下执行rpm安装</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">sudo</span> rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm
<span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">sudo</span> rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm
<span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">sudo</span> rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm
<span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">sudo</span> rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm
<span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">sudo</span> rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意:按照顺序依次执行</p><p>如果Linux是最小化安装的，在安装mysql-community-server-5.7.28-1.el7.x86_64.rpm时可能会出 现如下错误</p><p>[molly@hadoop102 software]$ sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm</p><p>警告：mysql-community-server-5.7.28-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY</p><p>错误：依赖检测失败：</p><p>​ libaio.so.1()(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要</p><p>​ libaio.so.1(LIBAIO_0.1)(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要</p><p>​ libaio.so.1(LIBAIO_0.4)(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要</p><p>通过yum安装缺少的依赖,然后重新安装mysql-community-server-5.7.28-1.el7.x86_64 即可</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span> yum <span class="token function">install</span> -y libaio<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）删除/etc/my.cnf文件中datadir指向的目录下的所有内容,如果有内容的情况下:</p><p>查看datadir的值：</p><p>[mysqld]</p><p>datadir=/var/lib/mysql</p><p>删除/var/lib/mysql目录下的所有内容:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mysql<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># cd /var/lib/mysql</span>
<span class="token punctuation">[</span>molly@hadoop102 mysql<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># sudo rm -rf ./*  //注意执行命令的位置</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>6）初始化数据库</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 opt<span class="token punctuation">]</span>$ <span class="token function">sudo</span> mysqld --initialize --user<span class="token operator">=</span>mysql<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>7）查看临时生成的root用户的密码</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 opt<span class="token punctuation">]</span>$ <span class="token function">cat</span> /var/log/mysqld.log <span class="token operator">|</span><span class="token function">grep</span> password<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>8）启动MySQL服务</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 opt<span class="token punctuation">]</span>$ <span class="token function">sudo</span> systemctl start mysqld<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>9）登录MySQL数据库</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 opt<span class="token punctuation">]</span>$ mysql -uroot -p
Enter password:  输入临时生成的密码<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>登录成功.</p><p>10）必须先修改root用户的密码,否则执行其他的操作会报错</p><pre class="line-numbers language-bash"><code class="language-bash">mysql<span class="token operator">></span> <span class="token keyword">set</span> password <span class="token operator">=</span> password<span class="token punctuation">(</span><span class="token string">"000000"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>11）修改mysql库下的user表中的root用户允许任意ip连接</p><pre class="line-numbers language-bash"><code class="language-bash">mysql<span class="token operator">></span> update mysql.user <span class="token keyword">set</span> host<span class="token operator">=</span><span class="token string">'%'</span> where user<span class="token operator">=</span><span class="token string">'root'</span><span class="token punctuation">;</span>
mysql<span class="token operator">></span> flush privileges<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="2-3-Hive安装部署"><a href="#2-3-Hive安装部署" class="headerlink" title="2.3 Hive安装部署"></a>2.3 Hive安装部署</h2><p>1）把apache-hive-3.1.2-bin.tar.gz上传到linux的/opt/software目录下</p><p>2）解压apache-hive-3.1.2-bin.tar.gz到/opt/module/目录下面</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）修改apache-hive-3.1.2-bin.tar.gz的名称为hive</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">mv</span> /opt/module/apache-hive-3.1.2-bin/ /opt/module/hive-3.1.2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）修改/etc/profile.d/my_env.sh，添加环境变量</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">sudo</span> vim /etc/profile.d/my_env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）添加内容</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#HIVE_HOME</span>
HIVE_HOME<span class="token operator">=</span>/opt/module/hive-3.1.2
PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$JAVA_HOME</span>/bin:<span class="token variable">$HADOOP_HOME</span>/bin:<span class="token variable">$HADOOP_HOME</span>/sbin:<span class="token variable">$HIVE_HOME</span>/bin
<span class="token function">export</span> PATH JAVA_HOME HADOOP_HOME HIVE_HOME<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>6）解决日志Jar包冲突:Hive日志与Hadoop默认日志冲突，可以直接删除hive日志JAR</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">rm</span> -rf /opt/module/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-4-Hive元数据配置到MySql"><a href="#2-4-Hive元数据配置到MySql" class="headerlink" title="2.4 Hive元数据配置到MySql"></a>2.4 Hive元数据配置到MySql</h2><p>因为Hive默认使用的元数据库为derby，为了想多窗口操作，我们需要将Hive的元数据地址改为MySQL。下面安装好mysql后，进行将Hive元数据配置到mysql上。其中HIVE_HOME=/opt/module/hive-3.1.2</p><h3 id="2-4-1-拷贝驱动"><a href="#2-4-1-拷贝驱动" class="headerlink" title="2.4.1 拷贝驱动"></a>2.4.1 拷贝驱动</h3><p>将MySQL的JDBC驱动拷贝到Hive的lib目录下</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">cp</span> /opt/software/mysql-connector-java-5.1.48.jar <span class="token variable">$HIVE_HOME</span>/lib<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-4-2-配置Metastore到MySql"><a href="#2-4-2-配置Metastore到MySql" class="headerlink" title="2.4.2 配置Metastore到MySql"></a>2.4.2 配置Metastore到MySql</h3><p>在$HIVE_HOME/conf目录下新建hive-site.xml文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ vim <span class="token variable">$HIVE_HOME</span>/conf/hive-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加如下内容</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- jdbc连接的URL --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionURL<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>jdbc:mysql://hadoop102:3306/metastore?useSSL=false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- jdbc连接的Driver--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionDriverName<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.mysql.jdbc.Driver<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- jdbc连接的username--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionUserName<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>root<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- jdbc连接的password --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionPassword<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>123456<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- Hive默认在HDFS的工作目录 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.warehouse.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/user/hive/warehouse<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- Hive元数据存储的验证 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.schema.verification<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- 元数据存储授权 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.event.db.notification.api.auth<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-5-启动Hive"><a href="#2-5-启动Hive" class="headerlink" title="2.5 启动Hive"></a>2.5 启动Hive</h2><h3 id="2-5-1-初始化元数据库"><a href="#2-5-1-初始化元数据库" class="headerlink" title="2.5.1 初始化元数据库"></a>2.5.1 初始化元数据库</h3><p>1）登陆MySQL</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ mysql -uroot -p000000<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）新建Hive元数据库</p><pre class="line-numbers language-bash"><code class="language-bash">mysql<span class="token operator">></span> create database metastore<span class="token punctuation">;</span>
mysql<span class="token operator">></span> quit<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>3）初始化Hive元数据库</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ schematool -initSchema -dbType mysql -verbose<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-5-2-启动Hive"><a href="#2-5-2-启动Hive" class="headerlink" title="2.5.2 启动Hive"></a>2.5.2 启动Hive</h3><p>0）先启动hadoop集群</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ start-dfs.sh
<span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ start-yarn.sh
<span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ myjps.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>浏览器中查看hdfs：<a target="_blank" rel="noopener" href="http://hadoop102:9870/">http://Hadoop102:9870</a></p><p>浏览器中查看yarn ：<a target="_blank" rel="noopener" href="http://hadoop103:8088/">http://Hadoop103:8088</a></p><p>1）启动Hive</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/hive<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）使用Hive</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> show databases<span class="token punctuation">;</span>
hive<span class="token operator">></span> show tables<span class="token punctuation">;</span>
hive<span class="token operator">></span> create table <span class="token function">test</span> <span class="token punctuation">(</span>id int<span class="token punctuation">)</span><span class="token punctuation">;</span>
hive<span class="token operator">></span> insert into <span class="token function">test</span> values<span class="token punctuation">(</span>1<span class="token punctuation">)</span><span class="token punctuation">;</span>
hive<span class="token operator">></span> <span class="token keyword">select</span> * from <span class="token function">test</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-5-3-使用元数据服务的方式访问Hive"><a href="#2-5-3-使用元数据服务的方式访问Hive" class="headerlink" title="2.5.3 使用元数据服务的方式访问Hive"></a>2.5.3 使用元数据服务的方式访问Hive</h3><p>原始方法：Hive直接访问mysql</p><p><strong>使用元数据服务方式：Hive—》元数据服务，元数据服务—》访问mysql。</strong></p><p>1）在hive-site.xml文件中添加如下配置信息</p><pre class="line-numbers language-xml"><code class="language-xml">  <span class="token comment" spellcheck="true">&lt;!-- 指定存储元数据要连接的地址 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.uris<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>thrift://hadoop102:9083<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）启动metastore</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop202 hive<span class="token punctuation">]</span>$ hive --service metastore
2020-04-24 16:58:08: Starting Hive Metastore Server<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意: 启动后窗口不能再操作，需打开一个新的shell窗口做别的操作</p><p>3）启动 hive</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop202 hive<span class="token punctuation">]</span>$ bin/hive<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-5-4-使用JDBC方式访问Hive"><a href="#2-5-4-使用JDBC方式访问Hive" class="headerlink" title="2.5.4 使用JDBC方式访问Hive"></a>2.5.4 使用JDBC方式访问Hive</h3><p><strong>客户端是beeline（JDBC协议去访问）;</strong></p><p><strong>服务端：Hive使用hiveserver2提供JDBC协议：</strong></p><p><strong>所以访问数据流是：blleline通过hiveserver2去访问Hive。</strong></p><p>1）在hive-site.xml文件中添加如下配置信息</p><p>2）启动hiveserver2</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/hive --service hiveserver2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）启动beeline客户端（需要多等待一会）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/beeline -u jdbc:hive2://hadoop102:10000 -n molly<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）看到如下界面</p><pre class="line-numbers language-bash"><code class="language-bash">Connecting to jdbc:hive2://hadoop102:10000
Connected to: Apache Hive <span class="token punctuation">(</span>version 3.1.2<span class="token punctuation">)</span>
Driver: Hive JDBC <span class="token punctuation">(</span>version 3.1.2<span class="token punctuation">)</span>
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.2 by Apache Hive
0: jdbc:hive2://hadoop102:10000<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-常用命令"><a href="#3-常用命令" class="headerlink" title="3 常用命令"></a>3 常用命令</h1><h2 id="3-1-Hive常用交互命令"><a href="#3-1-Hive常用交互命令" class="headerlink" title="3.1 Hive常用交互命令"></a>3.1 Hive常用交互命令</h2><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/hive -help<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>1）“-e”不进入hive的交互窗口执行sql语句</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/hive -e <span class="token string">"select id from student;"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）“-f”执行脚本中sql语句</p><p>（1）在/opt/module/hive/下创建datas目录并在datas目录下创建hivef.sql文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 datas<span class="token punctuation">]</span>$ <span class="token function">touch</span> hivef.sql<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）文件中写入正确的sql语句</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token keyword">select</span> *from student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）执行文件中的sql语句</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/hive -f /opt/module/hive/datas/hivef.sql<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）执行文件中的sql语句并将结果写入文件中</p><pre><code>[molly@hadoop102 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt</code></pre><h2 id="3-2-Hive其他命令操作"><a href="#3-2-Hive其他命令操作" class="headerlink" title="3.2 Hive其他命令操作"></a>3.2 Hive其他命令操作</h2><p>1）退出hive窗口：</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>exit<span class="token punctuation">;</span>
hive<span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>quit<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在新版的hive中没区别了，在以前的版本是有的：</p><p>exit:先隐性提交数据，再退出；</p><p>quit:不提交数据，退出；</p><p>2）在hive cli命令窗口中如何查看hdfs文件系统</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>dfs -ls /<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）查看在hive中输入的所有历史命令</p><p>（1）进入到当前用户的根目录/root或/home/molly</p><p>（2）查看. hivehistory文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>atguig2u@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">cat</span> .hivehistory<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="3-3-Hive常见属性配置"><a href="#3-3-Hive常见属性配置" class="headerlink" title="3.3 Hive常见属性配置"></a>3.3 Hive常见属性配置</h2><h3 id="3-3-1-hive窗口打印默认库和表头"><a href="#3-3-1-hive窗口打印默认库和表头" class="headerlink" title="3.3.1 hive窗口打印默认库和表头"></a>3.3.1 hive窗口打印默认库和表头</h3><p>1）打印 当前库 和 表头</p><p>在hive-site.xml中加入如下两个配置:</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.cli.print.header<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.cli.print.current.db<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-3-2-Hive运行日志信息配置"><a href="#3-3-2-Hive运行日志信息配置" class="headerlink" title="3.3.2 Hive运行日志信息配置"></a>3.3.2 Hive运行日志信息配置</h3><p>1）Hive的log默认存放在/tmp/molly/hive.log目录下（当前用户名下）</p><p>2）修改hive的log存放日志到/opt/module/hive/logs</p><p>（1）修改/opt/module/hive/conf/hive-log4j2.properties.template文件名称为</p><p>hive-log4j2.properties</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ <span class="token function">pwd</span>
/opt/module/hive/conf
<span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ <span class="token function">mv</span> hive-log4j2.properties.template hive-log4j2.properties<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（2）在hive-log4j.properties文件中修改log存放位置</p><p>property.hive.log.dir=/opt/module/hive/logs</p><h3 id="3-3-3-参数配置方式"><a href="#3-3-3-参数配置方式" class="headerlink" title="3.3.3 参数配置方式"></a>3.3.3 参数配置方式</h3><p>1）查看当前所有的配置信息</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span>set<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）参数的配置三种方式</p><p>（1）配置文件方式</p><p>默认配置文件：hive-default.xml</p><p>用户自定义配置文件：hive-site.xml</p><p>注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p><p>（2）命令行参数方式</p><p>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。</p><p>例如：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop103 hive<span class="token punctuation">]</span>$ bin/hive -hiveconf mapred.reduce.tasks<span class="token operator">=</span>10<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：仅对本次hive启动有效</p><p>查看参数设置：</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapred.reduce.tasks<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）参数声明方式</p><p>可以在HQL中使用SET关键字设定参数</p><p>例如：</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapred.reduce.tasks<span class="token operator">=</span>100<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：仅对本次hive启动有效。</p><p>查看参数设置</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapred.reduce.tasks<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li></ul></div><span class="post-count">总字数3k</span> <span class="post-count">预计阅读13分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-hdfs1" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/12/bigdata-hdfs1/" class="article-date"><time class="published" datetime="2020-11-12T08:50:28.000Z" itemprop="datePublished">2020-11-12 发布</time> <time class="updated" datetime="2021-11-15T08:21:25.856Z" itemprop="dateUpdated">2021-11-15 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/12/bigdata-hdfs1/">Hadoop 教程（二）安装hadoop集群-完全分布式部署</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h2><p>本文来搭建hadoop集群，准备三台服务器，分别为hadoop102,hadoop103,hadoop104.其中hadoop 采用3.1.3版本，jdk 采用1.8.0_212 。</p><h2 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2 准备工作"></a>2 准备工作</h2><h3 id="2-1-映射"><a href="#2-1-映射" class="headerlink" title="2.1 映射"></a>2.1 映射</h3><p>为了方便直接通过主机名去访问，下面进行映射</p><p>1）修改克隆机主机名，以下以hadoop102举例说明</p><p>（1）修改主机名称，：修改/etc/hostname文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop100 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># vim /etc/hostname</span>
hadoop102<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（2）配置linux克隆机主机名称映射hosts文件，打开/etc/hosts</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop100 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># vim /etc/hosts</span>
192.168.1.102 hadoop102
192.168.1.103 hadoop103
192.168.1.104 hadoop104<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>2）重启hadoop102</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop100 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># reboot</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）修改windows的主机映射文件（hosts文件）</p><p>操作系统是window10，先拷贝出来，修改保存以后，再覆盖即可</p><p>（a）进入C:\Windows\System32\drivers\etc路径</p><p>（b）拷贝hosts文件到桌面</p><p>（c）打开桌面hosts文件并添加如下内容</p><pre><code>192.168.1.102 hadoop102
192.168.1.103 hadoop103
192.168.1.104 hadoop104</code></pre><p>（d）将桌面hosts文件覆盖C:\Windows\System32\drivers\etc路径hosts文件</p><h3 id="2-2-安装JDK"><a href="#2-2-安装JDK" class="headerlink" title="2.2 安装JDK"></a>2.2 安装JDK</h3><p>1）在Linux系统下的opt目录中下载软件包</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">ls</span> /opt/software/
jdk-8u212-linux-x64.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）解压JDK到/opt/module目录下</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）配置JDK环境变量</p><p>​ （1）新建/etc/profile.d/my_env.sh文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> vim /etc/profile.d/my_env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加如下内容</p><p>#JAVA_HOME</p><p>export JAVA_HOME=/opt/module/jdk1.8.0_212</p><p>export PATH=$PATH:$JAVA_HOME/bin</p><p>​ （2）保存后退出:wq</p><p>​ （3）source一下/etc/profile文件，让新的环境变量PATH生效</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">source</span> /etc/profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）测试JDK是否安装成功</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ java -version<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如果能看到以下结果，则代表Java安装成功。</p><p>java version “1.8.0_212”</p><p>注意：重启（如果java -version可以用就不用重启）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> <span class="token function">reboot</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-3-SSH免密码登录"><a href="#2-3-SSH免密码登录" class="headerlink" title="2.3 SSH免密码登录"></a>2.3 SSH免密码登录</h3><p>免密登录原理如下图所示：</p><p><img src="/2020/11/12/bigdata-hdfs1/1636709256729.png" alt="1636709256729"></p><p>具体操作如下：</p><p>1）生成公钥和私钥：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 .ssh<span class="token punctuation">]</span>$ ssh-keygen -t rsa<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p><p>2）将公钥拷贝到要免密登录的目标机器上</p><p>[root@hadoop102 .ssh]$ ssh-copy-id hadoop102</p><p>[root@hadoop102 .ssh]$ ssh-copy-id hadoop103</p><p>[root@hadoop102 .ssh]$ ssh-copy-id hadoop104</p><p>这样hadoop102登录到hadoop103和hadoop104就不需要输入密码了。可以相互登录 还需要在hadoop103和hadoop104上做同样的操作。</p><h3 id="2-4-编写集群分发脚本"><a href="#2-4-编写集群分发脚本" class="headerlink" title="2.4 编写集群分发脚本"></a>2.4 编写集群分发脚本</h3><p>为了在集群中各个主机中文件拷贝方便，我们可以写个脚本用于三台主机中分发文件。</p><p>（1）需求：循环复制文件到所有节点的相同目录下</p><p>（2）需求分析：</p><p>（a）rsync命令原始拷贝：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">rsync</span> -av   /opt/module     root@hadoop103:/opt/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（b）期望脚本：</p><p>xsync要同步的文件名称</p><p>（c）说明：在/home/root/bin这个目录下存放的脚本，root用户可以在系统任何地方直接执行。</p><p>（3）脚本实现</p><p>（a）在/home/root/bin目录下创建xsync文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 opt<span class="token punctuation">]</span>$ <span class="token function">cd</span> /home/root
<span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> bin
<span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">cd</span> bin
<span class="token punctuation">[</span>root@hadoop102 bin<span class="token punctuation">]</span>$ vim xsync<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>在该文件中编写如下代码</p><pre class="line-numbers language-sh"><code class="language-sh">#!/bin/bash
\#1. 判断参数个数
if [ $# -lt 1 ]
then
 echo Not Enough Arguement!
 exit;
fi
\#2. 遍历集群所有机器
for host in hadoop102 hadoop103 hadoop104
do
 echo ==================== $host ====================
 \#3. 遍历所有目录，挨个发送
 for file in $@
 do
  \#4. 判断文件是否存在
  if [ -e $file ]
  then
   \#5. 获取父目录
   pdir=$(cd -P $(dirname $file); pwd)
   \#6. 获取当前文件的名称
   fname=$(basename $file)
   ssh $host "mkdir -p $pdir"
   rsync -av $pdir/$fname $host:$pdir
  else
   echo $file does not exists!
  fi
 done
done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（b）修改脚本 xsync 具有执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> +x xsync<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（c）将脚本复制到/bin中，以便全局调用</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">sudo</span> <span class="token function">cp</span> xsync /bin/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（d）测试脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ xsync /home/root/bin
<span class="token punctuation">[</span>root@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">sudo</span> xsync /bin/xsync<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="3-安装hadoop"><a href="#3-安装hadoop" class="headerlink" title="3 安装hadoop"></a>3 安装hadoop</h2><p>Hadoop下载地址：<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/">https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/</a></p><p>1）下载hadoop并进入到Hadoop安装包路径下</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">cd</span> /opt/software/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）解压安装文件到/opt/module下面</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf hadoop-3.1.3.tar.gz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）查看是否解压成功</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">ls</span> /opt/module/
hadoop-3.1.3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>4）将Hadoop添加到环境变量</p><p>​ （1）获取Hadoop安装路径</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">pwd</span>
/opt/module/hadoop-3.1.3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>​ （2）打开/etc/profile.d/my_env.sh文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> vim /etc/profile.d/my_env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在my_env.sh文件末尾添加如下内容：（shift+g）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#HADOOP_HOME</span>
<span class="token function">export</span> HADOOP_HOME<span class="token operator">=</span>/opt/module/hadoop-3.1.3
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$HADOOP_HOME</span>/bin
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$HADOOP_HOME</span>/sbin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>（3）保存后退出:wq</p><p>（4）让修改后的文件生效</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">source</span> /etc/profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）测试是否安装成功</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop version
Hadoop 3.1.3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>6）重启(如果Hadoop命令不能用再重启)</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">sync</span>
<span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">sudo</span> <span class="token function">reboot</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="4-Hadoop运行模式启动"><a href="#4-Hadoop运行模式启动" class="headerlink" title="4  Hadoop运行模式启动"></a>4 Hadoop运行模式启动</h2><p>Hadoop运行模式包括：本地模式、伪分布式模式以及完全分布式模式。本地允许模式很简单，公司用的大部分是完全分布式模式。</p><p>Hadoop官方网站：<a target="_blank" rel="noopener" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></p><h3 id="4-1-本地运行模式"><a href="#4-1-本地运行模式" class="headerlink" title="4.1 本地运行模式"></a>4.1 本地运行模式</h3><p>下面展示hadoop本地运行模式，并成功计算一个wordcout功能</p><p>1）创建在hadoop-3.1.3文件下面创建一个wcinput文件夹</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> wcinpu<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）在wcinput文件下创建一个word.txt文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">cd</span> wcinput<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）编辑word.txt文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 wcinput<span class="token punctuation">]</span>$ vim word.txt
hadoop yarn
hadoop mapreduce
root
root<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>保存退出：：wq</p><p>4）回到Hadoop目录/opt/module/hadoop-3.1.3</p><p>5）执行程序</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>6）查看结果</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">cat</span> wcoutput/part-r-00000
root 2
hadoop 2
mapreduce    1
yarn  1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-2-完全分布式模式"><a href="#4-2-完全分布式模式" class="headerlink" title="4.2 完全分布式模式"></a>4.2 完全分布式模式</h3><h4 id="4-2-1-集群规划"><a href="#4-2-1-集群规划" class="headerlink" title="4.2.1 集群规划"></a>4.2.1 集群规划</h4><p>准备三台机器，分别安装HDFS和yarn。</p><table><thead><tr><th></th><th>hadoop102</th><th>hadoop103</th><th>hadoop104</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode DataNode</td><td>DataNode</td><td>SecondaryNameNode DataNode</td></tr><tr><td>YARN</td><td>NodeManager</td><td>ResourceManager NodeManager</td><td>NodeManager</td></tr></tbody></table><p>注意：NameNode和SecondaryNameNode不要安装在同一台服务器</p><p>注意：ResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在同一台机器上。</p><h4 id="4-2-2-配置文件说明"><a href="#4-2-2-配置文件说明" class="headerlink" title="4.2.2 配置文件说明"></a>4.2.2 配置文件说明</h4><p>Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。</p><p>（1）默认配置文件：</p><table><thead><tr><th>要获取的默认文件</th><th>文件存放在Hadoop的jar包中的位置</th></tr></thead><tbody><tr><td>[core-default.xml]</td><td>hadoop-common-3.1.3.jar/ core-default.xml</td></tr><tr><td>[hdfs-default.xml]</td><td>hadoop-hdfs-3.1.3.jar/ hdfs-default.xml</td></tr><tr><td>[yarn-default.xml]</td><td>hadoop-yarn-common-3.1.3.jar/ yarn-default.xml</td></tr><tr><td>[mapred-default.xml]</td><td>hadoop-mapreduce-client-core-3.1.3.jar/ mapred-default.xml</td></tr></tbody></table><p>2）自定义配置文件：</p><p>core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置。</p><p>（3）常用端口号说明</p><table><thead><tr><th>Daemon</th><th>App</th><th>Hadoop2</th><th>Hadoop3</th></tr></thead><tbody><tr><td>NameNode Port</td><td>Hadoop HDFS NameNode</td><td>8020 / 9000</td><td>9820</td></tr><tr><td></td><td>Hadoop HDFS NameNode HTTP UI</td><td>50070</td><td>9870</td></tr><tr><td>Secondary NameNode Port</td><td>Secondary NameNode</td><td>50091</td><td>9869</td></tr><tr><td></td><td>Secondary NameNode HTTP UI</td><td>50090</td><td>9868</td></tr><tr><td>DataNode Port</td><td>Hadoop HDFS DataNode IPC</td><td>50020</td><td>9867</td></tr><tr><td></td><td>Hadoop HDFS DataNode</td><td>50010</td><td>9866</td></tr><tr><td></td><td>Hadoop HDFS DataNode HTTP UI</td><td>50075</td><td>9864</td></tr></tbody></table><h4 id="4-2-3-配置集群"><a href="#4-2-3-配置集群" class="headerlink" title="4.2.3 配置集群"></a>4.2.3 配置集群</h4><p>（1）核心配置文件：配置core-site.xml</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">cd</span> <span class="token variable">$HADOOP_HOME</span>/etc/hadoop
<span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ vim core-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>文件内容如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- 指定NameNode的地址 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://hadoop102:9820<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 指定hadoop数据的存储目录 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/opt/module/hadoop-3.1.3/data<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 配HDFS网页登录使用的静态用户为root --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.http.staticuser.user<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>root<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 配置该root(superUser)允许通过代理访问的主机节点 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.proxyuser.root.hosts<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 配置该root(superUser)允许通过代理用户所属组 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.proxyuser.root.groups<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 配置该root(superUser)允许通过代理的用户--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.proxyuser.root.groups<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）HDFS配置文件:配置hdfs-site.xml</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ vim hdfs-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>文件内容如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- nn web端访问地址--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop102:9870<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- 2nn web端访问地址--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop104:9868<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）YARN配置文件:配置yarn-site.xml</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ vim yarn-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>文件内容如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- 指定MR走shuffle --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.aux-services<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mapreduce_shuffle<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 指定ResourceManager的地址--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.hostname<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop103<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 环境变量的继承 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.env-whitelist<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- yarn容器允许分配的最大最小内存 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.minimum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>512<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.maximum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- yarn容器允许管理的物理内存大小 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.resource.memory-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 关闭yarn对物理内存和虚拟内存的限制检查 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.pmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.vmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（4）MapReduce配置文件:配置mapred-site.xml</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ vim mapred-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>文件内容如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- 指定MapReduce程序运行在Yarn上 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.framework.name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>yarn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4）在集群上分发配置好的Hadoop配置文件，将配置文件同步到hadoop103和hadoop104</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ xsync /opt/module/hadoop-3.1.3/etc/hadoop/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）去103和104上查看文件分发情况</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop103 ~<span class="token punctuation">]</span>$ <span class="token function">cat</span> /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml
<span class="token punctuation">[</span>root@hadoop104 ~<span class="token punctuation">]</span>$ <span class="token function">cat</span> /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="4-2-4-启动集群"><a href="#4-2-4-启动集群" class="headerlink" title="4.2.4  启动集群"></a>4.2.4 启动集群</h4><p><strong>1）配置workers</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ vim /opt/module/hadoop-3.1.3/etc/hadoop/workers<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在该文件中增加如下内容：</p><pre><code>hadoop102
hadoop103
hadoop104</code></pre><p>注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。</p><p>同步所有节点配置文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ xsync /opt/module/hadoop-3.1.3/etc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>2）启动集群</strong></p><p>​ （1）如果集群是第一次启动，需要在hadoop102节点格式化NameNode（注意格式化NameNode，会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化NameNode的话，一定要先停止namenode和datanode进程，并且要删除所有机器的data和logs目录，然后再进行格式化。）</p><pre><code>[root@hadoop102 ~]$ hdfs namenode -format</code></pre><p>（2）启动HDFS</p><pre><code>[root@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</code></pre><p>（3）在配置了ResourceManager的节点（hadoop103）启动YARN</p><pre><code>[root@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</code></pre><p>（4）Web端查看HDFS的NameNode</p><p>​ （a）浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop102:9870/">http://hadoop102:9870</a></p><p>​ （b）查看HDFS上存储的数据信息</p><p>（5）Web端查看YARN的ResourceManager</p><p>​ （a）浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop103:8088/">http://hadoop103:8088</a></p><p>​ （b）查看YARN上运行的Job信息</p><h4 id="4-2-5-集群基本测试"><a href="#4-2-5-集群基本测试" class="headerlink" title="4.2.5 集群基本测试"></a>4.2.5 集群基本测试</h4><p>HDFS相当于一个文件存储框架，搭好集群后，可以在集群去对文件进行操作，上传，下载，删除，查看等。</p><p><strong>（1）上传文件到集群</strong></p><p>​ 上传小文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ hadoop fs -mkdir /input
<span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ hadoop fs -put <span class="token variable">$HADOOP_HOME</span>/wcinput/word.txt /input<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>​ 上传大文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ hadoop fs -put  /opt/software/jdk-8u212-linux-x64.tar.gz  /<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>（2）上传文件后查看文件存放在什么位置</strong></p><p>（a）查看HDFS文件存储路径</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 subdir0<span class="token punctuation">]</span>$ <span class="token function">pwd</span>
/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-938951106-192.168.10.107-1495462844069/current/finalized/subdir0/subdir0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（b）查看HDFS在磁盘存储文件内容</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 subdir0<span class="token punctuation">]</span>$ <span class="token function">cat</span> blk_1073741825
hadoop yarn
hadoop mapreduce 
root
root<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>（3）下载</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop104 software<span class="token punctuation">]</span>$ hadoop fs -get /jdk-8u212-linux-x64.tar.gz ./<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>（4）执行wordcount程序</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-2-6-集群启动-停止方式总结"><a href="#4-2-6-集群启动-停止方式总结" class="headerlink" title="4.2.6 集群启动/停止方式总结"></a>4.2.6 集群启动/停止方式总结</h4><p><strong>1）各个服务组件逐一启动/停止</strong></p><p>​ （1）分别启动/停止HDFS组件</p><pre class="line-numbers language-bash"><code class="language-bash">hdfs --daemon start/stop namenode/datanode/secondarynamenode<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​ （2）启动/停止YARN</p><pre class="line-numbers language-bash"><code class="language-bash">yarn --daemon start/stop resourcemanager/nodemanager<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）各个模块分开启动/停止（配置ssh是前提）常用</p><p>​ （1）整体启动/停止HDFS</p><pre class="line-numbers language-bash"><code class="language-bash">start-dfs.sh/stop-dfs.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​ （2）整体启动/停止YARN</p><pre class="line-numbers language-bash"><code class="language-bash">start-yarn.sh/stop-yarn.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-2-7-编写hadoop集群常用脚本"><a href="#4-2-7-编写hadoop集群常用脚本" class="headerlink" title="4.2.7 编写hadoop集群常用脚本"></a>4.2.7 编写hadoop集群常用脚本</h4><p><strong>(1）查看三台服务器java进程脚本：jpsall</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">cd</span> /home/root/bin
<span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ vim jpsall<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>然后输入</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>
<span class="token keyword">for</span> host <span class="token keyword">in</span> hadoop102 hadoop103 hadoop104
<span class="token keyword">do</span>
​    <span class="token keyword">echo</span> <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span> <span class="token variable">$host</span> <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span>
​    <span class="token function">ssh</span> <span class="token variable">$host</span> jps <span class="token variable">$@</span> <span class="token operator">|</span> <span class="token function">grep</span> -v Jps
<span class="token keyword">done</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>保存后退出，然后赋予脚本执行权限</p><p>[root@hadoop102 bin]$ chmod +x jpsall</p><p><strong>（2）hadoop集群启停脚本（包含hdfs，yarn，historyserver）：</strong>myhadoop.sh</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">cd</span> /home/root/bin
<span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ vim myhadoop.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>然后输入</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>

<span class="token keyword">if</span> <span class="token punctuation">[</span> $<span class="token comment" spellcheck="true"># -lt 1 ]</span>
<span class="token keyword">then</span>
  <span class="token keyword">echo</span> <span class="token string">"No Args Input..."</span>
  <span class="token keyword">exit</span> <span class="token punctuation">;</span>
<span class="token keyword">fi</span>
<span class="token keyword">case</span> <span class="token variable">$1</span> <span class="token keyword">in</span>
<span class="token string">"start"</span><span class="token punctuation">)</span>
​    <span class="token keyword">echo</span> <span class="token string">" =================== 启动 hadoop集群 ==================
​    echo "</span> --------------- 启动 hdfs ---------------<span class="token string">"
​    ssh hadoop102 "</span>/opt/module/hadoop-3.1.3/sbin/start-dfs.sh<span class="token string">"
​    echo "</span> --------------- 启动 yarn ---------------<span class="token string">"
​    ssh hadoop103 "</span>/opt/module/hadoop-3.1.3/sbin/start-yarn.sh<span class="token string">"
​    echo "</span> --------------- 启动 historyserver ---------------<span class="token string">"
​    ssh hadoop102 "</span>/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver<span class="token string">"
;;

"</span>stop<span class="token string">")
​    echo "</span> <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span> 关闭 hadoop集群 <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span>
​    <span class="token keyword">echo</span> <span class="token string">" --------------- 关闭 historyserver ---------------"</span>
​    <span class="token function">ssh</span> hadoop102 <span class="token string">"/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"</span>
​    <span class="token keyword">echo</span> <span class="token string">" --------------- 关闭 yarn ---------------"</span>
​    <span class="token function">ssh</span> hadoop103 <span class="token string">"/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"</span>
​    <span class="token keyword">echo</span> <span class="token string">" --------------- 关闭 hdfs ---------------"</span>
​    <span class="token function">ssh</span> hadoop102 <span class="token string">"/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"</span>

<span class="token punctuation">;</span><span class="token punctuation">;</span>

*<span class="token punctuation">)</span>
  <span class="token keyword">echo</span> <span class="token string">"Input Args Error..."</span>

<span class="token punctuation">;</span><span class="token punctuation">;</span>
esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>保存后退出，然后赋予脚本执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> +x myhadoop.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）分发/home/root/bin目录，保证自定义脚本在三台机器上都可以使用</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ xsync /home/root/bin/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="5-hdfs常用shell操作"><a href="#5-hdfs常用shell操作" class="headerlink" title="5 hdfs常用shell操作"></a>5 hdfs常用shell操作</h2><h3 id="2-1-基本语法"><a href="#2-1-基本语法" class="headerlink" title="2.1 基本语法"></a>2.1 基本语法</h3><p>hadoop fs 具体命令 OR hdfs dfs 具体命令</p><p>两个是完全相同的。</p><h3 id="2-2-命令大全"><a href="#2-2-命令大全" class="headerlink" title="2.2 命令大全"></a>2.2 命令大全</h3><p>查看所有命令</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ bin/hadoop fs<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-3-常用命令实操"><a href="#2-3-常用命令实操" class="headerlink" title="2.3 常用命令实操"></a>2.3 常用命令实操</h3><h4 id="2-3-1-准备工作"><a href="#2-3-1-准备工作" class="headerlink" title="2.3.1 准备工作"></a>2.3.1 准备工作</h4><p>1）启动Hadoop集群（方便后续的测试）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-dfs.sh
<span class="token punctuation">[</span>root@hadoop103 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-yarn.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）-help：输出这个命令参数</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -help <span class="token function">rm</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-3-2-上传"><a href="#2-3-2-上传" class="headerlink" title="2.3.2 上传"></a>2.3.2 上传</h4><p>1）-moveFromLocal：从本地剪切粘贴到HDFS</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">touch</span> kongming.txt
<span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -copyFromLocal README.txt /<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）-appendToFile：追加一个文件到已经存在的文件末尾</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">touch</span> liubei.txt
<span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">vi</span> liubei.txt
san gu mao lu
<span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>4）-put：等同于copyFromLocal</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -put ./liubei.txt /user/root/test/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-3-3-下载"><a href="#2-3-3-下载" class="headerlink" title="2.3.3 下载"></a>2.3.3 下载</h4><p>1）-copyToLocal：从HDFS拷贝到本地</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）-get：等同于copyToLocal，就是从HDFS下载文件到本地</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -get /sanguo/shuguo/kongming.txt ./<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）-getmerge：合并下载多个文件，比如HDFS的目录 /user/root/test下有多个文件:log.1, log.2,log.3,…</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -getmerge /user/root/test/* ./zaiyiqi.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="2-3-4-HDFS直接操作"><a href="#2-3-4-HDFS直接操作" class="headerlink" title="2.3.4 HDFS直接操作"></a>2.3.4 HDFS直接操作</h4><p>1）-ls: 显示目录信息</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -ls /<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）-mkdir：在HDFS上创建目录</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -mkdir -p /sanguo/shuguo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）-cat：显示文件内容</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -cat /sanguo/shuguo/kongming.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -chmod 666 /sanguo/shuguo/kongming.txt
<span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -chown root:root  /sanguo/shuguo/kongming.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>5）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>6）-mv：在HDFS目录中移动文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -mv /zhuge.txt /sanguo/shuguo/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>7）-tail：显示一个文件的末尾1kb的数据</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -tail /sanguo/shuguo/kongming.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>8）-rm：删除文件或文件夹</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -rm /user/root/test/jinlian2.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>9）-rmdir：删除空目录</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -mkdir /test
<span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -rmdir /test<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>10）-du统计文件夹的大小信息</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -du -s -h /user/root/test<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>11）-setrep：设置HDFS中文件的副本数量</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。</p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul></div><span class="post-count">总字数4.1k</span> <span class="post-count">预计阅读19分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-hdfs" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/12/bigdata-hdfs/" class="article-date"><time class="published" datetime="2020-11-12T07:34:38.000Z" itemprop="datePublished">2020-11-12 发布</time> <time class="updated" datetime="2021-11-12T08:52:00.044Z" itemprop="dateUpdated">2021-11-12 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/12/bigdata-hdfs/">Hadoop 教程（一）hadoop介绍</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="1-hadoop是什么"><a href="#1-hadoop是什么" class="headerlink" title="1 hadoop是什么"></a>1 hadoop是什么</h2><p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构。它提供了一个<strong>海量数据存储</strong>和<strong>分析计算</strong>的能力。广义上来说Hadoop这个词代表了Hadoop生态圈。</p><p>Hadoop的核心是YARN,HDFS和Mapreduce。随着处理任务不同，各种组件相继出现，丰富Hadoop生态圈，目前生态圈结构大致如图所示 。</p><p><img src="/2020/11/12/bigdata-hdfs/1636705530947.png" alt="1636705530947"></p><h2 id="2-Hadoop的特点"><a href="#2-Hadoop的特点" class="headerlink" title="2. Hadoop的特点"></a>2. Hadoop的特点</h2><ul><li>扩容能力（Scalable） 能可靠地（reliably）存储和处理千兆字节（PB）数据</li><li>成本低（Economical） 可以通过普通机器组成的服务器集群来分发以及处理数据。这些服务器几圈总计可以达到千个节点。</li><li>高效率（Efficient） 通过分发数据，hadoop 可以在数据所在的节点上并行的(parallel)处理它们，这使得处理非常快。</li><li>可靠性（Reliable） hadoop 能自动地维护数据的多份副本，并且在任务失败后能自动重新部署(redeploy)计算任务</li></ul><h2 id="3-Hadoop三大发行版本"><a href="#3-Hadoop三大发行版本" class="headerlink" title="3 Hadoop三大发行版本"></a>3 Hadoop三大发行版本</h2><p>Hadoop三大发行版本：Apache、Cloudera、Hortonworks。</p><p>Apache版本最原始（最基础）的版本，对于入门学习最好。</p><p>Cloudera内部集成了很多大数据框架。对应产品CDH。</p><p>Hortonworks文档较好。对应产品HDP。后面被Cloudera收购</p><h2 id="4-Hadoop组成"><a href="#4-Hadoop组成" class="headerlink" title="4 Hadoop组成"></a>4 Hadoop组成</h2><p>Hadoop的核心组件分为：HDFS（分布式文件系统）、MapRuduce（分布式运算编程框架）、YARN（运算资源调度系统） 。</p><p><img src="/2020/11/12/bigdata-hdfs/1636706148747.png" alt="1636706148747"></p><h3 id="4-1-HDFS"><a href="#4-1-HDFS" class="headerlink" title="4.1 HDFS"></a>4.1 HDFS</h3><p>​ 整个Hadoop的体系结构主要是通过HDFS（Hadoop分布式文件系统）来实现对分布式存储的底层支持，并通过MR来实现对分布式并行任务处理的程序支持。<strong>HDFS是Hadoop体系中数据存储管理的基础</strong>。</p><p>一个HDFS集群是由一个NameNode和若干个DataNode组成的。NameNodee存储元数据，作为主服务器，管理文件系统命名空间和客户端对文件的访问操作。DataNode存储数据，DataNode管理存储的数据。HDFS支持文件形式的数据。</p><p><img src="/2020/11/12/bigdata-hdfs/1636706441346.png" alt="1636706441346"></p><h3 id="4-2-YARN架构"><a href="#4-2-YARN架构" class="headerlink" title="4.2  YARN架构"></a>4.2 YARN架构</h3><p>上面我们说了 Hadoop2.x 中增加了 Yarn(资源调度)，那资源调度是在调度什么呢？在计算机中资源就是CPU和内存，CPU和内存都是有上限的，所以需要分配给更需要的进程来使用。</p><p><img src="/2020/11/12/bigdata-hdfs/1636706565758.png" alt="1636706565758"></p><p>ResourceManager（RM）就是资源管理者，外部的客户端提交作业请求都会先到 ResourceManager（RM），他代表了集群所有的资源，并监控 NodeManager、启动或监控ApplicationMaster。</p><p>NodeManager（NM） 只管理一个节点的资源，处理来自ResourceManager（RM）的命令和来自ApplicationMaster的命令。</p><p>ApplicationMaster（AM）负责数据的切分、为应用程序申请资源分配内部任务和任务的监控容错。当一个任务提交到 ResourceManager（RM）时就会选择一个节点启动一个ApplicationMaster（AM）来负责这个任务的跟进，也就是对这个任务的一个负责人。也就是说有一个作业任务就会有对应的一个ApplicationMaster（AM）来跟进这个作业任务的执行和调度。</p><p>Container 是对资源的一个抽象封装，里面会包含内存、CPU、磁盘、网络等资源，NodeManager（NM） 就是通过打开和关闭 Container 来调度资源的。</p><h3 id="4-3-MapReduce架构概述"><a href="#4-3-MapReduce架构概述" class="headerlink" title="4.3   MapReduce架构概述"></a>4.3 MapReduce架构概述</h3><p>MapReduce是一种编程模型，用于大规模数据集的并行计算，需要将数据分配到大量的机器上计算，每台机器运行一个子计算任务，最后再合并每台机器运算结果并输出。 MapReduce 的思想就是 『分而治之』.</p><p>MapReduce将计算过程分为两个阶段：Map和Reduce</p><p>1）Map阶段并行处理输入数据</p><p>2）Reduce阶段对Map结果进行汇总</p><p><img src="/2020/11/12/bigdata-hdfs/1636706761153.png" alt="1636706761153"></p><h2 id="5-大数据技术生态体系"><a href="#5-大数据技术生态体系" class="headerlink" title="5  大数据技术生态体系"></a>5 大数据技术生态体系</h2><p><img src="/2020/11/12/bigdata-hdfs/1636706809649.png" alt="1636706809649"></p><p>图中涉及的技术名词解释如下：</p><p>1）Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库（MySql）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p><p>2）Flume：Flume是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；</p><p>3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统；</p><p>4）Spark：是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。</p><p>5）Flink：Flink是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。</p><p>6）Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。</p><p>7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</p><p>8）Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p><p>9）ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。</p><h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1661405">https://cloud.tencent.com/developer/article/1661405</a></p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul></div><span class="post-count">总字数1.4k</span> <span class="post-count">预计阅读5分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-cipher-certificate-format" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/10/10/cipher-certificate-format/" class="article-date"><time class="published" datetime="2020-10-10T06:56:33.000Z" itemprop="datePublished">2020-10-10 发布</time> <time class="updated" datetime="2021-02-19T08:09:25.012Z" itemprop="dateUpdated">2021-02-19 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/10/10/cipher-certificate-format/">证书的各种格式</a></h1></header><div class="article-entry" itemprop="articleBody"><p>待完善</p></div><div class="article-info article-info-index"><div class="article-count"><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/" rel="tag">密码学</a></li></ul></div><span class="post-count">总字数98</span> <span class="post-count">预计阅读1分钟</span></div><p class="article-more-link"><a href="/2020/10/10/cipher-certificate-format/#more">阅读全文 >></a></p><div class="clearfix"></div></div></div></article><article id="post-docker-guide" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/09/28/docker-guide/" class="article-date"><time class="published" datetime="2020-09-28T07:22:20.000Z" itemprop="datePublished">2020-09-28 发布</time> <time class="updated" datetime="2021-11-23T09:11:41.252Z" itemprop="dateUpdated">2021-11-23 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/09/28/docker-guide/">docker使用大全</a></h1></header><div class="article-entry" itemprop="articleBody"><p>自己检索方便</p></div><div class="article-info article-info-index"><div class="article-count"><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8%E6%95%99%E7%A8%8B/" rel="tag">容器教程</a></li></ul></div><span class="post-count">总字数1.1k</span> <span class="post-count">预计阅读4分钟</span></div><p class="article-more-link"><a href="/2020/09/28/docker-guide/#more">阅读全文 >></a></p><div class="clearfix"></div></div></div></article><article id="post-linux-cmd" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/09/28/linux-cmd/" class="article-date"><time class="published" datetime="2020-09-28T07:22:20.000Z" itemprop="datePublished">2020-09-28 发布</time> <time class="updated" datetime="2021-04-09T08:42:58.820Z" itemprop="dateUpdated">2021-04-09 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/09/28/linux-cmd/">linux命令大全</a></h1></header><div class="article-entry" itemprop="articleBody"><p>linux命令大全，方便自己检索</p></div><div class="article-info article-info-index"><div class="article-count"><span class="post-count">总字数906</span> <span class="post-count">预计阅读4分钟</span></div><p class="article-more-link"><a href="/2020/09/28/linux-cmd/#more">阅读全文 >></a></p><div class="clearfix"></div></div></div></article><article id="post-pt-metosploitInAliyun" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/09/28/pt-metosploitInAliyun/" class="article-date"><time class="published" datetime="2020-09-28T07:22:20.000Z" itemprop="datePublished">2020-09-28 发布</time> <time class="updated" datetime="2021-02-19T08:03:35.854Z" itemprop="dateUpdated">2021-02-19 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/09/28/pt-metosploitInAliyun/">在阿里云主机反弹metosploit</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><p>一般反弹shell，需要一个公网Ip的机器，这里我们选择阿里云主机，因为阿里云主机默认是只开放了固定端口，因此我们想要对某个端口监听的，首先需要开起阿里云的具体端口，这里我们选择7777作为监听端口，具体配置如下图所示。这里还应该注意的是阿里云主机有两个IP，一个是公网IP，另外一个是内网IP，具体用法下面会说到。</p><p>利用阿里云作为反弹主机，因为阿里云主机默认是将端口关闭的，因此首先需要允许对应端口开放，这里我选择的是7777，具体配置如下图所示。需要注意的是，阿里主机有两个IP，一个为公网IP，一个为内网IP，这两个IP后面会说到。</p></div><div class="article-info article-info-index"><div class="article-count"><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" rel="tag">渗透测试</a></li></ul></div><span class="post-count">总字数733</span> <span class="post-count">预计阅读2分钟</span></div><p class="article-more-link"><a href="/2020/09/28/pt-metosploitInAliyun/#more">阅读全文 >></a></p><div class="clearfix"></div></div></div></article><nav id="page-nav"><a class="extend prev" rel="prev" href="/page/3/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/5/">Next &amp;raquo;</a></nav></div><footer id="footer"><div class="outer"><div id="footer-info"><div class="footer-left"><i class="fa fa-copyright"></i> 2017-2021 冀-18010769-1</div><div class="visit"><span id="busuanzi_container_site_pv" style="display:none"><span id="site-visit" title="本站到访人数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span> </span></span><span>| </span><span id="busuanzi_container_page_pv" style="display:none"><span id="page-visit" title="本站总访问量"><i class="fa fa-eye" aria-hidden="true"></i><span id="busuanzi_value_site_pv"></span></span></span></div><div class="footer-right"><i class="fa fa-heart"></i><a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架"> Hexo</a> Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a></div></div></div></footer></div><script type="application/javascript">var leftWidth,hide=!1;$(".hide-left-col").click(function(){hide=hide?($(".left-col").css("width",leftWidth),$(".left-col .intrude-less").fadeIn(200),$("#tocButton").fadeIn(200),"block"===$("#switch-btn").css("display")&&"block"===$("#switch-area").css("display")||$("#toc").fadeIn(200),$(".hide-left-col").css("left",leftWidth).html('<i class="fa fa-angle-double-left"></i>'),$(".mid-col").css("left",leftWidth),$("#post-nav-button").css("left",leftWidth),$("#post-nav-button > a:nth-child(2)").css("display","block"),!1):(leftWidth=$(".left-col")[0].style.width,$(".left-col").css("width",0),$(".left-col .intrude-less").fadeOut(200),$("#toc").fadeOut(100),$("#tocButton").fadeOut(100),$(".hide-left-col").css("left",0).html('<i class="fa fa-angle-double-right"></i>'),$(".mid-col").css("left",0),$("#post-nav-button").css("left",0),$("#post-nav-button > a:nth-child(2)").css("display","none"),$(".post-list").is(":visible")&&($("#post-nav-button .fa-bars,#post-nav-button .fa-times").toggle(),$(".post-list").toggle()),!0)})</script><script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.3.5/require.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[["$$","$$"],["$","$"],["\\(","\\)"]],processEscapes:!0,skipTags:["script","noscript","style","textarea","pre","code"]}}),MathJax.Hub.Queue(function(){var a,e=MathJax.Hub.getAllJax();for(a=0;a<e.length;a+=1)e[a].SourceElement().parentNode.className+=" has-jax"})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script><div class="scroll" id="scroll"><a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a> <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a> <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a></div><script>var oOpenInNew={post:".copyright a[href]",friends:"#js-friends a",socail:".social a"};for(var x in oOpenInNew)$(oOpenInNew[x]).attr("target","_blank")</script><script>var titleTime,originTitle=document.title;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="(つェ⊂)"+originTitle,clearTimeout(titleTime)):(document.title="(*´∇｀*)~ "+originTitle,titleTime=setTimeout(function(){document.title=originTitle},2e3))})</script><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><link href="//cdn.bootcss.com/aos/2.2.0/aos.css" rel="stylesheet"><script type="text/javascript">AOS.init({easing:"ease-out-back",once:!0})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"live2d_models/live2d-widget-model-izumi"},"display":{"position":"right","width":100,"height":200,"hOffset":-50,"vOffset":-85},"mobile":{"show":false},"react":{"opacityDefault":0.9,"opacityOnHover":0.3},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false});</script></body></html>