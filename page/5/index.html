<!DOCTYPE html><html lang="zh-Hans"><head><!--[if IE]><style>body{display:none;}</style><script>alert('IE浏览器下无法展示效果，请更换浏览器！');var headNode=document.getElementsByTagName('head')[0];var refresh=document.createElement('meta');refresh.setAttribute('http-equiv','Refresh');refresh.setAttribute('Content','0; url=http://outdatedbrowser.com/');headNode.appendChild(refresh);</script><![endif]--><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="m01ly"><meta name="baidu-site-verification" content="yvSXOjM1ag"><meta name="google-site-verification" content="riwnp5QCq6dRKbhBa3d3aDZrfGLQVhMFN9d4fMGYuoU"><meta name="description" content="description123456"><meta property="og:type" content="website"><meta property="og:title" content="Hexo"><meta property="og:url" content="https://m01ly.github.io/page/5/index.html"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="description123456"><meta property="og:locale"><meta property="article:author" content="m01ly"><meta property="article:tag" content="description123456"><meta name="twitter:card" content="summary"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="shortcut icon" href="/favicon.ico"><link href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css" rel="stylesheet"><link rel="stylesheet" href="/css/style.css"><title>Hexo</title><script src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"><script src="//cdn.bootcss.com/aos/2.2.0/aos.js"></script><script>var yiliaConfig={fancybox:!0,isHome:!0,isPost:!1,isArchive:!1,isTag:!1,isCategory:!1,fancybox_js:"//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js",search:!0}</script><script>yiliaConfig.jquery_ui=[!1]</script><script>yiliaConfig.rootUrl="/"</script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/rss+xml">
<link rel="stylesheet" href="/css/prism-a11y-dark.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="container"><div class="left-col"><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><form id="search-form"><input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false"> <i class="fa fa-times" onclick="resetSearch()"></i></form><div id="local-search-result"></div><p class="no-result">No results found <i class="fa fa-spinner fa-pulse"></i></p><div id="switch-btn" class="switch-btn"><div class="icon"><div class="icon-ctn"><div class="icon-wrap icon-house" data-idx="0"><div class="birdhouse"></div><div class="birdhouse_holes"></div></div><div class="icon-wrap icon-ribbon hide" data-idx="1"><div class="ribbon"></div></div><div class="icon-wrap icon-link hide" data-idx="2"><div class="loopback_l"></div><div class="loopback_r"></div></div><div class="icon-wrap icon-me hide" data-idx="3"><div class="user"></div><div class="shoulder"></div></div></div></div><div class="tips-box hide"><div class="tips-arrow"></div><ul class="tips-inner"><li>菜单</li><li>标签</li><li>友情链接</li><li>目标</li></ul></div></div><div id="switch-area" class="switch-area"><div class="switch-wrap"><section class="switch-part switch-part1"><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" rel="noopener" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" rel="noopener" href="https://music.163.com/" title="网易云音乐"></a></ul><ul class="social"><div class="donateIcon-position"><p style="display:block"><a class="donateIcon" href="javascript:void(0)" onmouseout='var qr=document.getElementById("donate");qr.style.display="none"' onmouseenter='var qr=document.getElementById("donate");qr.style.display="block"'>赏</a></p><div id="donate"><img id="multipay" src="/img/multipay.png" width="250px" alt="m01ly Multipay"><div class="triangle"></div></div></div></ul></nav></section><section class="switch-part switch-part2"><div class="widget tagcloud" id="js-tagcloud"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TLS/" rel="tag">TLS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zookeeper/" rel="tag">Zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/" rel="tag">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/" rel="tag">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BC%81%E4%B8%9A%E5%AE%89%E5%85%A8%E5%BB%BA%E8%AE%BE/" rel="tag">企业安全建设</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E5%85%A8%E6%89%AB%E6%8F%8F/" rel="tag">安全扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" rel="tag">安装教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8%E6%95%99%E7%A8%8B/" rel="tag">容器教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/" rel="tag">密码学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" rel="tag">插件开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E4%BB%93%E9%87%87%E9%9B%86%E9%A1%B9%E7%9B%AE/" rel="tag">数仓采集项目</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/" rel="tag">日志管理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/" rel="tag">流量分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" rel="tag">渗透测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%8F%E6%B4%9E%E6%89%AB%E6%8F%8F/" rel="tag">漏洞扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%BB%E5%8A%A8%E5%AE%89%E5%85%A8/" rel="tag">移动安全</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%B6%E5%9C%BAwriteup/" rel="tag">靶场writeup</a></li></ul></div></section><section class="switch-part switch-part3"><div id="js-friends"><a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/TechCatsLab">TechCatsLab</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://yangchenglong11.github.io">YangChengLong</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://jsharkc.github.io">LiuJiaChang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://blog.yusank.space">YusanKurban</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.lizebang.top">Lizebang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/sunanxiang">SunAnXiang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/DoubleWoodH">LinHao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.littlechao.top">ShiChao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/Txiaozhe">TangXiaoJi</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/LLLeon">JiaChenHui</a></div></section><section class="switch-part switch-part4"><div id="js-aboutme">不悲不喜，不卑不亢，努力成为一个更好的程序猿！</div></section></div></div></header></div></div><div class="hide-left-col" title="隐藏侧栏"><i class="fa fa-angle-double-left"></i></div><div class="mid-col"><nav id="mobile-nav"><div class="overlay"><div class="slider-trigger"></div><h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">m01ly</a></h1></div><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/" title="回到主页">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li><div class="clearfix"></div></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" href="https://music.163.com/" title="网易云音乐"></a></ul></nav></header></div><link class="menu-list" tags="标签" friends="友情链接" about="目标"></nav><div class="body-wrap"><article id="post-bigdata-hive2" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/15/bigdata-hive2/" class="article-date"><time class="published" datetime="2020-11-15T07:45:51.000Z" itemprop="datePublished">2020-11-15 发布</time> <time class="updated" datetime="2021-11-16T08:55:28.424Z" itemprop="dateUpdated">2021-11-16 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/15/bigdata-hive2/">Hive学习笔记（二） Hive对数据基本操作</a></h1></header><div class="article-entry" itemprop="articleBody"><p>下面我们通过Hive对数据进行操作，主要包括对数据库，表的基本操作和基本函数的使用。</p><h1 id="1-Hive数据类型"><a href="#1-Hive数据类型" class="headerlink" title="1   Hive数据类型"></a>1 Hive数据类型</h1><h2 id="1-1-基本数据类型"><a href="#1-1-基本数据类型" class="headerlink" title="1.1 基本数据类型"></a>1.1 基本数据类型</h2><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE FALSE</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td></td><td>字节数组</td><td></td></tr></tbody></table><p>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p><h2 id="1-2-集合数据类型"><a href="#1-2-集合数据类型" class="headerlink" title="1.2 集合数据类型"></a>1.2 集合数据类型</h2><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct() 例如struct&lt;street:string, city:string&gt;</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map() 例如map&lt;string, int&gt;</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array() 例如array<string></string></td></tr></tbody></table><p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p><p>1）案例实操</p><p>（1）假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</p><pre class="line-numbers language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>

  <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"songsong"</span><span class="token punctuation">,</span>

  <span class="token property">"friends"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"bingbing"</span> <span class="token punctuation">,</span> <span class="token string">"lili"</span><span class="token punctuation">]</span> <span class="token punctuation">,</span>    //列表Array<span class="token punctuation">,</span> 

  <span class="token property">"children"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>           //键值Map<span class="token punctuation">,</span>

​    <span class="token property">"xiao song"</span><span class="token operator">:</span> <span class="token number">19</span> <span class="token punctuation">,</span>

​    <span class="token property">"xiaoxiao song"</span><span class="token operator">:</span> <span class="token number">18</span>

  &amp;#<span class="token number">125</span><span class="token punctuation">;</span>

  <span class="token property">"address"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>           //结构Struct<span class="token punctuation">,</span>

​    <span class="token property">"street"</span><span class="token operator">:</span> <span class="token string">"hui long guan"</span> <span class="token punctuation">,</span>

​    <span class="token property">"city"</span><span class="token operator">:</span> <span class="token string">"beijing"</span> 

  &amp;#<span class="token number">125</span><span class="token punctuation">;</span>

&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据。</p><p>创建本地测试文件test.txt</p><p>songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</p><p>yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</p><p>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p><p>（3）Hive上创建测试表test</p><pre class="line-numbers language-bash"><code class="language-bash">create table test<span class="token punctuation">(</span>
name string,
friends array<span class="token operator">&lt;</span>string<span class="token operator">></span>,
children map<span class="token operator">&lt;</span>string, int<span class="token operator">></span>,
address struct<span class="token operator">&lt;</span>street:string, city:string<span class="token operator">></span>
<span class="token punctuation">)</span>
row <span class="token function">format</span> delimited fields terminated by <span class="token string">','</span>
collection items terminated by <span class="token string">'_'</span>
map keys terminated by <span class="token string">':'</span>
lines terminated by <span class="token string">'\n'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>字段解释：</p><p>row format delimited fields terminated by ‘,’ – 列分隔符</p><p>collection items terminated by ‘_’ –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p><p>map keys terminated by ‘:’ – MAP中的key与value的分隔符</p><p>lines terminated by ‘\n’; – 行分隔符</p><p>（4）导入文本数据到测试表</p><p>load data local inpath ‘/opt/module/hive/datas/test.txt’ into table test;</p><p>（5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> friends<span class="token punctuation">[</span>1<span class="token punctuation">]</span>,children<span class="token punctuation">[</span><span class="token string">'xiao song'</span><span class="token punctuation">]</span>,address.city from <span class="token function">test</span>
where name<span class="token operator">=</span><span class="token string">"songsong"</span><span class="token punctuation">;</span>
OK
_c0   _c1   city
lili  18   beijing
Time taken: 0.076 seconds, Fetched: 1 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="1-3-类型转化"><a href="#1-3-类型转化" class="headerlink" title="1.3 类型转化"></a>1.3 类型转化</h2><p>Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p><p>1）隐式类型转换规则如下</p><p>（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。</p><p>（2）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</p><p>（3）TINYINT、SMALLINT、INT都可以转换为FLOAT。</p><p>（4）BOOLEAN类型不可以转换为任何其它的类型。</p><p>2）可以使用CAST操作显示进行数据类型转换</p><p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p><pre class="line-numbers language-bash"><code class="language-bash">0: jdbc:hive2://hadoop102:10000<span class="token operator">></span> <span class="token keyword">select</span> <span class="token string">'1'</span>+2, cast<span class="token punctuation">(</span><span class="token string">'1'</span>as int<span class="token punctuation">)</span> + 2<span class="token punctuation">;</span>
+------+------+--+
<span class="token operator">|</span> _c0 <span class="token operator">|</span> _c1 <span class="token operator">|</span>
+------+------+--+
<span class="token operator">|</span> 3.0 <span class="token operator">|</span> 3  <span class="token operator">|</span>
+------+------+--+<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2 DDL数据定义</p><h1 id="2-DDL数据定义"><a href="#2-DDL数据定义" class="headerlink" title="2 DDL数据定义"></a>2 DDL数据定义</h1><p>数据库模式定义语言DDL(Data Definition Language)，是用于描述数据库中要存储的现实世界实体的语言。具体其实就是对数据库和表的CRUD操作。</p><h2 id="2-1-创建数据库"><a href="#2-1-创建数据库" class="headerlink" title="2.1 创建数据库"></a>2.1 创建数据库</h2><p>创建数据库语法如下所示：</p><pre class="line-numbers language-bash"><code class="language-bash">CREATE DATABASE <span class="token punctuation">[</span>IF NOT EXISTS<span class="token punctuation">]</span> database_name  --数据库名称
<span class="token punctuation">[</span>COMMENT database_comment<span class="token punctuation">]</span>  --数据库备注
<span class="token punctuation">[</span>LOCATION hdfs_path<span class="token punctuation">]</span>  ---数据库存储位置
<span class="token punctuation">[</span>WITH DBPROPERTIES <span class="token punctuation">(</span>property_name<span class="token operator">=</span>property_value, <span class="token punctuation">..</span>.<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">;</span> --DB的一些属性<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>1）创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create datavase mydb<span class="token punctuation">;</span>
create table <span class="token keyword">if</span> not exists test1<span class="token punctuation">(</span>
 <span class="token function">id</span> int comment <span class="token string">"this is id"</span>,
 name string comment <span class="token string">"this is name"</span>
<span class="token punctuation">)</span>
comment <span class="token string">"this is table"</span>
row <span class="token function">format</span> delimited fields terminated by <span class="token string">','</span>
STORED as textfile
TBLPROPERTIES<span class="token punctuation">(</span><span class="token string">"createtime="</span> 2020-4-11"<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以在hdfs上看到创建的数据库目录。</p><p><img src="/2020/11/15/bigdata-hive2/1636962952895.png" alt="1636962952898"></p><p>2）避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法）</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create database db_hive<span class="token punctuation">;</span>
FAILED: Execution Error, <span class="token keyword">return</span> code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create database <span class="token keyword">if</span> not exists db_hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>3）创建一个数据库，指定数据库在HDFS上存放的位置</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create database db_hive2 location <span class="token string">'/db_hive2.db'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-2-查询数据库"><a href="#2-2-查询数据库" class="headerlink" title="2.2 查询数据库"></a>2.2 查询数据库</h2><p>1）显示数据库</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> show databases<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）显示数据库信息</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> desc database db_hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）显示数据库详细信息，extended</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> desc database extended db_hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）切换当前数据库</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> use db_hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-3-修改数据库"><a href="#2-3-修改数据库" class="headerlink" title="2.3 修改数据库"></a>2.3 修改数据库</h2><p>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> alter database db_hive <span class="token keyword">set</span> dbproperties<span class="token punctuation">(</span><span class="token string">'createtime'</span><span class="token operator">=</span><span class="token string">'20170830'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在hive中查看修改结果</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> desc database extended db_hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-4-删除数据库"><a href="#2-4-删除数据库" class="headerlink" title="2.4 删除数据库"></a>2.4 删除数据库</h2><p>1）删除空数据库</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span>drop database db_hive2<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）如果删除的数据库不存在，最好采用 if exists判断数据库是否存在</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> drop database db_hive<span class="token punctuation">;</span>
FAILED: SemanticException <span class="token punctuation">[</span>Error 10072<span class="token punctuation">]</span>: Database does not exist: db_hive
hive<span class="token operator">></span> drop database <span class="token keyword">if</span> exists db_hive2<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>3）如果数据库不为空，可以采用cascade命令，强制删除</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> drop database db_hive<span class="token punctuation">;</span>
FAILED: Execution Error, <span class="token keyword">return</span> code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException<span class="token punctuation">(</span>message:Database db_hive is not empty. One or <span class="token function">more</span> tables exist.<span class="token punctuation">)</span>
hive<span class="token operator">></span> drop database db_hive cascade<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="2-5-创建表"><a href="#2-5-创建表" class="headerlink" title="2.5 创建表"></a>2.5 创建表</h2><p><strong>1）建表语法</strong></p><pre class="line-numbers language-bash"><code class="language-bash">CREATE <span class="token punctuation">[</span>EXTERNAL<span class="token punctuation">]</span> TABLE <span class="token punctuation">[</span>IF NOT EXISTS<span class="token punctuation">]</span> table_name -
<span class="token punctuation">[</span><span class="token punctuation">(</span>col_name data_type <span class="token punctuation">[</span>COMMENT col_comment<span class="token punctuation">]</span>, <span class="token punctuation">..</span>.<span class="token punctuation">)</span><span class="token punctuation">]</span> 
<span class="token punctuation">[</span>COMMENT table_comment<span class="token punctuation">]</span> 
<span class="token punctuation">[</span>PARTITIONED BY <span class="token punctuation">(</span>col_name data_type <span class="token punctuation">[</span>COMMENT col_comment<span class="token punctuation">]</span>, <span class="token punctuation">..</span>.<span class="token punctuation">)</span><span class="token punctuation">]</span> 
<span class="token punctuation">[</span>CLUSTERED BY <span class="token punctuation">(</span>col_name, col_name, <span class="token punctuation">..</span>.<span class="token punctuation">)</span> 
<span class="token punctuation">[</span>SORTED BY <span class="token punctuation">(</span>col_name <span class="token punctuation">[</span>ASC<span class="token operator">|</span>DESC<span class="token punctuation">]</span>, <span class="token punctuation">..</span>.<span class="token punctuation">)</span><span class="token punctuation">]</span> INTO num_buckets BUCKETS<span class="token punctuation">]</span> 
<span class="token punctuation">[</span>ROW FORMAT row_format<span class="token punctuation">]</span> 
<span class="token punctuation">[</span>STORED AS file_format<span class="token punctuation">]</span> 
<span class="token punctuation">[</span>LOCATION hdfs_path<span class="token punctuation">]</span>
<span class="token punctuation">[</span>TBLPROPERTIES <span class="token punctuation">(</span>property_name<span class="token operator">=</span>property_value, <span class="token punctuation">..</span>.<span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span>AS select_statement<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>2）字段解释说明</strong></p><p>（1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p><p>（2）<strong>EXTERNAL 关键字可以让用户创建一个外部表</strong>，在建表的同时可以指定一个指向实际数据的路径（LOCATION），<strong>在删除表的时候，内部表的元数据和hdfs数据会被一起删除，而外部表只删除元数据，不删除存在hdfs上的数据。</strong></p><p>（3）COMMENT：为表和列添加注释。</p><p>（4）PARTITIONED BY创建分区表</p><p>（5）CLUSTERED BY创建分桶表</p><p>（6）SORTED BY不常用，对桶中的一个或多个列另外排序</p><p>（7）ROW FORMAT</p><p>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]</p><p>​ [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</p><p>| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]</p><p>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。</p><p>SerDe是Serialize/Deserilize的简称， hive使用Serde进行行对象的序列与反序列化。</p><p>（8）STORED AS指定存储文件类型</p><p>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</p><p>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</p><p>（9）LOCATION ：指定表在HDFS上的存储位置。</p><p>（10）AS：后跟查询语句，根据查询结果创建表。</p><p>（11）LIKE允许用户复制现有的表结构，但是不复制数据。</p><h3 id="2-3-1-内部表-管理表"><a href="#2-3-1-内部表-管理表" class="headerlink" title="2.3.1 内部表(管理表)"></a>2.3.1 内部表(管理表)</h3><p>1）理论</p><p>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。 当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</p><p>（1）普通创建表</p><pre class="line-numbers language-bash"><code class="language-bash">create table <span class="token keyword">if</span> not exists student<span class="token punctuation">(</span>
<span class="token function">id</span> int, name string
<span class="token punctuation">)</span>
row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span>
stored as textfile
location <span class="token string">'/user/hive/warehouse/student'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）根据查询结果创建表（查询的结果会添加到新创建的表中）</p><pre class="line-numbers language-bash"><code class="language-bash">create table <span class="token keyword">if</span> not exists student2 as <span class="token keyword">select</span> id, name from student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）根据已经存在的表结构创建表</p><pre class="line-numbers language-bash"><code class="language-bash">create table <span class="token keyword">if</span> not exists student3 like student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）查询表的类型</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> desc formatted student2<span class="token punctuation">;</span>
Table Type:       MANAGED_TABLE <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="2-3-2-外部表"><a href="#2-3-2-外部表" class="headerlink" title="2.3.2 外部表"></a>2.3.2 外部表</h3><p>1）理论</p><p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</p><p>2）管理表和外部表的使用场景</p><p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p><p>3）案例实操</p><p>分别创建部门和员工外部表，并向表中导入数据。</p><p>（1）上传数据到HDFS</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -mkdir /student<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -put /opt/module/datas/student.txt /student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（2）建表语句，创建外部表</p><p>创建部门表</p><pre class="line-numbers language-bash"><code class="language-bash">create external table <span class="token keyword">if</span> not exists dept<span class="token punctuation">(</span>
deptno int,
dname string,
loc int<span class="token punctuation">)</span>
row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）查看创建的表</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>show tables<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）查看表格式化数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> desc formatted dept<span class="token punctuation">;</span>
Table Type:       EXTERNAL_TABLE<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（5）删除外部表</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> drop table dept<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>外部表删除后，hdfs中的数据还在，但是metadata中dept的元数据已被删除</p><h3 id="2-5-3-管理表与外部表的互相转换"><a href="#2-5-3-管理表与外部表的互相转换" class="headerlink" title="2.5.3 管理表与外部表的互相转换"></a>2.5.3 管理表与外部表的互相转换</h3><p>（1）查询表的类型</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> desc formatted student2<span class="token punctuation">;</span>
Table Type:       MANAGED_TABLE<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（2）修改内部表student2为外部表</p><pre class="line-numbers language-bash"><code class="language-bash">alter table student2 <span class="token keyword">set</span> tblproperties<span class="token punctuation">(</span><span class="token string">'EXTERNAL'</span><span class="token operator">=</span><span class="token string">'TRUE'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）修改外部表student2为内部表</p><pre class="line-numbers language-bash"><code class="language-bash">alter table student2 <span class="token keyword">set</span> tblproperties<span class="token punctuation">(</span><span class="token string">'EXTERNAL'</span><span class="token operator">=</span><span class="token string">'FALSE'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</p><h2 id="2-6-修改表"><a href="#2-6-修改表" class="headerlink" title="2.6 修改表"></a>2.6 修改表</h2><h3 id="2-6-1-重命名表"><a href="#2-6-1-重命名表" class="headerlink" title="2.6.1 重命名表"></a>2.6.1 重命名表</h3><p>1）语法</p><pre class="line-numbers language-bash"><code class="language-bash">ALTER TABLE table_name RENAME TO new_table_name<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）实操案例</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> alter table dept_partition2 <span class="token function">rename</span> to dept_partition3<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-6-2-增加-修改-替换列信息"><a href="#2-6-2-增加-修改-替换列信息" class="headerlink" title="2.6.2 增加/修改/替换列信息"></a>2.6.2 增加/修改/替换列信息</h3><p>1）语法</p><p>（1）更新列</p><pre class="line-numbers language-bash"><code class="language-bash">ALTER TABLE table_name CHANGE <span class="token punctuation">[</span>COLUMN<span class="token punctuation">]</span> col_old_name col_new_name column_type <span class="token punctuation">[</span>COMMENT col_comment<span class="token punctuation">]</span> <span class="token punctuation">[</span>FIRST<span class="token operator">|</span>AFTER column_name<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）增加和替换列</p><pre class="line-numbers language-bash"><code class="language-bash">ALTER TABLE table_name ADD<span class="token operator">|</span>REPLACE COLUMNS <span class="token punctuation">(</span>col_name data_type <span class="token punctuation">[</span>COMMENT col_comment<span class="token punctuation">]</span>, <span class="token punctuation">..</span>.<span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。</p><p>2）实操案例</p><p>（1）查询表结构</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> desc dept<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）添加列</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> alter table dept add columns<span class="token punctuation">(</span>deptdesc string<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）更新列</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> alter table dept change column deptdesc desc string<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（5）替换列</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> alter table dept replace columns<span class="token punctuation">(</span>deptno string, dname
 string, loc string<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="2-7-删除表"><a href="#2-7-删除表" class="headerlink" title="2.7 删除表"></a>2.7 删除表</h2><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> drop table dept<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="3-DML数据操作"><a href="#3-DML数据操作" class="headerlink" title="3  DML数据操作"></a>3 DML数据操作</h1><h2 id="3-1-数据导入"><a href="#3-1-数据导入" class="headerlink" title="3.1 数据导入"></a>3.1 数据导入</h2><p>让hdfs的数据和hive表产生关联。上传表数据：hive创建的表其实就是hdfs中的一个目录，所以表数据就映射为hdfs目录下的数据txt。</p><h3 id="3-1-1-向表中装载数据（Load）"><a href="#3-1-1-向表中装载数据（Load）" class="headerlink" title="3.1.1 向表中装载数据（Load）"></a>3.1.1 向表中装载数据（Load）</h3><p><strong>1）语法</strong></p><p>hive&gt; load data [local] inpath ‘数据的path’ [overwrite] into table student [partition (partcol1=val1,…)];</p><p>（1）load data:表示加载数据</p><p>（2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p><p>（3）inpath:表示加载数据的路径</p><p>（4）overwrite:表示覆盖表中已有数据，否则表示追加</p><p>（5）into table:表示加载到哪张表</p><p>（6）student:表示具体的表</p><p>（7）partition:表示上传到指定分区</p><p><strong>2）实操案例</strong></p><p>（0）创建一张表</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create table student<span class="token punctuation">(</span>id string, name string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（1）加载本地文件到hive</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> load data local inpath <span class="token string">'/opt/module/hive/datas/student.txt'</span> into table default.student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）加载HDFS文件到hive中</p><p>上传文件到HDFS</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -put /opt/module/hive/datas/student.txt /user/molly/hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>加载HDFS上数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> load data inpath <span class="token string">'/user/molly/hive/student.txt'</span> into table default.student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）加载数据覆盖表中已有的数据</p><p>上传文件到HDFS</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -put /opt/module/datas/student.txt /user/molly/hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>加载数据覆盖表中已有的数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> load data inpath <span class="token string">'/user/molly/hive/student.txt'</span> overwrite into table default.student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-1-2-通过查询语句向表中插入数据（Insert）"><a href="#3-1-2-通过查询语句向表中插入数据（Insert）" class="headerlink" title="3.1.2 通过查询语句向表中插入数据（Insert）"></a>3.1.2 通过查询语句向表中插入数据（Insert）</h3><p>1）创建一张表</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create table student_par<span class="token punctuation">(</span>id int, name string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）基本插入数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert into table student_par values<span class="token punctuation">(</span>1,<span class="token string">'wangwu'</span><span class="token punctuation">)</span>,<span class="token punctuation">(</span>2,<span class="token string">'zhaoliu'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）基本模式插入（根据单张表查询结果）</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite table student_par <span class="token keyword">select</span> id, name from student <span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>insert into：以追加数据的方式插入到表或分区，原有数据不会删除</p><p>insert overwrite：会覆盖表中已存在的数据</p><p>注意：insert不支持插入部分字段</p><p>4）多表（多分区）插入模式（根据多张表查询结果）</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> from student
​       insert overwrite table student partition<span class="token punctuation">(</span>month<span class="token operator">=</span><span class="token string">'201707'</span><span class="token punctuation">)</span>
​       <span class="token keyword">select</span> id, name where month<span class="token operator">=</span><span class="token string">'201709'</span>
​       insert overwrite table student partition<span class="token punctuation">(</span>month<span class="token operator">=</span><span class="token string">'201706'</span><span class="token punctuation">)</span>
​       <span class="token keyword">select</span> id, name where month<span class="token operator">=</span><span class="token string">'201709'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-1-3-查询语句中创建表并加载数据（As-Select）"><a href="#3-1-3-查询语句中创建表并加载数据（As-Select）" class="headerlink" title="3.1.3 查询语句中创建表并加载数据（As Select）"></a>3.1.3 查询语句中创建表并加载数据（As Select）</h3><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>create table <span class="token keyword">if</span> not exists student3 as <span class="token keyword">select</span> id, name from student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-1-4-创建表时通过Location指定加载数据路径—用的多"><a href="#3-1-4-创建表时通过Location指定加载数据路径—用的多" class="headerlink" title="3.1.4 创建表时通过Location指定加载数据路径—用的多"></a>3.1.4 创建表时通过Location指定加载数据路径—用的多</h3><p>1）上传数据到hdfs上</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -mkdir /student<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -put /opt/module/datas/student.txt /student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）创建表，并指定在hdfs上的位置</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create external table <span class="token keyword">if</span> not exists student5<span class="token punctuation">(</span>
​       <span class="token function">id</span> int, name string<span class="token punctuation">)</span>
​       row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span>
​       location '/student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>先建表，后指定hdfss数据文件到该表。</p><pre class="line-numbers language-bash"><code class="language-bash">hadoop fs -put test.txt /test2.table<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）查询数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from student5<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-1-5-Import数据到指定Hive表中"><a href="#3-1-5-Import数据到指定Hive表中" class="headerlink" title="3.1.5 Import数据到指定Hive表中"></a>3.1.5 Import数据到指定Hive表中</h3><p>注意：先用export导出后，再将数据导入。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token function">import</span> table student2  from <span class="token string">'/user/hive/warehouse/export/student'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="3-2-数据导出"><a href="#3-2-数据导出" class="headerlink" title="3.2 数据导出"></a>3.2 数据导出</h2><h3 id="3-2-1-Insert导出"><a href="#3-2-1-Insert导出" class="headerlink" title="3.2.1 Insert导出"></a>3.2.1 Insert导出</h3><p>1）将查询的结果导出到本地(有local)</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite local directory <span class="token string">'/opt/module/hive/datas/export/student'</span>
​      <span class="token keyword">select</span> * from student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）将查询的结果<strong>格式化</strong>导出到本地</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>insert overwrite local directory <span class="token string">'/opt/module/hive/datas/export/student1'</span>
​      ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="token string">'\t'</span>       <span class="token keyword">select</span> * from student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>3）将查询的结果导出到HDFS上(没有local)</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite directory <span class="token string">'/user/molly/student2'</span>
​       ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="token string">'\t'</span> 
​       <span class="token keyword">select</span> * from student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="3-2-2-Hadoop命令导出到本地"><a href="#3-2-2-Hadoop命令导出到本地" class="headerlink" title="3.2.2 Hadoop命令导出到本地"></a>3.2.2 Hadoop命令导出到本地</h3><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -get /user/hive/warehouse/student/student.txt
/opt/module/datas/export/student3.txt<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="3-2-3-Hive-Shell-命令导出"><a href="#3-2-3-Hive-Shell-命令导出" class="headerlink" title="3.2.3 Hive Shell 命令导出"></a>3.2.3 Hive Shell 命令导出</h3><p>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/hive -e <span class="token string">'select * from default.student;'</span> <span class="token operator">></span> /opt/module/hive/datas/export/student4.txt<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-2-4-Export导出到HDFS上"><a href="#3-2-4-Export导出到HDFS上" class="headerlink" title="3.2.4 Export导出到HDFS上"></a>3.2.4 Export导出到HDFS上</h3><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>export table default.student to <span class="token string">'/user/hive/warehouse/export/student'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p><h3 id="3-2-5-清除表中数据（Truncate）"><a href="#3-2-5-清除表中数据（Truncate）" class="headerlink" title="3.2.5 清除表中数据（Truncate）"></a>3.2.5 清除表中数据（Truncate）</h3><p>注意：Truncate只能删除管理表，不能删除外部表中数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> truncate table student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="4-数据查询—用的很多"><a href="#4-数据查询—用的很多" class="headerlink" title="4  数据查询—用的很多"></a>4 数据查询—用的很多</h1><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">查询语句语法</a>：</p><pre class="line-numbers language-bash"><code class="language-bash">SELECT <span class="token punctuation">[</span>ALL <span class="token operator">|</span> DISTINCT<span class="token punctuation">]</span> select_expr, select_expr, <span class="token punctuation">..</span>.
 FROM table_reference
 <span class="token punctuation">[</span>WHERE where_condition<span class="token punctuation">]</span> -筛选条件
 <span class="token punctuation">[</span>GROUP BY col_list<span class="token punctuation">]</span>---分组
 <span class="token punctuation">[</span>HAVING having_condition<span class="token punctuation">]</span>---分组后的过滤条件
 <span class="token punctuation">[</span>ORDER BY col_list<span class="token punctuation">]</span>—--全局排序
 <span class="token punctuation">[</span>CLUSTER BY col_list—---分区排序
<span class="token operator">|</span> 
<span class="token punctuation">[</span>DISTRIBUTE BY col_list<span class="token punctuation">]</span> –---分区排序  类似MR中的分区
<span class="token punctuation">[</span>SORT BY col_list<span class="token punctuation">]</span> –区内排序
 <span class="token punctuation">]</span>
 <span class="token punctuation">[</span>LIMIT number<span class="token punctuation">]</span>--限制返回条数<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>HQL查询语法和sql大致相同，这里就不一一列举，这里需要提出说的是排序的用法。</p><p>需要知道的是order by是全局排序，针对所有数据进行排序；SORT BY是分区内的排序</p><h3 id="4-1-全局排序（Order-By）"><a href="#4-1-全局排序（Order-By）" class="headerlink" title="4.1 全局排序（Order By）"></a>4.1 全局排序（Order By）</h3><p>Order By：<strong>全局排序</strong>，只有一个Reducer</p><p>例如：查询员工信息按工资升序排列</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp order by sal<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-2-每个Reduce内部排序（Sort-By）"><a href="#4-2-每个Reduce内部排序（Sort-By）" class="headerlink" title="4.2 每个Reduce内部排序（Sort By）"></a>4.2 每个Reduce内部排序（Sort By）</h3><p>Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。</p><p><strong>Sort by为每个reducer产生一个排序文件（即为分区内的排序）。每个Reducer内部进行排序，对全局结果集来说不是排序</strong>。</p><p>1）设置reduce个数</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce.job.reduces<span class="token operator">=</span>3<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）查看设置reduce个数</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce.job.reduces<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）根据部门编号降序查看员工信息</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp <span class="token function">sort</span> by deptno desc<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）将查询结果导入到文件中（按照部门编号降序排序）</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite local directory <span class="token string">'/opt/module/hive/datas/sortby-result'</span>
 <span class="token keyword">select</span> * from emp <span class="token function">sort</span> by deptno desc<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="4-3-分区（Distribute-By）"><a href="#4-3-分区（Distribute-By）" class="headerlink" title="4.3 分区（Distribute By）"></a>4.3 分区（Distribute By）</h3><p>Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。<strong>distribute by</strong> 子句可以做这件事。<strong>distribute by</strong>类似MR中partition（自定义分区），进行分区，结合sort by使用。</p><p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>1）案例实操：</p><p>（1）先按照部门编号分区，再按照员工编号降序排序。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce.job.reduces<span class="token operator">=</span>3<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite local directory <span class="token string">'/opt/module/hive/datas/distribute-result'</span> <span class="token keyword">select</span> * from emp distribute by deptno <span class="token function">sort</span> by empno desc<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意：</p><p>Ø distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。</p><p>Ø Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</p><h3 id="4-4-Cluster-By"><a href="#4-4-Cluster-By" class="headerlink" title="4.4 Cluster By"></a>4.4 Cluster By</h3><p>当distribute by和sort by字段相同时，可以使用cluster by方式。</p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p><p>（1）以下两种写法等价</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp cluster by deptno<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp distribute by deptno <span class="token function">sort</span> by deptno<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p><h1 id="5-函数使用"><a href="#5-函数使用" class="headerlink" title="5 函数使用"></a>5 函数使用</h1><h2 id="5-1-系统内置函数"><a href="#5-1-系统内置函数" class="headerlink" title="5.1 系统内置函数"></a>5.1 系统内置函数</h2><p>1）查看系统自带的函数</p><pre><code>hive&gt; show functions;</code></pre><p>2）显示自带的函数的用法</p><pre><code>hive&gt; desc function upper;</code></pre><p>3）详细显示自带的函数的用法</p><pre><code>hive&gt; desc function extended upper;</code></pre><h2 id="5-2-常用内置函数"><a href="#5-2-常用内置函数" class="headerlink" title="5.2 常用内置函数"></a>5.2 常用内置函数</h2><h3 id="5-2-1-空字段赋值"><a href="#5-2-1-空字段赋值" class="headerlink" title="5.2.1 空字段赋值"></a>5.2.1 空字段赋值</h3><p>1）函数说明</p><p>NVL：给值为NULL的数据赋值，它的格式是NVL( value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值，如果两个参数都为NULL ，则返回NULL。</p><p>2）数据准备：采用员工表</p><p>3）查询：如果员工的comm为NULL，则用-1代替</p><pre><code>hive (default)&gt; select comm,nvl(comm, -1) from emp;</code></pre><h3 id="5-2-2-CASE-WHEN-THEN-ELSE-END"><a href="#5-2-2-CASE-WHEN-THEN-ELSE-END" class="headerlink" title="5.2.2 CASE WHEN THEN ELSE END"></a>5.2.2 CASE WHEN THEN ELSE END</h3><p>1）数据准备</p><table><thead><tr><th>name</th><th>dept_id</th><th>sex</th></tr></thead><tbody><tr><td>悟空</td><td>A</td><td>男</td></tr><tr><td>大海</td><td>A</td><td>男</td></tr><tr><td>宋宋</td><td>B</td><td>男</td></tr><tr><td>凤姐</td><td>A</td><td>女</td></tr><tr><td>婷姐</td><td>B</td><td>女</td></tr><tr><td>婷婷</td><td>B</td><td>女</td></tr></tbody></table><p>2）需求</p><p>求出不同部门男女各多少人。结果如下：</p><p>dept_Id 男 女</p><p>A 2 1</p><p>B 1 2</p><p>3）按需求查询数据</p><p>select</p><pre><code> dept_id,
 sum(case sex when &#39;男&#39; then 1 else 0 end) male_count,
 sum(case sex when &#39;女&#39; then 1 else 0 end) female_count
from 
 emp_sex
group by
 dept_id;</code></pre><h3 id="5-2-3-行转列"><a href="#5-2-3-行转列" class="headerlink" title="5.2.3 行转列"></a>5.2.3 行转列</h3><p>1）相关函数说明</p><p><strong>CONCAT</strong>(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p><p><strong>CONCAT_WS</strong>(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p><p>注意: CONCAT_WS must be “string or array<string></string></p><p><strong>COLLECT_SET</strong>(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p><p><strong>COLLECT_list</strong>(col): 相比COLLECT_SET就是不去重的操作。</p><p>2）数据准备</p><table><thead><tr><th>name</th><th>constellation</th><th>blood_type</th></tr></thead><tbody><tr><td>孙悟空</td><td>白羊座</td><td>A</td></tr><tr><td>大海</td><td>射手座</td><td>A</td></tr><tr><td>宋宋</td><td>白羊座</td><td>B</td></tr><tr><td>猪八戒</td><td>白羊座</td><td>A</td></tr><tr><td>凤姐</td><td>射手座</td><td>A</td></tr><tr><td>苍老师</td><td>白羊座</td><td>B</td></tr></tbody></table><p>3）需求</p><p>把星座和血型一样的人归类到一起。结果如下：</p><pre><code>射手座,A      大海|凤姐
白羊座,A      孙悟空|猪八戒
白羊座,B       宋宋|苍老师</code></pre><p>4）创建本地constellation.txt，导入数据</p><pre><code>[molly@hadoop102 datas]$ vim person_info.txt
孙悟空  白羊座  A
大海 射手座  A
宋宋 白羊座  B
猪八戒  白羊座  A
凤姐 射手座  A
苍老师  白羊座  B</code></pre><p>5）创建hive表并导入数据</p><pre><code>create table person_info(
name string, 
constellation string, 
blood_type string) 
row format delimited fields terminated by &quot;\t&quot;;
load data local inpath &quot;/opt/module/hive/datas/person_info.txt&quot; into table person_info;</code></pre><p>6）按需求查询数据</p><pre><code>SELECT t1.c_b , CONCAT_WS(&quot;|&quot;,collect_set(t1.name))
FROM (
SELECT NAME ,CONCAT_WS(&#39;,&#39;,constellation,blood_type) c_b
FROM person_info
)t1 
GROUP BY t1.c_b</code></pre><h3 id="5-2-4-列转行"><a href="#5-2-4-列转行" class="headerlink" title="5.2.4 列转行"></a>5.2.4 列转行</h3><p>1）函数说明</p><p>**EXPLODE(col)**：将hive一列中复杂的array或者map结构拆分成多行。</p><p><strong>LATERAL VIEW</strong>（侧写表）</p><p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p><p>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p><p>2）数据准备</p><p>表6-7 数据准备</p><table><thead><tr><th>movie</th><th>category</th></tr></thead><tbody><tr><td>《疑犯追踪》</td><td>悬疑,动作,科幻,剧情</td></tr><tr><td>《Lie to me》</td><td>悬疑,警匪,动作,心理,剧情</td></tr><tr><td>《战狼2》</td><td>战争,动作,灾难</td></tr></tbody></table><p>3）需求</p><p>将电影分类中的数组数据展开。结果如下：</p><pre><code>《疑犯追踪》   悬疑
《疑犯追踪》   动作
《疑犯追踪》   科幻
《疑犯追踪》   剧情
《Lie to me》  悬疑
《Lie to me》  警匪
《Lie to me》  动作
《Lie to me》  心理
《Lie to me》  剧情
《战狼2》     战争
《战狼2》     动作
《战狼2》     灾难</code></pre><p>4）创建本地movie.txt，导入数据</p><pre><code>[molly@hadoop102 datas]$ vi movie_info.txt
《疑犯追踪》 悬疑,动作,科幻,剧情
《Lie to me》 悬疑,警匪,动作,心理,剧情
《战狼2》 战争,动作,灾难</code></pre><p>5）创建hive表并导入数据</p><pre><code>create table movie_info(
  movie string, 
  category string) 
row format delimited fields terminated by &quot;\t&quot;;
load data local inpath &quot;/opt/module/hive/datas/movie_info.txt&quot; into table movie_info;</code></pre><p>6）按需求查询数据</p><pre><code>SELECT movie,category_name 
FROM movie_info 
lateral VIEW
explode(split(category,&quot;,&quot;)) movie_info_tmp AS category_name ;</code></pre><h3 id="5-2-5-窗口函数（开窗函数）"><a href="#5-2-5-窗口函数（开窗函数）" class="headerlink" title="5.2.5 窗口函数（开窗函数）"></a>5.2.5 窗口函数（开窗函数）</h3><p>1）相关函数说明</p><p>**OVER()**：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的改变而变化。涉及关键字如下</p><table><thead><tr><th>CURRENT ROW</th><th>当前行</th></tr></thead><tbody><tr><td>n PRECEDING</td><td>往前n行数据</td></tr><tr><td>n FOLLOWING</td><td>往后n行数据</td></tr><tr><td>UNBOUNDED</td><td>起点</td></tr><tr><td>UNBOUNDED PRECEDING</td><td>表示从前面的起点</td></tr><tr><td>UNBOUNDED FOLLOWING</td><td>表示到后面的终点</td></tr><tr><td>LAG(col,n,default_val)</td><td>往前第n行数据</td></tr><tr><td>LEAD(col,n, default_val)</td><td>往后第n行数据</td></tr><tr><td>NTILE(n)</td><td>把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型</td></tr></tbody></table><p><strong>总结如下：</strong></p><p><strong>OVER()</strong> 默认为每条数据都开一个窗口，窗口大小是当前数据集的大小。</p><p><strong>OVER(partition by…. )</strong> 会按照指定字段进行分区，将分区字段的值仙童的数据划分到相同的区；每个区中的每条数据都会开启一个窗口，每条数据的窗口大小默认为当前分区数据的大小。</p><p><strong>OVER(order by …)</strong> 会在窗口中按照指定的字段对数据进行排序，会为每条数据都开启一个窗口，默认窗口大小为从数据集开始到当前行。</p><p>**OVER(partition by …order by..)**：会按照指定的字段进行分区，将分区字段的值仙童的数据划分到相同的区，在每个区会按照指定字段进行排序；每个区中的每条数据都会开启一个窗口，每条数据的窗口大小默认为当前分区中从数据集开始到当前行。相当于：partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row</p><p><strong>关键字总结</strong></p><table><thead><tr><th>Order by</th><th>全局排序；窗口函数中排序</th></tr></thead><tbody><tr><td>Distribute by</td><td>分区</td></tr><tr><td>Sort by</td><td>区内排序</td></tr><tr><td>Cluster by</td><td>分区排序</td></tr><tr><td>Partition by</td><td>窗口函数中分区</td></tr><tr><td>Partitioned by</td><td>建表 指定分区字段</td></tr><tr><td>Clustered by</td><td>建表 指定分桶字段</td></tr></tbody></table><p><strong>注意partition by …order by组合；Distribute by和Sort by 组合使用</strong></p><p>2）数据准备：name，orderdate，cost</p><pre class="line-numbers language-sh"><code class="language-sh">jack,2017-01-01,10
tony,2017-01-02,15
jack,2017-02-03,23
tony,2017-01-04,29
jack,2017-01-05,46
jack,2017-04-06,42
tony,2017-01-07,50
jack,2017-01-08,55
mart,2017-04-08,62
mart,2017-04-09,68
neil,2017-05-10,12
mart,2017-04-11,75
neil,2017-06-12,80
mart,2017-04-13,94<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）需求</p><p>（1）查询在2017年4月份购买过的顾客及总人数</p><p>（2）查询顾客的购买明细及月购买总额</p><p>（3）上述的场景, 将每个顾客的cost按照日期进行累加</p><p>（4）查询每个顾客上次的购买时间</p><p>（5）查询前20%时间的订单信息</p><p>4）创建本地business.txt，导入数据</p><pre><code>[molly@hadoop102 datas]$ vi business.txt</code></pre><p>5）创建hive表并导入数据</p><pre><code>create table business(
name string, 
orderdate string,
cost int
) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;;
load data local inpath &quot;/opt/module/hive/datas/business.txt&quot; into table business;</code></pre><p>6）按需求查询数据</p><p>（1） 查询在2017年4月份购买过的顾客及总人数</p><pre><code>select name,count(*) over () 
from business 
where substring(orderdate,1,7) = &#39;2017-04&#39; 
group by name;</code></pre><p>（2） 查询顾客的购买明细及所有顾客的月购买总额</p><p>使用分区：按照每个月做分区</p><pre><code>sum(cost) over(partition by month(orderdate))：表示对某个月的所有顾客的购买总额求sum；当前over的窗口大小是分区大小。
select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from
business;</code></pre><p>（3） 将每个顾客的cost按照日期进行累加</p><pre><code>select name,orderdate,cost, 
sum(cost) over() as sample1,--所有行相加 
sum(cost) over(partition by name) as sample2,--按name分组，组内数据相加 
sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加 </code></pre><p>rows必须跟在Order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量</p><p>sample3的执行结果如下：</p><p>​ <img src="/2020/11/15/bigdata-hive2/1637036085261.png" alt="1637036085261"></p><p><strong>拓展：数据窗口大小变化</strong></p><pre><code>sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到当前行的聚合 
sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合 
sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行 
sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行 
from business;</code></pre><p>（4） 查看顾客上次的购买时间和下一次购买时间：使用lag和lead函数</p><pre><code>select name,orderdate,cost, 
lag(orderdate,1,&#39;1900-01-01&#39;) over(partition by name order by orderdate ) as p_ orderdate, lead(orderdate,1,&#39;9999-01-01&#39;) over (partition by name order by orderdate) as p_ orderdate
from business;</code></pre><p>​ <img src="/2020/11/15/bigdata-hive2/1637036128510.png" alt="1637036128510"></p><p>（5） 查询前20%时间的订单信息，使用NTILE函数进行分组。按照时间排序分成5组，取第一个组，即前20%</p><pre><code>select * from (
  select name,orderdate,cost, ntile(5) over(order by orderdate) gid
   from business
) t
where t. gid = 1;</code></pre><p>​ <img src="/2020/11/15/bigdata-hive2/1637036162813.png" alt="1637036162813"></p><h3 id="5-2-6-Rank"><a href="#5-2-6-Rank" class="headerlink" title="5.2.6 Rank"></a>5.2.6 Rank</h3><p>1）函数说明</p><p><strong>RANK() 排序相同时会重复，总数不会变</strong></p><p><strong>DENSE_RANK() 排序相同时会重复，总数会减少</strong></p><p><strong>ROW_NUMBER() 会根据顺序计算</strong></p><p>2）数据准备</p><table><thead><tr><th>name</th><th>subject</th><th>score</th></tr></thead><tbody><tr><td>孙悟空</td><td>语文</td><td>87</td></tr><tr><td>孙悟空</td><td>数学</td><td>95</td></tr><tr><td>孙悟空</td><td>英语</td><td>68</td></tr><tr><td>大海</td><td>语文</td><td>94</td></tr><tr><td>大海</td><td>数学</td><td>56</td></tr><tr><td>大海</td><td>英语</td><td>84</td></tr><tr><td>宋宋</td><td>语文</td><td>64</td></tr><tr><td>宋宋</td><td>数学</td><td>86</td></tr><tr><td>宋宋</td><td>英语</td><td>84</td></tr><tr><td>婷婷</td><td>语文</td><td>65</td></tr><tr><td>婷婷</td><td>数学</td><td>85</td></tr><tr><td>婷婷</td><td>英语</td><td>78</td></tr></tbody></table><p>3）需求</p><p>计算每门学科成绩排名。</p><p>4）创建本地score.txt，导入数据</p><pre><code>[molly@hadoop102 datas]$ vi score.txt</code></pre><p>5）创建hive表并导入数据</p><pre><code>create table score(
name string,
subject string, 
score int) 
row format delimited fields terminated by &quot;\t&quot;;
load data local inpath &#39;/opt/module/hive/datas/score.txt&#39; into table score;</code></pre><p>6）按需求查询数据</p><pre><code>select name,subject,score,rank() over(partition by subject order by score desc) rp,
dense_rank() over(partition by subject order by score desc) drp,
row_number() over(partition by subject order by score desc) rmp
from score;</code></pre><p>查询结果如下面所示：</p><pre><code>name  subject score  rp   drp   rmp
孙悟空 数学  95   1    1    1
宋宋  数学  86   2    2    2
婷婷  数学  85   3    3    3
大海  数学  56   4    4    4
宋宋  英语  84   1    1    1
大海  英语  84   1    1    2
婷婷  英语  78   3    2    3
孙悟空 英语  68   4    3    4
大海  语文  94   1    1    1
孙悟空 语文  87   2    2    2
婷婷  语文  65   3    3    3
宋宋  语文  64   4    4    4</code></pre><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li></ul></div><span class="post-count">总字数7.3k</span> <span class="post-count">预计阅读31分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-kafka1-setup" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/15/bigdata-kafka1-setup/" class="article-date"><time class="published" datetime="2020-11-14T22:46:51.000Z" itemprop="datePublished">2020-11-15 发布</time> <time class="updated" datetime="2021-11-19T06:15:42.011Z" itemprop="dateUpdated">2021-11-19 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/15/bigdata-kafka1-setup/">kafka学习笔记（一） kafka搭建</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="1-Kafka概述"><a href="#1-Kafka概述" class="headerlink" title="1 Kafka概述"></a>1 Kafka概述</h1><h2 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h2><p>Kafka是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。</p><h2 id="1-2-消息队列"><a href="#1-2-消息队列" class="headerlink" title="1.2 消息队列"></a>1.2 消息队列</h2><h3 id="1-2-1-传统消息队列的应用场景"><a href="#1-2-1-传统消息队列的应用场景" class="headerlink" title="1.2.1 传统消息队列的应用场景"></a>1.2.1 传统消息队列的应用场景</h3><p><img src="/2020/11/15/bigdata-kafka1-setup/1637153923628.png" alt="1637153923628"></p><p>使用消息队列的好处</p><p>1）解耦</p><p>允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p><p>2）可恢复性</p><p>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</p><p>3）缓冲</p><p>有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</p><p>4）灵活性 &amp; 峰值处理能力</p><p>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</p><p>5）异步通信</p><p>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</p><h3 id="1-2-2-消息队列的两种模式"><a href="#1-2-2-消息队列的两种模式" class="headerlink" title="1.2.2 消息队列的两种模式"></a>1.2.2 消息队列的两种模式</h3><p>（1）点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）</p><p>消息生产者生产消息发送到Queue中，然后消息消费者从Queue中<strong>取出</strong>并且消费消息。（<strong>这里注意是消费者主动拉取的</strong>）</p><p>消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。</p><p><img src="/2020/11/15/bigdata-kafka1-setup/1637153951339.png" alt="1637153951339"></p><p>（2）发布/订阅模式（一对多，消费者消费数据之后不会清除消息）</p><p>消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）<strong>消费</strong>该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。（<strong>这里注意数据也是消费者拉取的，因为消费者会一直轮询topic是否有消息</strong>）</p><p><img src="/2020/11/15/bigdata-kafka1-setup/1637153963262.png" alt="1637153963262"></p><h2 id="1-3-Kafka基础架构"><a href="#1-3-Kafka基础架构" class="headerlink" title="1.3 Kafka基础架构"></a>1.3 Kafka基础架构</h2><p><img src="/2020/11/15/bigdata-kafka1-setup/1637154054233.png" alt="1637154054233"></p><p>1）Producer ：消息生产者，就是向kafka broker发消息的客户端；</p><p>2）Consumer ：消息消费者，向kafka broker取消息的客户端；</p><p>3）Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</p><p>4）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</p><p>5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；</p><p>6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；</p><p>7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。</p><p>8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。</p><p>9）follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。</p><p>10）kafka集群依赖于zookeeper管理。</p><h1 id="2-Kafka安装部署"><a href="#2-Kafka安装部署" class="headerlink" title="2 Kafka安装部署"></a>2 Kafka安装部署</h1><h2 id="2-1-集群规划"><a href="#2-1-集群规划" class="headerlink" title="2.1 集群规划"></a>2.1 集群规划</h2><table><thead><tr><th>hadoop102</th><th>hadoop103</th><th>hadoop104</th></tr></thead><tbody><tr><td>zk</td><td>zk</td><td>zk</td></tr><tr><td>kafka</td><td>kafka</td><td>kafka</td></tr></tbody></table><h2 id="2-2-Kafka-下载"><a href="#2-2-Kafka-下载" class="headerlink" title="2.2 Kafka 下载"></a>2.2 Kafka 下载</h2><p><a target="_blank" rel="noopener" href="http://kafka.apache.org/downloads.html">http://kafka.apache.org/downloads.html</a></p><h2 id="2-3-集群部署"><a href="#2-3-集群部署" class="headerlink" title="2.3 集群部署"></a>2.3 集群部署</h2><p>1）解压安装包</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）修改解压后的文件名称</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ <span class="token function">mv</span> kafka_2.11-2.4.1.tgz kafka<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）在/opt/module/kafka目录下创建logs文件夹</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> logs<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）修改配置文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ <span class="token function">cd</span> config/
<span class="token punctuation">[</span>molly@hadoop102 config<span class="token punctuation">]</span>$ <span class="token function">vi</span> server.properties<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>输入以下内容：</p><pre class="line-numbers language-sh"><code class="language-sh">#broker的全局唯一编号，不能重复
broker.id=0
#删除topic功能使能,当前版本此配置默认为true，已从配置文件移除
delete.topic.enable=true
#处理网络请求的线程数量
num.network.threads=3
#用来处理磁盘IO的线程数量
num.io.threads=8
#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400
#接收套接字的缓冲区大小
socket.receive.buffer.bytes=102400
#请求套接字的缓冲区大小
socket.request.max.bytes=104857600
#kafka运行日志存放的路径
log.dirs=/opt/module/kafka/logs
#topic在当前broker上的分区个数
num.partitions=1
#用来恢复和清理data下数据的线程数量
num.recovery.threads.per.data.dir=1
#segment文件保留的最长时间，超时将被删除
log.retention.hours=168
#配置连接Zookeeper集群地址
zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5）配置环境变量</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ <span class="token function">sudo</span> vim /etc/profile.d/my_env.sh
<span class="token comment" spellcheck="true">#KAFKA_HOME</span>
<span class="token function">export</span> KAFKA_HOME<span class="token operator">=</span>/opt/module/kafka
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$KAFKA_HOME</span>/bin
<span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ <span class="token function">source</span> /etc/profile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>6）分发安装包</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ xsync kafka/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​ 注意：分发之后记得配置其他机器的环境变量</p><p>7）分别在hadoop103和hadoop104上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2</p><p>​ 注：broker.id不得重复</p><p>8）启动集群</p><p>​ 先启动Zookeeper集群，然后启动kafaka</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102  kafka<span class="token punctuation">]</span>$ zk.sh start <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>依次在hadoop102、hadoop103、hadoop104节点上启动kafka</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-server-start.sh -daemon config/server.properties

<span class="token punctuation">[</span>molly@hadoop103 kafka<span class="token punctuation">]</span>$ bin/kafka-server-start.sh -daemon  config/server.properties

<span class="token punctuation">[</span>molly@hadoop104 kafka<span class="token punctuation">]</span>$ bin/kafka-server-start.sh -daemon  config/server.properties<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>9）关闭集群</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-server-stop.sh stop
<span class="token punctuation">[</span>molly@hadoop103 kafka<span class="token punctuation">]</span>$ bin/kafka-server-stop.sh stop
<span class="token punctuation">[</span>molly@hadoop104 kafka<span class="token punctuation">]</span>$ bin/kafka-server-stop.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>10）kafka群起脚本</p><pre class="line-numbers language-sh"><code class="language-sh">#!/bin/bash
if [ $# -lt 1 ]
then 
  echo "Input Args Error....."
  exit
fi
for i in hadoop102 hadoop103 hadoop104
do

case $1 in
start)
  echo "==================START $i KAFKA==================="
  ssh $i /opt/module/kafka_2.11-2.4.1/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.11-2.4.1/config/server.properties
;;
stop)
  echo "==================STOP $i KAFKA==================="
  ssh $i /opt/module/kafka_2.11-2.4.1/bin/kafka-server-stop.sh stop
;;

*)
 echo "Input Args Error....."
 exit
;;  
esac
done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-Kafka命令行操作"><a href="#3-Kafka命令行操作" class="headerlink" title="3 Kafka命令行操作"></a>3 Kafka命令行操作</h1><p>kafka提供了测试脚本kafka-topics.sh用来测试。</p><p>1）查看当前服务器中的所有topic</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --list<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）创建topic</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>选项说明：</p><p>–topic 定义topic名<br>–replication-factor 定义副本数<br>–partitions 定义分区数</p><p>3）删除topic</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）发送消息：生产消息– <strong>9092是kafka默认端口</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first
<span class="token operator">></span>hello world
<span class="token operator">></span>molly molly<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>5）消费消息</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-console-consumer.sh \
--bootstrap-server hadoop102:9092 --topic first

<span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-console-consumer.sh \
--bootstrap-server hadoop102:9092 --from-beginning --topic first<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>–from-beginning：会把主题中现有的所有的数据都读取出来</strong>。</p><p>6）查看某个Topic的详情</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe –-topic first<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>7）修改分区数 alter只能修改</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --alter –-topic first --partitions 6<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="4-Kafka监控"><a href="#4-Kafka监控" class="headerlink" title="4 Kafka监控"></a>4 Kafka监控</h1><p>我们知道一个叫kafka manager的kafka管理工具，这个工具管理kafka确实很强大，但是没有安全认证，随便都可以创建，删除，修改topic，而且告警系统，流量波动做的不好。所以，在这里浪尖，再给大家推荐一款kafka 的告警监控管理工具，kafka-eagle。Kafka Eagle是一款开源的Kafka集群监控系统。能够实现broker级常见的JMX监控；能对consumer消费进度进行监控；能在页面上直接对多个集群进行管理；安装方式简单，二进制包解压即用；可以配置告警（钉钉、微信、email均可）。</p><p>kafka-eagle主要是有几个我们关注 但kafkamanager不存在的点，值得一提：</p><ul><li>流量，最长可以查看最近七天的流量波动图</li><li>lag size邮件告警</li><li>可以用kafkasql分析</li></ul><p>相关官方地址：</p><ul><li>源码： <a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://github.com/smartloli/kafka-eagle/">https://github.com/smartloli/kafka-eagle/</a></li><li>官网：<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://www.kafka-eagle.org/">https://www.kafka-eagle.org/</a></li><li>下载： <a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=http://download.kafka-eagle.org/">http://download.kafka-eagle.org/</a></li><li>安装文档： <a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://docs.kafka-eagle.org/2.env-and-install">https://docs.kafka-eagle.org/2.env-and-install</a></li></ul><p><strong>1）修改kafka启动命令</strong></p><p>修改kafka-server-start.sh命令中</p><pre class="line-numbers language-sh"><code class="language-sh">if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
    export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"
fi<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>为</p><pre class="line-numbers language-sh"><code class="language-sh">if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
    export KAFKA_HEAP_OPTS="-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70"
    export JMX_PORT="9999"
    #export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"
fi<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意：修改之后在启动Kafka之前要分发之其他节点</p><p><strong>2）上传压缩包kafka-eagle-bin-1.4.5.tar.gz到集群/opt/software目录</strong></p><p><strong>3）解压到本地</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf kafka-eagle-bin-1.4.5.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>4）进入刚才解压的目录</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka-eagle-bin-1.4.5<span class="token punctuation">]</span>$ ll
总用量 82932
-rw-rw-r--. 1 molly molly 84920710 8月 13 23:00 kafka-eagle-web-1.4.5-bin.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>5）将kafka-eagle-web-1.3.7-bin.tar.gz解压至/opt/module</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka-eagle-bin-1.4.5<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf kafka-eagle-web-1.4.5-bin.tar.gz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>6）修改名称</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ <span class="token function">mv</span> kafka-eagle-web-1.4.5/ eagle<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>7）给启动文件执行权限</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 eagle<span class="token punctuation">]</span>$ <span class="token function">cd</span> bin/
<span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ ll

总用量 12

-rw-r--r--. 1 molly molly 1848 8月 22 2017 ke.bat

-rw-r--r--. 1 molly molly 7190 7月 30 20:12 ke.sh

<span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> 777 ke.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>8）修改配置文件 conf/system-config.properties</strong></p><pre class="line-numbers language-sh"><code class="language-sh">######################################
# multi zookeeper&kafka cluster list
######################################
kafka.eagle.zk.cluster.alias=cluster1
cluster1.zk.list=hadoop102:2181,hadoop103:2181,hadoop104:2181

######################################
# kafka offset storage
######################################
cluster1.kafka.eagle.offset.storage=kafka

######################################
# enable kafka metrics
######################################
kafka.eagle.metrics.charts=true
kafka.eagle.sql.fix.error=false

######################################
# kafka jdbc driver address
######################################
kafka.eagle.driver=com.mysql.jdbc.Driver
kafka.eagle.url=jdbc:mysql://hadoop102:3306/ke?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull
kafka.eagle.username=root
kafka.eagle.password=123456<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>9）添加环境变量</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">export</span> KE_HOME<span class="token operator">=</span>/opt/module/eagle
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$KE_HOME</span>/bin
<span class="token comment" spellcheck="true">#注意：source /etc/profile</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>10）启动</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>atguigu@hadoop102 eagle<span class="token punctuation">]</span>$ bin/ke.sh start
<span class="token punctuation">..</span>. <span class="token punctuation">..</span>.
<span class="token punctuation">..</span>. <span class="token punctuation">..</span>.
*******************************************************************
* Kafka Eagle Service has started success.
* Welcome, Now you can visit <span class="token string">'http://192.168.202.102:8048/ke'</span>
* Account:admin ,Password:123456
*******************************************************************
* <span class="token operator">&lt;</span>Usage<span class="token operator">></span> ke.sh <span class="token punctuation">[</span>start<span class="token operator">|</span>status<span class="token operator">|</span>stop<span class="token operator">|</span>restart<span class="token operator">|</span>stats<span class="token punctuation">]</span> <span class="token operator">&lt;</span>/Usage<span class="token operator">></span>
* <span class="token operator">&lt;</span>Usage<span class="token operator">></span> https://www.kafka-eagle.org/ <span class="token operator">&lt;</span>/Usage<span class="token operator">></span>
*******************************************************************<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>注意：启动之前需要先启动ZK以及KAFKA</strong></p><p><strong>11）登录页面查看监控数据</strong></p><p><a target="_blank" rel="noopener" href="http://192.168.202.102:8048/ke">http://192.168.202.102:8048/ke</a></p><p><img src="/2020/11/15/bigdata-kafka1-setup/1637153214475.png" alt="1637153214475"></p><p>​</p><h1 id="5-Flume对接Kafka"><a href="#5-Flume对接Kafka" class="headerlink" title="5 Flume对接Kafka"></a>5 Flume对接Kafka</h1><h2 id="5-1-简单实现"><a href="#5-1-简单实现" class="headerlink" title="5.1 简单实现"></a>5.1 简单实现</h2><p><strong>1）配置flume</strong></p><pre class="line-numbers language-sh"><code class="language-sh"># define
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F  /opt/module/data/flume.log

# sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sinks.k1.kafka.topic = first
a1.sinks.k1.kafka.flumeBatchSize = 20
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1

# channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# bind
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2） 启动kafka消费者</p><p>3） 进入flume根目录下，启动flume</p><pre class="line-numbers language-bash"><code class="language-bash">$ bin/flume-ng agent -c conf/ -n a1 -f jobs/flume-kafka.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4） 向 /opt/module/data/flume.log里追加数据，查看kafka消费者消费情况</p><pre class="line-numbers language-bash"><code class="language-bash">$ <span class="token keyword">echo</span> hello <span class="token operator">>></span> /opt/module/data/flume.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="5-2-数据分离"><a href="#5-2-数据分离" class="headerlink" title="5.2 数据分离"></a>5.2 数据分离</h2><p>0)需求</p><p>将flume采集的数据按照不同的类型输入到不同的topic中</p><p>​ 将日志数据中带有molly的，输入到Kafka的first主题中，</p><p>​ 将日志数据中带有shangguigu的,输入到Kafka的second主题中，</p><p>​ 其他的数据输入到Kafka的third主题中</p><p>1） 编写Flume的Interceptor</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>atguigu<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>flumeInterceptor<span class="token punctuation">;</span>

<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>Context<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>Event<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>interceptor<span class="token punctuation">.</span>Interceptor<span class="token punctuation">;</span>

<span class="token keyword">import</span> javax<span class="token punctuation">.</span>swing<span class="token punctuation">.</span>text<span class="token punctuation">.</span>html<span class="token punctuation">.</span>HTMLEditorKit<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>List<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Map<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">FlumeKafkaInterceptor</span> <span class="token keyword">implements</span> <span class="token class-name">Interceptor</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">initialize</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">/**
     * 如果包含"atguigu"的数据，发送到first主题
     * 如果包含"sgg"的数据，发送到second主题
     * 其他的数据发送到third主题
     * @param event
     * @return
     */</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> Event <span class="token function">intercept</span><span class="token punctuation">(</span>Event event<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">//1.获取event的header</span>
        Map<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> String<span class="token operator">></span> headers <span class="token operator">=</span> event<span class="token punctuation">.</span><span class="token function">getHeaders</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">//2.获取event的body</span>
        String body <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">String</span><span class="token punctuation">(</span>event<span class="token punctuation">.</span><span class="token function">getBody</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">if</span><span class="token punctuation">(</span>body<span class="token punctuation">.</span><span class="token function">contains</span><span class="token punctuation">(</span><span class="token string">"atguigu"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            headers<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">"topic"</span><span class="token punctuation">,</span><span class="token string">"first"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token keyword">else</span> <span class="token keyword">if</span><span class="token punctuation">(</span>body<span class="token punctuation">.</span><span class="token function">contains</span><span class="token punctuation">(</span><span class="token string">"sgg"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            headers<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">"topic"</span><span class="token punctuation">,</span><span class="token string">"second"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> event<span class="token punctuation">;</span>

    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> List<span class="token operator">&lt;</span>Event<span class="token operator">></span> <span class="token function">intercept</span><span class="token punctuation">(</span>List<span class="token operator">&lt;</span>Event<span class="token operator">></span> events<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>Event event <span class="token operator">:</span> events<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
          <span class="token function">intercept</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> events<span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">MyBuilder</span> <span class="token keyword">implements</span>  <span class="token class-name">Builder</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> Interceptor <span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span>  <span class="token keyword">new</span> <span class="token class-name">FlumeKafkaInterceptor</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">configure</span><span class="token punctuation">(</span>Context context<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）将写好的interceptor打包上传到Flume安装目录的lib目录下</p><p>3）配置flume</p><pre class="line-numbers language-sh"><code class="language-sh"># Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 6666


# Describe the sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.topic = third
a1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sinks.k1.kafka.flumeBatchSize = 20
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1

#Interceptor
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.atguigu.kafka.flumeInterceptor.FlumeKafkaInterceptor$MyBuilder

# # Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4） 启动kafka消费者</p><p>5） 进入flume根目录下，启动flume</p><pre class="line-numbers language-bash"><code class="language-bash">$ bin/flume-ng agent -c conf/ -n a1 -f jobs/flume-kafka.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>6） 向6666端口写数据，查看kafka消费者消费情况</p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li></ul></div><span class="post-count">总字数3.4k</span> <span class="post-count">预计阅读15分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-hive1" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/14/bigdata-hive1/" class="article-date"><time class="published" datetime="2020-11-14T07:08:50.000Z" itemprop="datePublished">2020-11-14 发布</time> <time class="updated" datetime="2021-11-15T07:46:28.407Z" itemprop="dateUpdated">2021-11-15 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/14/bigdata-hive1/">Hive学习笔记（一） Hive安装</a></h1></header><div class="article-entry" itemprop="articleBody"><p>前面学习到利用mapreduce去计算，但是mapreduce写起来麻烦，并且代码重复度高，可以进行封装，所以就出来了Hive，hive工具通过执行类SQL来启动写好的mapreduce,进一步执行hdfs中的资源。</p><h1 id="1-Hive是什么"><a href="#1-Hive是什么" class="headerlink" title="1 Hive是什么"></a>1 Hive是什么</h1><h2 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h2><p>1） hive简介</p><p>Hive：由Facebook开源用于解决海量结构化日志的数据统计工具。</p><p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。</p><p>2） Hive本质：将HQL转化成MapReduce程序如下图所示。需要注意的是以下三点：</p><p>（1）Hive处理的数据存储在HDFS</p><p>（2）Hive分析数据底层的实现是MapReduce</p><p>（3）执行程序运行在Yarn上</p><p><img src="/2020/11/14/bigdata-hive1/1636945549117.png" alt="1636945549117"></p><h2 id="1-2-Hive架构"><a href="#1-2-Hive架构" class="headerlink" title="1.2 Hive架构"></a>1.2 Hive架构</h2><p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。具体架构图如下所示。</p><p><img src="/2020/11/14/bigdata-hive1/1636945658710.png" alt="1636945658710"></p><p>1）用户接口：Client</p><p>CLI（command-line interface）、JDBC/ODBC(jdbc访问hive)、WEBUI（浏览器访问hive）</p><p>2）元数据：Metastore</p><p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</p><p><strong>默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore</strong></p><p>3）Hadoop</p><p><strong>使用HDFS进行存储，使用MapReduce进行计算。</strong></p><p>4）驱动器：Driver</p><p>（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p><p>（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。</p><p>（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。</p><p>（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</p><h1 id="2-Hive-安装"><a href="#2-Hive-安装" class="headerlink" title="2 Hive 安装"></a>2 Hive 安装</h1><h2 id="2-1-Hive安装地址"><a href="#2-1-Hive安装地址" class="headerlink" title="2.1 Hive安装地址"></a>2.1 Hive安装地址</h2><p>1）Hive官网地址</p><p><a target="_blank" rel="noopener" href="http://hive.apache.org/">http://hive.apache.org/</a></p><p>2）文档查看地址</p><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p><p>3）下载地址</p><p><a target="_blank" rel="noopener" href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></p><p>4）github地址</p><p><a target="_blank" rel="noopener" href="https://github.com/apache/hive">https://github.com/apache/hive</a></p><h2 id="2-2-MySql安装"><a href="#2-2-MySql安装" class="headerlink" title="2.2 MySql安装"></a>2.2 MySql安装</h2><p>0）为什么需要Mysql</p><p>原因在于Hive默认使用的元数据库为derby，开启Hive之后就会占用元数据库，且不与其他客户端共享数据，如果想多窗口操作就会报错，操作比较局限。以我们需要将Hive的元数据地址改为MySQL，可支持多窗口操作。</p><p>1）检查当前系统是否安装过Mysql</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ rpm -qa<span class="token operator">|</span><span class="token function">grep</span> -I -E mysql\<span class="token operator">|</span>mariadb
mariadb-libs-5.5.56-2.el7.x86_64 //如果存在通过如下命令卸载
<span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> rpm -e --nodeps mariadb-libs  //用此命令卸载mariadb
<span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ rpm -qa<span class="token operator">|</span><span class="token function">grep</span> -I -E mysql\<span class="token operator">|</span>mariadb <span class="token operator">|</span> <span class="token function">xargs</span> -n1 <span class="token function">sudo</span> rpm -e --nodeps<span class="token comment" spellcheck="true">#卸载所有</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>2）将MySQL安装包拷贝到/opt/software目录下</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># ll</span>
总用量 528384
-rw-r--r--. 1 root root 609556480 3月 21 15:41 mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>3）解压MySQL安装包</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># tar -xf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）在安装目录下执行rpm安装</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">sudo</span> rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm
<span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">sudo</span> rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm
<span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">sudo</span> rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm
<span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">sudo</span> rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm
<span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">sudo</span> rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意:按照顺序依次执行</p><p>如果Linux是最小化安装的，在安装mysql-community-server-5.7.28-1.el7.x86_64.rpm时可能会出 现如下错误</p><p>[molly@hadoop102 software]$ sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm</p><p>警告：mysql-community-server-5.7.28-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY</p><p>错误：依赖检测失败：</p><p>​ libaio.so.1()(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要</p><p>​ libaio.so.1(LIBAIO_0.1)(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要</p><p>​ libaio.so.1(LIBAIO_0.4)(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要</p><p>通过yum安装缺少的依赖,然后重新安装mysql-community-server-5.7.28-1.el7.x86_64 即可</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span> yum <span class="token function">install</span> -y libaio<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）删除/etc/my.cnf文件中datadir指向的目录下的所有内容,如果有内容的情况下:</p><p>查看datadir的值：</p><p>[mysqld]</p><p>datadir=/var/lib/mysql</p><p>删除/var/lib/mysql目录下的所有内容:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mysql<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># cd /var/lib/mysql</span>
<span class="token punctuation">[</span>molly@hadoop102 mysql<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># sudo rm -rf ./*  //注意执行命令的位置</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>6）初始化数据库</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 opt<span class="token punctuation">]</span>$ <span class="token function">sudo</span> mysqld --initialize --user<span class="token operator">=</span>mysql<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>7）查看临时生成的root用户的密码</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 opt<span class="token punctuation">]</span>$ <span class="token function">cat</span> /var/log/mysqld.log <span class="token operator">|</span><span class="token function">grep</span> password<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>8）启动MySQL服务</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 opt<span class="token punctuation">]</span>$ <span class="token function">sudo</span> systemctl start mysqld<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>9）登录MySQL数据库</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 opt<span class="token punctuation">]</span>$ mysql -uroot -p
Enter password:  输入临时生成的密码<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>登录成功.</p><p>10）必须先修改root用户的密码,否则执行其他的操作会报错</p><pre class="line-numbers language-bash"><code class="language-bash">mysql<span class="token operator">></span> <span class="token keyword">set</span> password <span class="token operator">=</span> password<span class="token punctuation">(</span><span class="token string">"000000"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>11）修改mysql库下的user表中的root用户允许任意ip连接</p><pre class="line-numbers language-bash"><code class="language-bash">mysql<span class="token operator">></span> update mysql.user <span class="token keyword">set</span> host<span class="token operator">=</span><span class="token string">'%'</span> where user<span class="token operator">=</span><span class="token string">'root'</span><span class="token punctuation">;</span>
mysql<span class="token operator">></span> flush privileges<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="2-3-Hive安装部署"><a href="#2-3-Hive安装部署" class="headerlink" title="2.3 Hive安装部署"></a>2.3 Hive安装部署</h2><p>1）把apache-hive-3.1.2-bin.tar.gz上传到linux的/opt/software目录下</p><p>2）解压apache-hive-3.1.2-bin.tar.gz到/opt/module/目录下面</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）修改apache-hive-3.1.2-bin.tar.gz的名称为hive</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">mv</span> /opt/module/apache-hive-3.1.2-bin/ /opt/module/hive-3.1.2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）修改/etc/profile.d/my_env.sh，添加环境变量</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">sudo</span> vim /etc/profile.d/my_env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）添加内容</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#HIVE_HOME</span>
HIVE_HOME<span class="token operator">=</span>/opt/module/hive-3.1.2
PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$JAVA_HOME</span>/bin:<span class="token variable">$HADOOP_HOME</span>/bin:<span class="token variable">$HADOOP_HOME</span>/sbin:<span class="token variable">$HIVE_HOME</span>/bin
<span class="token function">export</span> PATH JAVA_HOME HADOOP_HOME HIVE_HOME<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>6）解决日志Jar包冲突:Hive日志与Hadoop默认日志冲突，可以直接删除hive日志JAR</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">rm</span> -rf /opt/module/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-4-Hive元数据配置到MySql"><a href="#2-4-Hive元数据配置到MySql" class="headerlink" title="2.4 Hive元数据配置到MySql"></a>2.4 Hive元数据配置到MySql</h2><p>因为Hive默认使用的元数据库为derby，为了想多窗口操作，我们需要将Hive的元数据地址改为MySQL。下面安装好mysql后，进行将Hive元数据配置到mysql上。其中HIVE_HOME=/opt/module/hive-3.1.2</p><h3 id="2-4-1-拷贝驱动"><a href="#2-4-1-拷贝驱动" class="headerlink" title="2.4.1 拷贝驱动"></a>2.4.1 拷贝驱动</h3><p>将MySQL的JDBC驱动拷贝到Hive的lib目录下</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">cp</span> /opt/software/mysql-connector-java-5.1.48.jar <span class="token variable">$HIVE_HOME</span>/lib<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-4-2-配置Metastore到MySql"><a href="#2-4-2-配置Metastore到MySql" class="headerlink" title="2.4.2 配置Metastore到MySql"></a>2.4.2 配置Metastore到MySql</h3><p>在$HIVE_HOME/conf目录下新建hive-site.xml文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ vim <span class="token variable">$HIVE_HOME</span>/conf/hive-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加如下内容</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- jdbc连接的URL --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionURL<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>jdbc:mysql://hadoop102:3306/metastore?useSSL=false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- jdbc连接的Driver--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionDriverName<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.mysql.jdbc.Driver<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- jdbc连接的username--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionUserName<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>root<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- jdbc连接的password --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>javax.jdo.option.ConnectionPassword<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>123456<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- Hive默认在HDFS的工作目录 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.warehouse.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/user/hive/warehouse<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- Hive元数据存储的验证 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.schema.verification<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- 元数据存储授权 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.event.db.notification.api.auth<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-5-启动Hive"><a href="#2-5-启动Hive" class="headerlink" title="2.5 启动Hive"></a>2.5 启动Hive</h2><h3 id="2-5-1-初始化元数据库"><a href="#2-5-1-初始化元数据库" class="headerlink" title="2.5.1 初始化元数据库"></a>2.5.1 初始化元数据库</h3><p>1）登陆MySQL</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ mysql -uroot -p000000<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）新建Hive元数据库</p><pre class="line-numbers language-bash"><code class="language-bash">mysql<span class="token operator">></span> create database metastore<span class="token punctuation">;</span>
mysql<span class="token operator">></span> quit<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>3）初始化Hive元数据库</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ schematool -initSchema -dbType mysql -verbose<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-5-2-启动Hive"><a href="#2-5-2-启动Hive" class="headerlink" title="2.5.2 启动Hive"></a>2.5.2 启动Hive</h3><p>0）先启动hadoop集群</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ start-dfs.sh
<span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ start-yarn.sh
<span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ myjps.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>浏览器中查看hdfs：<a target="_blank" rel="noopener" href="http://hadoop102:9870/">http://Hadoop102:9870</a></p><p>浏览器中查看yarn ：<a target="_blank" rel="noopener" href="http://hadoop103:8088/">http://Hadoop103:8088</a></p><p>1）启动Hive</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/hive<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）使用Hive</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> show databases<span class="token punctuation">;</span>
hive<span class="token operator">></span> show tables<span class="token punctuation">;</span>
hive<span class="token operator">></span> create table <span class="token function">test</span> <span class="token punctuation">(</span>id int<span class="token punctuation">)</span><span class="token punctuation">;</span>
hive<span class="token operator">></span> insert into <span class="token function">test</span> values<span class="token punctuation">(</span>1<span class="token punctuation">)</span><span class="token punctuation">;</span>
hive<span class="token operator">></span> <span class="token keyword">select</span> * from <span class="token function">test</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-5-3-使用元数据服务的方式访问Hive"><a href="#2-5-3-使用元数据服务的方式访问Hive" class="headerlink" title="2.5.3 使用元数据服务的方式访问Hive"></a>2.5.3 使用元数据服务的方式访问Hive</h3><p>原始方法：Hive直接访问mysql</p><p><strong>使用元数据服务方式：Hive—》元数据服务，元数据服务—》访问mysql。</strong></p><p>1）在hive-site.xml文件中添加如下配置信息</p><pre class="line-numbers language-xml"><code class="language-xml">  <span class="token comment" spellcheck="true">&lt;!-- 指定存储元数据要连接的地址 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.metastore.uris<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>thrift://hadoop102:9083<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）启动metastore</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop202 hive<span class="token punctuation">]</span>$ hive --service metastore
2020-04-24 16:58:08: Starting Hive Metastore Server<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意: 启动后窗口不能再操作，需打开一个新的shell窗口做别的操作</p><p>3）启动 hive</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop202 hive<span class="token punctuation">]</span>$ bin/hive<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-5-4-使用JDBC方式访问Hive"><a href="#2-5-4-使用JDBC方式访问Hive" class="headerlink" title="2.5.4 使用JDBC方式访问Hive"></a>2.5.4 使用JDBC方式访问Hive</h3><p><strong>客户端是beeline（JDBC协议去访问）;</strong></p><p><strong>服务端：Hive使用hiveserver2提供JDBC协议：</strong></p><p><strong>所以访问数据流是：blleline通过hiveserver2去访问Hive。</strong></p><p>1）在hive-site.xml文件中添加如下配置信息</p><p>2）启动hiveserver2</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/hive --service hiveserver2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）启动beeline客户端（需要多等待一会）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/beeline -u jdbc:hive2://hadoop102:10000 -n molly<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）看到如下界面</p><pre class="line-numbers language-bash"><code class="language-bash">Connecting to jdbc:hive2://hadoop102:10000
Connected to: Apache Hive <span class="token punctuation">(</span>version 3.1.2<span class="token punctuation">)</span>
Driver: Hive JDBC <span class="token punctuation">(</span>version 3.1.2<span class="token punctuation">)</span>
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.2 by Apache Hive
0: jdbc:hive2://hadoop102:10000<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-常用命令"><a href="#3-常用命令" class="headerlink" title="3 常用命令"></a>3 常用命令</h1><h2 id="3-1-Hive常用交互命令"><a href="#3-1-Hive常用交互命令" class="headerlink" title="3.1 Hive常用交互命令"></a>3.1 Hive常用交互命令</h2><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/hive -help<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>1）“-e”不进入hive的交互窗口执行sql语句</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/hive -e <span class="token string">"select id from student;"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）“-f”执行脚本中sql语句</p><p>（1）在/opt/module/hive/下创建datas目录并在datas目录下创建hivef.sql文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 datas<span class="token punctuation">]</span>$ <span class="token function">touch</span> hivef.sql<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）文件中写入正确的sql语句</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token keyword">select</span> *from student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）执行文件中的sql语句</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/hive -f /opt/module/hive/datas/hivef.sql<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）执行文件中的sql语句并将结果写入文件中</p><pre><code>[molly@hadoop102 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt</code></pre><h2 id="3-2-Hive其他命令操作"><a href="#3-2-Hive其他命令操作" class="headerlink" title="3.2 Hive其他命令操作"></a>3.2 Hive其他命令操作</h2><p>1）退出hive窗口：</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>exit<span class="token punctuation">;</span>
hive<span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>quit<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在新版的hive中没区别了，在以前的版本是有的：</p><p>exit:先隐性提交数据，再退出；</p><p>quit:不提交数据，退出；</p><p>2）在hive cli命令窗口中如何查看hdfs文件系统</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>dfs -ls /<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）查看在hive中输入的所有历史命令</p><p>（1）进入到当前用户的根目录/root或/home/molly</p><p>（2）查看. hivehistory文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>atguig2u@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">cat</span> .hivehistory<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="3-3-Hive常见属性配置"><a href="#3-3-Hive常见属性配置" class="headerlink" title="3.3 Hive常见属性配置"></a>3.3 Hive常见属性配置</h2><h3 id="3-3-1-hive窗口打印默认库和表头"><a href="#3-3-1-hive窗口打印默认库和表头" class="headerlink" title="3.3.1 hive窗口打印默认库和表头"></a>3.3.1 hive窗口打印默认库和表头</h3><p>1）打印 当前库 和 表头</p><p>在hive-site.xml中加入如下两个配置:</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.cli.print.header<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hive.cli.print.current.db<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-3-2-Hive运行日志信息配置"><a href="#3-3-2-Hive运行日志信息配置" class="headerlink" title="3.3.2 Hive运行日志信息配置"></a>3.3.2 Hive运行日志信息配置</h3><p>1）Hive的log默认存放在/tmp/molly/hive.log目录下（当前用户名下）</p><p>2）修改hive的log存放日志到/opt/module/hive/logs</p><p>（1）修改/opt/module/hive/conf/hive-log4j2.properties.template文件名称为</p><p>hive-log4j2.properties</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ <span class="token function">pwd</span>
/opt/module/hive/conf
<span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ <span class="token function">mv</span> hive-log4j2.properties.template hive-log4j2.properties<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（2）在hive-log4j.properties文件中修改log存放位置</p><p>property.hive.log.dir=/opt/module/hive/logs</p><h3 id="3-3-3-参数配置方式"><a href="#3-3-3-参数配置方式" class="headerlink" title="3.3.3 参数配置方式"></a>3.3.3 参数配置方式</h3><p>1）查看当前所有的配置信息</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span>set<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）参数的配置三种方式</p><p>（1）配置文件方式</p><p>默认配置文件：hive-default.xml</p><p>用户自定义配置文件：hive-site.xml</p><p>注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p><p>（2）命令行参数方式</p><p>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。</p><p>例如：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop103 hive<span class="token punctuation">]</span>$ bin/hive -hiveconf mapred.reduce.tasks<span class="token operator">=</span>10<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：仅对本次hive启动有效</p><p>查看参数设置：</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapred.reduce.tasks<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）参数声明方式</p><p>可以在HQL中使用SET关键字设定参数</p><p>例如：</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapred.reduce.tasks<span class="token operator">=</span>100<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：仅对本次hive启动有效。</p><p>查看参数设置</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapred.reduce.tasks<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li></ul></div><span class="post-count">总字数3k</span> <span class="post-count">预计阅读13分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-hdfs1" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/12/bigdata-hdfs1/" class="article-date"><time class="published" datetime="2020-11-12T08:50:28.000Z" itemprop="datePublished">2020-11-12 发布</time> <time class="updated" datetime="2021-12-01T06:44:50.144Z" itemprop="dateUpdated">2021-12-01 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/12/bigdata-hdfs1/">Hadoop 教程（二）安装hadoop集群-完全分布式部署及API使用</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h2><p>本文来搭建hadoop集群，准备三台服务器，分别为hadoop102,hadoop103,hadoop104.其中hadoop 采用3.1.3版本，jdk 采用1.8.0_212 。</p><h2 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2 准备工作"></a>2 准备工作</h2><h3 id="2-1-映射"><a href="#2-1-映射" class="headerlink" title="2.1 映射"></a>2.1 映射</h3><p>为了方便直接通过主机名去访问，下面进行映射</p><p>1）修改克隆机主机名，以下以hadoop102举例说明</p><p>（1）修改主机名称，：修改/etc/hostname文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop100 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># vim /etc/hostname</span>
hadoop102<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（2）配置linux克隆机主机名称映射hosts文件，打开/etc/hosts</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop100 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># vim /etc/hosts</span>
192.168.1.102 hadoop102
192.168.1.103 hadoop103
192.168.1.104 hadoop104<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>2）重启hadoop102</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop100 ~<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># reboot</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）修改windows的主机映射文件（hosts文件）</p><p>操作系统是window10，先拷贝出来，修改保存以后，再覆盖即可</p><p>（a）进入C:\Windows\System32\drivers\etc路径</p><p>（b）拷贝hosts文件到桌面</p><p>（c）打开桌面hosts文件并添加如下内容</p><pre><code>192.168.1.102 hadoop102
192.168.1.103 hadoop103
192.168.1.104 hadoop104</code></pre><p>（d）将桌面hosts文件覆盖C:\Windows\System32\drivers\etc路径hosts文件</p><h3 id="2-2-安装JDK"><a href="#2-2-安装JDK" class="headerlink" title="2.2 安装JDK"></a>2.2 安装JDK</h3><p>1）在Linux系统下的opt目录中下载软件包</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">ls</span> /opt/software/
jdk-8u212-linux-x64.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）解压JDK到/opt/module目录下</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）配置JDK环境变量</p><p>​ （1）新建/etc/profile.d/my_env.sh文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> vim /etc/profile.d/my_env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加如下内容</p><p>#JAVA_HOME</p><p>export JAVA_HOME=/opt/module/jdk1.8.0_212</p><p>export PATH=$PATH:$JAVA_HOME/bin</p><p>​ （2）保存后退出:wq</p><p>​ （3）source一下/etc/profile文件，让新的环境变量PATH生效</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">source</span> /etc/profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）测试JDK是否安装成功</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ java -version<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如果能看到以下结果，则代表Java安装成功。</p><p>java version “1.8.0_212”</p><p>注意：重启（如果java -version可以用就不用重启）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">sudo</span> <span class="token function">reboot</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-3-SSH免密码登录"><a href="#2-3-SSH免密码登录" class="headerlink" title="2.3 SSH免密码登录"></a>2.3 SSH免密码登录</h3><p>免密登录原理如下图所示：</p><p><img src="/2020/11/12/bigdata-hdfs1/1636709256729.png" alt="1636709256729"></p><p>具体操作如下：</p><p>1）生成公钥和私钥：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 .ssh<span class="token punctuation">]</span>$ ssh-keygen -t rsa<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p><p>2）将公钥拷贝到要免密登录的目标机器上</p><p>[root@hadoop102 .ssh]$ ssh-copy-id hadoop102</p><p>[root@hadoop102 .ssh]$ ssh-copy-id hadoop103</p><p>[root@hadoop102 .ssh]$ ssh-copy-id hadoop104</p><p>这样hadoop102登录到hadoop103和hadoop104就不需要输入密码了。可以相互登录 还需要在hadoop103和hadoop104上做同样的操作。</p><h3 id="2-4-编写集群分发脚本"><a href="#2-4-编写集群分发脚本" class="headerlink" title="2.4 编写集群分发脚本"></a>2.4 编写集群分发脚本</h3><p>为了在集群中各个主机中文件拷贝方便，我们可以写个脚本用于三台主机中分发文件。</p><p>（1）需求：循环复制文件到所有节点的相同目录下</p><p>（2）需求分析：</p><p>（a）rsync命令原始拷贝：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">rsync</span> -av   /opt/module     root@hadoop103:/opt/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（b）期望脚本：</p><p>xsync要同步的文件名称</p><p>（c）说明：在/home/root/bin这个目录下存放的脚本，root用户可以在系统任何地方直接执行。</p><p>（3）脚本实现</p><p>（a）在/home/root/bin目录下创建xsync文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 opt<span class="token punctuation">]</span>$ <span class="token function">cd</span> /home/root
<span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> bin
<span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">cd</span> bin
<span class="token punctuation">[</span>root@hadoop102 bin<span class="token punctuation">]</span>$ vim xsync<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>在该文件中编写如下代码</p><pre class="line-numbers language-sh"><code class="language-sh">#!/bin/bash
\#1. 判断参数个数
if [ $# -lt 1 ]
then
 echo Not Enough Arguement!
 exit;
fi
\#2. 遍历集群所有机器
for host in hadoop102 hadoop103 hadoop104
do
 echo ==================== $host ====================
 \#3. 遍历所有目录，挨个发送
 for file in $@
 do
  \#4. 判断文件是否存在
  if [ -e $file ]
  then
   \#5. 获取父目录
   pdir=$(cd -P $(dirname $file); pwd)
   \#6. 获取当前文件的名称
   fname=$(basename $file)
   ssh $host "mkdir -p $pdir"
   rsync -av $pdir/$fname $host:$pdir
  else
   echo $file does not exists!
  fi
 done
done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（b）修改脚本 xsync 具有执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> +x xsync<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（c）将脚本复制到/bin中，以便全局调用</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">sudo</span> <span class="token function">cp</span> xsync /bin/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（d）测试脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ xsync /home/root/bin
<span class="token punctuation">[</span>root@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">sudo</span> xsync /bin/xsync<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="3-安装hadoop"><a href="#3-安装hadoop" class="headerlink" title="3 安装hadoop"></a>3 安装hadoop</h2><p>Hadoop下载地址：<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/">https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/</a></p><p>1）下载hadoop并进入到Hadoop安装包路径下</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">cd</span> /opt/software/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）解压安装文件到/opt/module下面</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf hadoop-3.1.3.tar.gz -C /opt/module/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）查看是否解压成功</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 software<span class="token punctuation">]</span>$ <span class="token function">ls</span> /opt/module/
hadoop-3.1.3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>4）将Hadoop添加到环境变量</p><p>​ （1）获取Hadoop安装路径</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">pwd</span>
/opt/module/hadoop-3.1.3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>​ （2）打开/etc/profile.d/my_env.sh文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> vim /etc/profile.d/my_env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在my_env.sh文件末尾添加如下内容：（shift+g）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#HADOOP_HOME</span>
<span class="token function">export</span> HADOOP_HOME<span class="token operator">=</span>/opt/module/hadoop-3.1.3
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$HADOOP_HOME</span>/bin
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$HADOOP_HOME</span>/sbin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>（3）保存后退出:wq</p><p>（4）让修改后的文件生效</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">source</span> /etc/profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）测试是否安装成功</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop version
Hadoop 3.1.3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>6）重启(如果Hadoop命令不能用再重启)</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">sync</span>
<span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">sudo</span> <span class="token function">reboot</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="4-Hadoop运行模式启动"><a href="#4-Hadoop运行模式启动" class="headerlink" title="4  Hadoop运行模式启动"></a>4 Hadoop运行模式启动</h2><p>Hadoop运行模式包括：本地模式、伪分布式模式以及完全分布式模式。本地允许模式很简单，公司用的大部分是完全分布式模式。</p><p>Hadoop官方网站：<a target="_blank" rel="noopener" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></p><h3 id="4-1-本地运行模式"><a href="#4-1-本地运行模式" class="headerlink" title="4.1 本地运行模式"></a>4.1 本地运行模式</h3><p>下面展示hadoop本地运行模式，并成功计算一个wordcout功能</p><p>1）创建在hadoop-3.1.3文件下面创建一个wcinput文件夹</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> wcinpu<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）在wcinput文件下创建一个word.txt文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">cd</span> wcinput<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）编辑word.txt文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 wcinput<span class="token punctuation">]</span>$ vim word.txt
hadoop yarn
hadoop mapreduce
root
root<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>保存退出：：wq</p><p>4）回到Hadoop目录/opt/module/hadoop-3.1.3</p><p>5）执行程序</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>6）查看结果</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">cat</span> wcoutput/part-r-00000
root 2
hadoop 2
mapreduce    1
yarn  1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-2-完全分布式模式"><a href="#4-2-完全分布式模式" class="headerlink" title="4.2 完全分布式模式"></a>4.2 完全分布式模式</h3><h4 id="4-2-1-集群规划"><a href="#4-2-1-集群规划" class="headerlink" title="4.2.1 集群规划"></a>4.2.1 集群规划</h4><p>准备三台机器，分别安装HDFS和yarn。</p><table><thead><tr><th></th><th>hadoop102</th><th>hadoop103</th><th>hadoop104</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode DataNode</td><td>DataNode</td><td>SecondaryNameNode DataNode</td></tr><tr><td>YARN</td><td>NodeManager</td><td>ResourceManager NodeManager</td><td>NodeManager</td></tr></tbody></table><p>注意：NameNode和SecondaryNameNode不要安装在同一台服务器</p><p>注意：ResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在同一台机器上。</p><h4 id="4-2-2-配置文件说明"><a href="#4-2-2-配置文件说明" class="headerlink" title="4.2.2 配置文件说明"></a>4.2.2 配置文件说明</h4><p>Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。</p><p>（1）默认配置文件：</p><table><thead><tr><th>要获取的默认文件</th><th>文件存放在Hadoop的jar包中的位置</th></tr></thead><tbody><tr><td>[core-default.xml]</td><td>hadoop-common-3.1.3.jar/ core-default.xml</td></tr><tr><td>[hdfs-default.xml]</td><td>hadoop-hdfs-3.1.3.jar/ hdfs-default.xml</td></tr><tr><td>[yarn-default.xml]</td><td>hadoop-yarn-common-3.1.3.jar/ yarn-default.xml</td></tr><tr><td>[mapred-default.xml]</td><td>hadoop-mapreduce-client-core-3.1.3.jar/ mapred-default.xml</td></tr></tbody></table><p>2）自定义配置文件：</p><p>core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置。</p><p>（3）常用端口号说明</p><table><thead><tr><th>Daemon</th><th>App</th><th>Hadoop2</th><th>Hadoop3</th></tr></thead><tbody><tr><td>NameNode Port</td><td>Hadoop HDFS NameNode</td><td>8020 / 9000</td><td>9820</td></tr><tr><td></td><td>Hadoop HDFS NameNode HTTP UI</td><td>50070</td><td>9870</td></tr><tr><td>Secondary NameNode Port</td><td>Secondary NameNode</td><td>50091</td><td>9869</td></tr><tr><td></td><td>Secondary NameNode HTTP UI</td><td>50090</td><td>9868</td></tr><tr><td>DataNode Port</td><td>Hadoop HDFS DataNode IPC</td><td>50020</td><td>9867</td></tr><tr><td></td><td>Hadoop HDFS DataNode</td><td>50010</td><td>9866</td></tr><tr><td></td><td>Hadoop HDFS DataNode HTTP UI</td><td>50075</td><td>9864</td></tr></tbody></table><h4 id="4-2-3-配置集群"><a href="#4-2-3-配置集群" class="headerlink" title="4.2.3 配置集群"></a>4.2.3 配置集群</h4><p>（1）核心配置文件：配置core-site.xml</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">cd</span> <span class="token variable">$HADOOP_HOME</span>/etc/hadoop
<span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ vim core-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>文件内容如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- 指定NameNode的地址 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://hadoop102:9820<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 指定hadoop数据的存储目录 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/opt/module/hadoop-3.1.3/data<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 配HDFS网页登录使用的静态用户为root --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.http.staticuser.user<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>root<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 配置该root(superUser)允许通过代理访问的主机节点 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.proxyuser.root.hosts<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 配置该root(superUser)允许通过代理用户所属组 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.proxyuser.root.groups<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 配置该root(superUser)允许通过代理的用户--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.proxyuser.root.groups<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）HDFS配置文件:配置hdfs-site.xml</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ vim hdfs-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>文件内容如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- nn web端访问地址--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop102:9870<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- 2nn web端访问地址--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop104:9868<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）YARN配置文件:配置yarn-site.xml</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ vim yarn-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>文件内容如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- 指定MR走shuffle --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.aux-services<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mapreduce_shuffle<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 指定ResourceManager的地址--></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.hostname<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hadoop103<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 环境变量的继承 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.env-whitelist<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- yarn容器允许分配的最大最小内存 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.minimum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>512<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.maximum-allocation-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- yarn容器允许管理的物理内存大小 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.resource.memory-mb<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>4096<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 关闭yarn对物理内存和虚拟内存的限制检查 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.pmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.vmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（4）MapReduce配置文件:配置mapred-site.xml</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ vim mapred-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>文件内容如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token comment" spellcheck="true">&lt;!-- 指定MapReduce程序运行在Yarn上 --></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.framework.name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>yarn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4）在集群上分发配置好的Hadoop配置文件，将配置文件同步到hadoop103和hadoop104</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ xsync /opt/module/hadoop-3.1.3/etc/hadoop/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）去103和104上查看文件分发情况</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop103 ~<span class="token punctuation">]</span>$ <span class="token function">cat</span> /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml
<span class="token punctuation">[</span>root@hadoop104 ~<span class="token punctuation">]</span>$ <span class="token function">cat</span> /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="4-2-4-启动集群"><a href="#4-2-4-启动集群" class="headerlink" title="4.2.4  启动集群"></a>4.2.4 启动集群</h4><p><strong>1）配置workers</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ vim /opt/module/hadoop-3.1.3/etc/hadoop/workers<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在该文件中增加如下内容：</p><pre><code>hadoop102
hadoop103
hadoop104</code></pre><p>注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。</p><p>同步所有节点配置文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop<span class="token punctuation">]</span>$ xsync /opt/module/hadoop-3.1.3/etc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>2）启动集群</strong></p><p>​ （1）如果集群是第一次启动，需要在hadoop102节点格式化NameNode（注意格式化NameNode，会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化NameNode的话，一定要先停止namenode和datanode进程，并且要删除所有机器的data和logs目录，然后再进行格式化。）</p><pre><code>[root@hadoop102 ~]$ hdfs namenode -format</code></pre><p>（2）启动HDFS</p><pre><code>[root@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</code></pre><p>（3）在配置了ResourceManager的节点（hadoop103）启动YARN</p><pre><code>[root@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</code></pre><p>（4）Web端查看HDFS的NameNode</p><p>​ （a）浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop102:9870/">http://hadoop102:9870</a></p><p>​ （b）查看HDFS上存储的数据信息</p><p>（5）Web端查看YARN的ResourceManager</p><p>​ （a）浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop103:8088/">http://hadoop103:8088</a></p><p>​ （b）查看YARN上运行的Job信息</p><h4 id="4-2-5-集群基本测试"><a href="#4-2-5-集群基本测试" class="headerlink" title="4.2.5 集群基本测试"></a>4.2.5 集群基本测试</h4><p>HDFS相当于一个文件存储框架，搭好集群后，可以在集群去对文件进行操作，上传，下载，删除，查看等。</p><p><strong>（1）上传文件到集群</strong></p><p>​ 上传小文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ hadoop fs -mkdir /input
<span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ hadoop fs -put <span class="token variable">$HADOOP_HOME</span>/wcinput/word.txt /input<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>​ 上传大文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ hadoop fs -put  /opt/software/jdk-8u212-linux-x64.tar.gz  /<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>（2）上传文件后查看文件存放在什么位置</strong></p><p>（a）查看HDFS文件存储路径</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 subdir0<span class="token punctuation">]</span>$ <span class="token function">pwd</span>
/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-938951106-192.168.10.107-1495462844069/current/finalized/subdir0/subdir0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（b）查看HDFS在磁盘存储文件内容</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 subdir0<span class="token punctuation">]</span>$ <span class="token function">cat</span> blk_1073741825
hadoop yarn
hadoop mapreduce 
root
root<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>（3）下载</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop104 software<span class="token punctuation">]</span>$ hadoop fs -get /jdk-8u212-linux-x64.tar.gz ./<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>（4）执行wordcount程序</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-2-6-集群启动-停止方式总结"><a href="#4-2-6-集群启动-停止方式总结" class="headerlink" title="4.2.6 集群启动/停止方式总结"></a>4.2.6 集群启动/停止方式总结</h4><p><strong>1）各个服务组件逐一启动/停止</strong></p><p>​ （1）分别启动/停止HDFS组件</p><pre class="line-numbers language-bash"><code class="language-bash">hdfs --daemon start/stop namenode/datanode/secondarynamenode<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​ （2）启动/停止YARN</p><pre class="line-numbers language-bash"><code class="language-bash">yarn --daemon start/stop resourcemanager/nodemanager<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）各个模块分开启动/停止（配置ssh是前提）常用</p><p>​ （1）整体启动/停止HDFS</p><pre class="line-numbers language-bash"><code class="language-bash">start-dfs.sh/stop-dfs.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​ （2）整体启动/停止YARN</p><pre class="line-numbers language-bash"><code class="language-bash">start-yarn.sh/stop-yarn.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="4-2-7-编写hadoop集群常用脚本"><a href="#4-2-7-编写hadoop集群常用脚本" class="headerlink" title="4.2.7 编写hadoop集群常用脚本"></a>4.2.7 编写hadoop集群常用脚本</h4><p><strong>(1）查看三台服务器java进程脚本：jpsall</strong></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">cd</span> /home/root/bin
<span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ vim jpsall<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>然后输入</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>
<span class="token keyword">for</span> host <span class="token keyword">in</span> hadoop102 hadoop103 hadoop104
<span class="token keyword">do</span>
​    <span class="token keyword">echo</span> <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span> <span class="token variable">$host</span> <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span>
​    <span class="token function">ssh</span> <span class="token variable">$host</span> jps <span class="token variable">$@</span> <span class="token operator">|</span> <span class="token function">grep</span> -v Jps
<span class="token keyword">done</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>保存后退出，然后赋予脚本执行权限</p><p>[root@hadoop102 bin]$ chmod +x jpsall</p><p><strong>（2）hadoop集群启停脚本（包含hdfs，yarn，historyserver）：</strong>myhadoop.sh</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ <span class="token function">cd</span> /home/root/bin
<span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ vim myhadoop.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>然后输入</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>

<span class="token keyword">if</span> <span class="token punctuation">[</span> $<span class="token comment" spellcheck="true"># -lt 1 ]</span>
<span class="token keyword">then</span>
  <span class="token keyword">echo</span> <span class="token string">"No Args Input..."</span>
  <span class="token keyword">exit</span> <span class="token punctuation">;</span>
<span class="token keyword">fi</span>
<span class="token keyword">case</span> <span class="token variable">$1</span> <span class="token keyword">in</span>
<span class="token string">"start"</span><span class="token punctuation">)</span>
​    <span class="token keyword">echo</span> <span class="token string">" =================== 启动 hadoop集群 ==================
​    echo "</span> --------------- 启动 hdfs ---------------<span class="token string">"
​    ssh hadoop102 "</span>/opt/module/hadoop-3.1.3/sbin/start-dfs.sh<span class="token string">"
​    echo "</span> --------------- 启动 yarn ---------------<span class="token string">"
​    ssh hadoop103 "</span>/opt/module/hadoop-3.1.3/sbin/start-yarn.sh<span class="token string">"
​    echo "</span> --------------- 启动 historyserver ---------------<span class="token string">"
​    ssh hadoop102 "</span>/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver<span class="token string">"
;;

"</span>stop<span class="token string">")
​    echo "</span> <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span> 关闭 hadoop集群 <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span>
​    <span class="token keyword">echo</span> <span class="token string">" --------------- 关闭 historyserver ---------------"</span>
​    <span class="token function">ssh</span> hadoop102 <span class="token string">"/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"</span>
​    <span class="token keyword">echo</span> <span class="token string">" --------------- 关闭 yarn ---------------"</span>
​    <span class="token function">ssh</span> hadoop103 <span class="token string">"/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"</span>
​    <span class="token keyword">echo</span> <span class="token string">" --------------- 关闭 hdfs ---------------"</span>
​    <span class="token function">ssh</span> hadoop102 <span class="token string">"/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"</span>

<span class="token punctuation">;</span><span class="token punctuation">;</span>

*<span class="token punctuation">)</span>
  <span class="token keyword">echo</span> <span class="token string">"Input Args Error..."</span>

<span class="token punctuation">;</span><span class="token punctuation">;</span>
esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>保存后退出，然后赋予脚本执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> +x myhadoop.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）分发/home/root/bin目录，保证自定义脚本在三台机器上都可以使用</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 ~<span class="token punctuation">]</span>$ xsync /home/root/bin/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="5-hdfs常用shell操作"><a href="#5-hdfs常用shell操作" class="headerlink" title="5 hdfs常用shell操作"></a>5 hdfs常用shell操作</h2><h3 id="5-1-基本语法"><a href="#5-1-基本语法" class="headerlink" title="5.1 基本语法"></a>5.1 基本语法</h3><p>hadoop fs 具体命令 OR hdfs dfs 具体命令</p><p>两个是完全相同的。</p><h3 id="5-2-命令大全"><a href="#5-2-命令大全" class="headerlink" title="5.2 命令大全"></a>5.2 命令大全</h3><p>查看所有命令</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ bin/hadoop fs<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="5-3-常用命令实操"><a href="#5-3-常用命令实操" class="headerlink" title="5.3 常用命令实操"></a>5.3 常用命令实操</h3><h4 id="5-3-1-准备工作"><a href="#5-3-1-准备工作" class="headerlink" title="5.3.1 准备工作"></a>5.3.1 准备工作</h4><p>1）启动Hadoop集群（方便后续的测试）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-dfs.sh
<span class="token punctuation">[</span>root@hadoop103 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-yarn.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）-help：输出这个命令参数</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -help <span class="token function">rm</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-3-2-上传"><a href="#5-3-2-上传" class="headerlink" title="5.3.2 上传"></a>5.3.2 上传</h4><p>1）-moveFromLocal：从本地剪切粘贴到HDFS</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">touch</span> kongming.txt
<span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -copyFromLocal README.txt /<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）-appendToFile：追加一个文件到已经存在的文件末尾</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">touch</span> liubei.txt
<span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">vi</span> liubei.txt
san gu mao lu
<span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>4）-put：等同于copyFromLocal</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -put ./liubei.txt /user/root/test/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-3-3-下载"><a href="#5-3-3-下载" class="headerlink" title="5.3.3 下载"></a>5.3.3 下载</h4><p>1）-copyToLocal：从HDFS拷贝到本地</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）-get：等同于copyToLocal，就是从HDFS下载文件到本地</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -get /sanguo/shuguo/kongming.txt ./<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）-getmerge：合并下载多个文件，比如HDFS的目录 /user/root/test下有多个文件:log.1, log.2,log.3,…</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -getmerge /user/root/test/* ./zaiyiqi.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="5-3-4-HDFS直接操作"><a href="#5-3-4-HDFS直接操作" class="headerlink" title="5.3.4 HDFS直接操作"></a>5.3.4 HDFS直接操作</h4><p>1）-ls: 显示目录信息</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -ls /<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）-mkdir：在HDFS上创建目录</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -mkdir -p /sanguo/shuguo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）-cat：显示文件内容</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -cat /sanguo/shuguo/kongming.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -chmod 666 /sanguo/shuguo/kongming.txt
<span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -chown root:root  /sanguo/shuguo/kongming.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>5）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>6）-mv：在HDFS目录中移动文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -mv /zhuge.txt /sanguo/shuguo/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>7）-tail：显示一个文件的末尾1kb的数据</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -tail /sanguo/shuguo/kongming.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>8）-rm：删除文件或文件夹</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -rm /user/root/test/jinlian2.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>9）-rmdir：删除空目录</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -mkdir /test
<span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -rmdir /test<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>10）-du统计文件夹的大小信息</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -du -s -h /user/root/test<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>11）-setrep：设置HDFS中文件的副本数量</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。</p><h1 id="6-hdfs的API操作"><a href="#6-hdfs的API操作" class="headerlink" title="6 hdfs的API操作"></a>6 hdfs的API操作</h1><h2 id="6-1-准备Windows关于Hadoop的开发环境"><a href="#6-1-准备Windows关于Hadoop的开发环境" class="headerlink" title="6.1 准备Windows关于Hadoop的开发环境"></a>6.1 准备Windows关于Hadoop的开发环境</h2><p>1）找到资料目录下的Windows依赖目录，打开：</p><p>选择Hadoop-3.1.0，拷贝到其他地方(比如d:)。<br>2）配置HADOOP_HOME环境变量。</p><p>3）配置Path环境变量。然后重启电脑</p><p>4）创建一个Maven工程HdfsClientDemo，并导入相应的依赖坐标+日志添加</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>junit<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>junit<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>4.12<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.logging.log4j<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>log4j-slf4j-impl<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.12.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>hadoop-client<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.1.3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">></span></span>
在项目的src/main/resources目录下，新建一个文件，命名为“log4j2.xml”，在文件中填入
<span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Configuration</span> <span class="token attr-name">status</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>error<span class="token punctuation">"</span></span> <span class="token attr-name">strict</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>XMLConfig<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Appenders</span><span class="token punctuation">></span></span>
        <span class="token comment" spellcheck="true">&lt;!-- 类型名为Console，名称为必须属性 --></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Appender</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>Console<span class="token punctuation">"</span></span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>STDOUT<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
            <span class="token comment" spellcheck="true">&lt;!-- 布局为PatternLayout的方式，
            输出样式为[INFO] [2018-01-22 17:34:01][org.test.Console]I'm here --></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Layout</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>PatternLayout<span class="token punctuation">"</span></span>
                    <span class="token attr-name">pattern</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>[%p] [%d&amp;#123;yyyy-MM-dd HH:mm:ss&amp;#125;][%c&amp;#123;10&amp;#125;]%m%n<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Appender</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Appenders</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Loggers</span><span class="token punctuation">></span></span>
        <span class="token comment" spellcheck="true">&lt;!-- 可加性为false --></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Logger</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>test<span class="token punctuation">"</span></span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>info<span class="token punctuation">"</span></span> <span class="token attr-name">additivity</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>false<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>AppenderRef</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>STDOUT<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Logger</span><span class="token punctuation">></span></span>

        <span class="token comment" spellcheck="true">&lt;!-- root loggerConfig设置 --></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Root</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>info<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>AppenderRef</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>STDOUT<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Root</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Loggers</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5）创建包名：com.atguigu.hdfs<br>6）创建HdfsClient类</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">HdfsClient</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>    
<span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testMkdirs</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 1 获取文件系统</span>
        Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">// 配置在集群上运行</span>
        <span class="token comment" spellcheck="true">// configuration.set("fs.defaultFS", "hdfs://hadoop102:9820");</span>
        <span class="token comment" spellcheck="true">// FileSystem fs = FileSystem.get(configuration);</span>

        FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9820"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"atguigu"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 2 创建目录</span>
        fs<span class="token punctuation">.</span><span class="token function">mkdirs</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/1108/daxian/banzhang"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 3 关闭资源</span>
        fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>7）执行程序<br>运行时需要配置用户名称</p><p>客户端去操作HDFS时，是有一个用户身份的。默认情况下，HDFS客户端API会从JVM中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=atguigu，atguigu为用户名称。</p><h2 id="6-2-HDFS的API操作"><a href="#6-2-HDFS的API操作" class="headerlink" title="6.2 HDFS的API操作"></a>6.2 HDFS的API操作</h2><h3 id="6-2-1-HDFS文件上传（测试参数优先级）"><a href="#6-2-1-HDFS文件上传（测试参数优先级）" class="headerlink" title="6.2.1 HDFS文件上传（测试参数优先级）"></a>6.2.1 HDFS文件上传（测试参数优先级）</h3><p>1）编写源代码</p><pre class="line-numbers language-java"><code class="language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testCopyFromLocalFile</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 1 获取文件系统</span>
        Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"dfs.replication"</span><span class="token punctuation">,</span> <span class="token string">"2"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:8020"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"atguigu"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 2 上传文件</span>
        fs<span class="token punctuation">.</span><span class="token function">copyFromLocalFile</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"e:/banzhang.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/banzhang.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 3 关闭资源</span>
        fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"over"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
｝<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）将hdfs-site.xml拷贝到项目的根目录下</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token prolog">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）参数优先级<br>参数优先级排序：（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的自定义配置(xxx-site.xml) &gt;（4）服务器的默认配置(xxx-default.xml)</p><h3 id="6-2-2-HDFS文件下载"><a href="#6-2-2-HDFS文件下载" class="headerlink" title="6.2.2 HDFS文件下载"></a>6.2.2 HDFS文件下载</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testCopyToLocalFile</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 1 获取文件系统</span>
        Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9820"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"atguigu"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 2 执行下载操作</span>
        <span class="token comment" spellcheck="true">// boolean delSrc 指是否将原文件删除</span>
        <span class="token comment" spellcheck="true">// Path src 指要下载的文件路径</span>
        <span class="token comment" spellcheck="true">// Path dst 指将文件下载到的路径</span>
        <span class="token comment" spellcheck="true">// boolean useRawLocalFileSystem 是否开启文件校验</span>
        fs<span class="token punctuation">.</span><span class="token function">copyToLocalFile</span><span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/banzhang.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"e:/banhua.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 3 关闭资源</span>
        fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="6-2-3-HDFS删除文件和目录"><a href="#6-2-3-HDFS删除文件和目录" class="headerlink" title="6.2.3 HDFS删除文件和目录"></a>6.2.3 HDFS删除文件和目录</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testDelete</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">// 1 获取文件系统</span>
    Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9820"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"atguigu"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">// 2 执行删除</span>
    fs<span class="token punctuation">.</span><span class="token function">delete</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/0508/"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">// 3 关闭资源</span>
    fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="6-2-4-HDFS文件更名和移动"><a href="#6-2-4-HDFS文件更名和移动" class="headerlink" title="6.2.4 HDFS文件更名和移动"></a>6.2.4 HDFS文件更名和移动</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testRename</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">// 1 获取文件系统</span>
    Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9820"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"atguigu"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 

    <span class="token comment" spellcheck="true">// 2 修改文件名称</span>
    fs<span class="token punctuation">.</span><span class="token function">rename</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/banzhang.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/banhua.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">// 3 关闭资源</span>
    fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="6-2-5-HDFS文件详情查看"><a href="#6-2-5-HDFS文件详情查看" class="headerlink" title="6.2.5 HDFS文件详情查看"></a>6.2.5 HDFS文件详情查看</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//查看文件名称、权限、长度、块信息</span>
<span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testListFiles</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">// 1获取文件系统</span>
    Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9820"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"atguigu"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 

    <span class="token comment" spellcheck="true">// 2 获取文件详情</span>
    RemoteIterator<span class="token operator">&lt;</span>LocatedFileStatus<span class="token operator">></span> listFiles <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">listFiles</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token keyword">while</span><span class="token punctuation">(</span>listFiles<span class="token punctuation">.</span><span class="token function">hasNext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        LocatedFileStatus status <span class="token operator">=</span> listFiles<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 输出详情</span>
        <span class="token comment" spellcheck="true">// 文件名称</span>
        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>status<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">// 长度</span>
        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>status<span class="token punctuation">.</span><span class="token function">getLen</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">// 权限</span>
        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>status<span class="token punctuation">.</span><span class="token function">getPermission</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">// 分组</span>
        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>status<span class="token punctuation">.</span><span class="token function">getGroup</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 获取存储的块信息</span>
        BlockLocation<span class="token punctuation">[</span><span class="token punctuation">]</span> blockLocations <span class="token operator">=</span> status<span class="token punctuation">.</span><span class="token function">getBlockLocations</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token keyword">for</span> <span class="token punctuation">(</span>BlockLocation blockLocation <span class="token operator">:</span> blockLocations<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

            <span class="token comment" spellcheck="true">// 获取块存储的主机节点</span>
            String<span class="token punctuation">[</span><span class="token punctuation">]</span> hosts <span class="token operator">=</span> blockLocation<span class="token punctuation">.</span><span class="token function">getHosts</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

            <span class="token keyword">for</span> <span class="token punctuation">(</span>String host <span class="token operator">:</span> hosts<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
                System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>host<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"-----------班长的分割线----------"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// 3 关闭资源</span>
fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="6-2-6-HDFS文件和文件夹判断"><a href="#6-2-6-HDFS文件和文件夹判断" class="headerlink" title="6.2.6 HDFS文件和文件夹判断"></a>6.2.6 HDFS文件和文件夹判断</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testListStatus</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>


    <span class="token comment" spellcheck="true">// 1 获取文件配置信息</span>
    Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9820"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"atguigu"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">// 2 判断是文件还是文件夹</span>
    FileStatus<span class="token punctuation">[</span><span class="token punctuation">]</span> listStatus <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">listStatus</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token keyword">for</span> <span class="token punctuation">(</span>FileStatus fileStatus <span class="token operator">:</span> listStatus<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 如果是文件</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>fileStatus<span class="token punctuation">.</span><span class="token function">isFile</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
                System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"f:"</span><span class="token operator">+</span>fileStatus<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token keyword">else</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
                System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"d:"</span><span class="token operator">+</span>fileStatus<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token comment" spellcheck="true">// 3 关闭资源</span>
    fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul></div><span class="post-count">总字数5.4k</span> <span class="post-count">预计阅读26分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-hdfs" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/12/bigdata-hdfs/" class="article-date"><time class="published" datetime="2020-11-12T07:34:38.000Z" itemprop="datePublished">2020-11-12 发布</time> <time class="updated" datetime="2021-11-12T08:52:00.044Z" itemprop="dateUpdated">2021-11-12 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/12/bigdata-hdfs/">Hadoop 教程（一）hadoop介绍</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="1-hadoop是什么"><a href="#1-hadoop是什么" class="headerlink" title="1 hadoop是什么"></a>1 hadoop是什么</h2><p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构。它提供了一个<strong>海量数据存储</strong>和<strong>分析计算</strong>的能力。广义上来说Hadoop这个词代表了Hadoop生态圈。</p><p>Hadoop的核心是YARN,HDFS和Mapreduce。随着处理任务不同，各种组件相继出现，丰富Hadoop生态圈，目前生态圈结构大致如图所示 。</p><p><img src="/2020/11/12/bigdata-hdfs/1636705530947.png" alt="1636705530947"></p><h2 id="2-Hadoop的特点"><a href="#2-Hadoop的特点" class="headerlink" title="2. Hadoop的特点"></a>2. Hadoop的特点</h2><ul><li>扩容能力（Scalable） 能可靠地（reliably）存储和处理千兆字节（PB）数据</li><li>成本低（Economical） 可以通过普通机器组成的服务器集群来分发以及处理数据。这些服务器几圈总计可以达到千个节点。</li><li>高效率（Efficient） 通过分发数据，hadoop 可以在数据所在的节点上并行的(parallel)处理它们，这使得处理非常快。</li><li>可靠性（Reliable） hadoop 能自动地维护数据的多份副本，并且在任务失败后能自动重新部署(redeploy)计算任务</li></ul><h2 id="3-Hadoop三大发行版本"><a href="#3-Hadoop三大发行版本" class="headerlink" title="3 Hadoop三大发行版本"></a>3 Hadoop三大发行版本</h2><p>Hadoop三大发行版本：Apache、Cloudera、Hortonworks。</p><p>Apache版本最原始（最基础）的版本，对于入门学习最好。</p><p>Cloudera内部集成了很多大数据框架。对应产品CDH。</p><p>Hortonworks文档较好。对应产品HDP。后面被Cloudera收购</p><h2 id="4-Hadoop组成"><a href="#4-Hadoop组成" class="headerlink" title="4 Hadoop组成"></a>4 Hadoop组成</h2><p>Hadoop的核心组件分为：HDFS（分布式文件系统）、MapRuduce（分布式运算编程框架）、YARN（运算资源调度系统） 。</p><p><img src="/2020/11/12/bigdata-hdfs/1636706148747.png" alt="1636706148747"></p><h3 id="4-1-HDFS"><a href="#4-1-HDFS" class="headerlink" title="4.1 HDFS"></a>4.1 HDFS</h3><p>​ 整个Hadoop的体系结构主要是通过HDFS（Hadoop分布式文件系统）来实现对分布式存储的底层支持，并通过MR来实现对分布式并行任务处理的程序支持。<strong>HDFS是Hadoop体系中数据存储管理的基础</strong>。</p><p>一个HDFS集群是由一个NameNode和若干个DataNode组成的。NameNodee存储元数据，作为主服务器，管理文件系统命名空间和客户端对文件的访问操作。DataNode存储数据，DataNode管理存储的数据。HDFS支持文件形式的数据。</p><p><img src="/2020/11/12/bigdata-hdfs/1636706441346.png" alt="1636706441346"></p><h3 id="4-2-YARN架构"><a href="#4-2-YARN架构" class="headerlink" title="4.2  YARN架构"></a>4.2 YARN架构</h3><p>上面我们说了 Hadoop2.x 中增加了 Yarn(资源调度)，那资源调度是在调度什么呢？在计算机中资源就是CPU和内存，CPU和内存都是有上限的，所以需要分配给更需要的进程来使用。</p><p><img src="/2020/11/12/bigdata-hdfs/1636706565758.png" alt="1636706565758"></p><p>ResourceManager（RM）就是资源管理者，外部的客户端提交作业请求都会先到 ResourceManager（RM），他代表了集群所有的资源，并监控 NodeManager、启动或监控ApplicationMaster。</p><p>NodeManager（NM） 只管理一个节点的资源，处理来自ResourceManager（RM）的命令和来自ApplicationMaster的命令。</p><p>ApplicationMaster（AM）负责数据的切分、为应用程序申请资源分配内部任务和任务的监控容错。当一个任务提交到 ResourceManager（RM）时就会选择一个节点启动一个ApplicationMaster（AM）来负责这个任务的跟进，也就是对这个任务的一个负责人。也就是说有一个作业任务就会有对应的一个ApplicationMaster（AM）来跟进这个作业任务的执行和调度。</p><p>Container 是对资源的一个抽象封装，里面会包含内存、CPU、磁盘、网络等资源，NodeManager（NM） 就是通过打开和关闭 Container 来调度资源的。</p><h3 id="4-3-MapReduce架构概述"><a href="#4-3-MapReduce架构概述" class="headerlink" title="4.3   MapReduce架构概述"></a>4.3 MapReduce架构概述</h3><p>MapReduce是一种编程模型，用于大规模数据集的并行计算，需要将数据分配到大量的机器上计算，每台机器运行一个子计算任务，最后再合并每台机器运算结果并输出。 MapReduce 的思想就是 『分而治之』.</p><p>MapReduce将计算过程分为两个阶段：Map和Reduce</p><p>1）Map阶段并行处理输入数据</p><p>2）Reduce阶段对Map结果进行汇总</p><p><img src="/2020/11/12/bigdata-hdfs/1636706761153.png" alt="1636706761153"></p><h2 id="5-大数据技术生态体系"><a href="#5-大数据技术生态体系" class="headerlink" title="5  大数据技术生态体系"></a>5 大数据技术生态体系</h2><p><img src="/2020/11/12/bigdata-hdfs/1636706809649.png" alt="1636706809649"></p><p>图中涉及的技术名词解释如下：</p><p>1）Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库（MySql）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p><p>2）Flume：Flume是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；</p><p>3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统；</p><p>4）Spark：是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。</p><p>5）Flink：Flink是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。</p><p>6）Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。</p><p>7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</p><p>8）Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p><p>9）ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。</p><h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1661405">https://cloud.tencent.com/developer/article/1661405</a></p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul></div><span class="post-count">总字数1.4k</span> <span class="post-count">预计阅读5分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-yarn-framework" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/12/bigdata-yarn-framework/" class="article-date"><time class="published" datetime="2020-11-12T07:17:06.000Z" itemprop="datePublished">2020-11-12 发布</time> <time class="updated" datetime="2021-12-01T07:29:04.133Z" itemprop="dateUpdated">2021-12-01 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/12/bigdata-yarn-framework/">Hadoop 教程（六）yarn-架构解析</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="1-Yarn资源调度器"><a href="#1-Yarn资源调度器" class="headerlink" title="1 Yarn资源调度器"></a>1 Yarn资源调度器</h1><p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p><h2 id="1-1-Yarn基本架构"><a href="#1-1-Yarn基本架构" class="headerlink" title="1.1 Yarn基本架构"></a>1.1 Yarn基本架构</h2><pre><code> YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。</code></pre><p><img src="/2020/11/12/bigdata-yarn-framework/1638343400280.png" alt="1638343400280"></p><h2 id="1-2-Yarn工作机制"><a href="#1-2-Yarn工作机制" class="headerlink" title="1.2 Yarn工作机制"></a>1.2 Yarn工作机制</h2><p><img src="/2020/11/12/bigdata-yarn-framework/1638343444066.png" alt="1638343444066"></p><p>（1）MR程序提交到客户端所在的节点。<br>（2）YarnRunner向ResourceManager申请一个Application。<br>（3）RM将该应用程序的资源路径返回给YarnRunner。<br>（4）该程序将运行所需资源提交到HDFS上。<br>（5）程序资源提交完毕后，申请运行mrAppMaster。<br>（6）RM将用户的请求初始化成一个Task。<br>（7）其中一个NodeManager领取到Task任务。<br>（8）该NodeManager创建容器Container，并产生MRAppmaster。<br>（9）Container从HDFS上拷贝资源到本地。<br>（10）MRAppmaster向RM 申请运行MapTask资源。<br>（11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。<br>（12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。<br>（13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。<br>（14）ReduceTask向MapTask获取相应分区的数据。<br>（15）程序运行完毕后，MR会向RM申请注销自己。</p><h2 id="1-3-作业提交全过程"><a href="#1-3-作业提交全过程" class="headerlink" title="1.3 作业提交全过程"></a>1.3 作业提交全过程</h2><p>作业提交工作机制</p><p><img src="/2020/11/12/bigdata-yarn-framework/1638343485216.png" alt="1638343485216"></p><p>作业提交全过程详解</p><p>（1）作业提交<br>第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。<br>第2步：Client向RM申请一个作业id。<br>第3步：RM给Client返回该job资源的提交路径和作业id。<br>第4步：Client提交jar包、切片信息和配置文件到指定的资源提交路径。<br>第5步：Client提交完资源后，向RM申请运行MrAppMaster。<br>（2）作业初始化<br>第6步：当RM收到Client的请求后，将该job添加到容量调度器中。<br>第7步：某一个空闲的NM领取到该Job。<br>第8步：该NM创建Container，并产生MRAppmaster。<br>第9步：下载Client提交的资源到本地。<br>（3）任务分配<br>第10步：MrAppMaster向RM申请运行多个MapTask任务资源。<br>第11步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。<br>（4）任务运行<br>第12步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。<br>第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。<br>第14步：ReduceTask向MapTask获取相应分区的数据。<br>第15步：程序运行完毕后，MR会向RM申请注销自己。<br>（5）进度和状态更新<br>YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。<br>（6）作业完成<br>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</p><p><img src="/2020/11/12/bigdata-yarn-framework/1638343533411.png" alt="1638343533411"></p><h2 id="1-4-资源调度器"><a href="#1-4-资源调度器" class="headerlink" title="1.4 资源调度器"></a>1.4 资源调度器</h2><p>目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop3.1.3默认的资源调度器是Capacity Scheduler。<br>具体设置详见：yarn-default.xml文件</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>The class to use as the resource scheduler.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.scheduler.class<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-4-1-先进先出调度器（FIFO）"><a href="#1-4-1-先进先出调度器（FIFO）" class="headerlink" title="1.4.1 先进先出调度器（FIFO）"></a>1.4.1 先进先出调度器（FIFO）</h3><p><img src="/2020/11/12/bigdata-yarn-framework/1638343565235.png" alt="1638343565235"></p><p>Hadoop最初设计目的是支持大数据批处理作业，如日志挖掘、Web索引等作业，<br>为此，Hadoop仅提供了一个非常简单的调度机制：FIFO，即先来先服务，在该调度机制下，所有作业被统一提交到一个队列中，Hadoop按照提交顺序依次运行这些作业。<br>但随着Hadoop的普及，单个Hadoop集群的用户量越来越大，不同用户提交的应用程序往往具有不同的服务质量要求，典型的应用有以下几种：<br>批处理作业：这种作业往往耗时较长，对时间完成一般没有严格要求，如数据挖掘、机器学习等方面的应用程序。<br>交互式作业：这种作业期望能及时返回结果，如SQL查询（Hive）等。<br>生产性作业：这种作业要求有一定量的资源保证，如统计值计算、垃圾数据分析等。<br>此外，这些应用程序对硬件资源需求量也是不同的，如过滤、统计类作业一般为CPU密集型作业，而数据挖掘、机器学习作业一般为I/O密集型作业。因此，简单的FIFO调度策略不仅不能满足多样化需求，也不能充分利用硬件资源。</p><h3 id="1-4-2-容量调度器（Capacity-Scheduler）"><a href="#1-4-2-容量调度器（Capacity-Scheduler）" class="headerlink" title="1.4.2 容量调度器（Capacity Scheduler）"></a>1.4.2 容量调度器（Capacity Scheduler）</h3><p><img src="/2020/11/12/bigdata-yarn-framework/1638343621340.png" alt="1638343621340"></p><p>Capacity Scheduler Capacity Scheduler 是Yahoo开发的多用户调度器，它以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用。而当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。<br>总之，Capacity Scheduler 主要有以下几个特点：<br>①容量保证。管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源。<br>②灵活性，如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。这种资源灵活分配的方式可明显提高资源利用率。<br>③多重租赁。支持多用户共享集群和多应用程序同时运行。为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束（比如单个应用程序同时运行的任务数等）。<br>④安全保证。每个队列有严格的ACL列表规定它的访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序（比如杀死应用程序）。此外，管理员可指定队列管理员和集群系统管理员。<br>⑤动态更新配置文件。管理员可根据需要动态修改各种配置参数，以实现在线集群管理。</p><h3 id="1-4-3-公平调度器（Fair-Scheduler）（了解）"><a href="#1-4-3-公平调度器（Fair-Scheduler）（了解）" class="headerlink" title="1.4.3 公平调度器（Fair Scheduler）（了解）"></a>1.4.3 公平调度器（Fair Scheduler）（了解）</h3><p><img src="/2020/11/12/bigdata-yarn-framework/1638343637281.png" alt="1638343637281"></p><p>Fair Scheduler Fair Schedulere是Facebook开发的多用户调度器。<br>公平调度器的目的是让所有的作业随着时间的推移，都能平均地获取等同的共享资源。当有作业提交上来，系统会将空闲的资源分配给新的作业，每个任务大致上会获取平等数量的资源。和传统的调度策略不同的是它会让小的任务在合理的时间完成，同时不会让需要长时间运行的耗费大量资源的任务挨饿！<br>同Capacity Scheduler类似，它以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用；当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。<br>当然，Fair Scheduler也存在很多与Capacity Scheduler不同之处，这主要体现在以下几个方面：<br>① 资源公平共享。在每个队列中，Fair Scheduler 可选择按照FIFO、Fair或DRF策略为应用程序分配资源。其中，<br><strong>FIFO策略</strong><br>公平调度器每个队列资源分配策略如果选择FIFO的话，就是禁用掉每个队列中的Task共享队列资源，此时公平调度器相当于上面讲过的容量调度器。<br>Fair策略<br>Fair 策略(默认)是一种基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源；如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。</p><p><strong>DRF策略</strong><br>DRF(Dominant Resource Fairness)，我们之前说的资源，都是单一标准，例如只考虑内存(也是yarn默认的情况)。但是很多时候我们资源有很多种，例如内存，CPU，网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。<br>那么在YARN中，我们用DRF来决定如何调度：假设集群一共有100 CPU和10T 内存，而应用A需要(2 CPU, 300GB)，应用B需要(6 CPU, 100GB)。则两个应用分别需要A(2%CPU, 3%内存)和B(6%CPU, 1%内存)的资源，这就意味着A是内存主导的, B是CPU主导的，针对这种情况，我们可以选择DRF策略对不同应用进行不同资源（CPU和内存）的一个不同比例的限制。<br>②支持资源抢占。当某个队列中有剩余资源时，调度器会将这些资源共享给其他队列，而当该队列中有新的应用程序提交时，调度器要为它回收资源。为了尽可能降低不必要的计算浪费，调度器采用了先等待再强制回收的策略，即如果等待一段时间后尚有未归还的资源，则会进行资源抢占：从那些超额使用资源的队列中杀死一部分任务，进而释放资源。<br>yarn.scheduler.fair.preemption=true 通过该配置开启资源抢占。<br>③提高小应用程序响应时间。由于采用了最大最小公平算法，小作业可以快速获取资源并运行完成</p><h1 id="2-容量调度器多队列提交案例"><a href="#2-容量调度器多队列提交案例" class="headerlink" title="2 容量调度器多队列提交案例"></a>2 容量调度器多队列提交案例</h1><h2 id="2-1-需求"><a href="#2-1-需求" class="headerlink" title="2.1 需求"></a>2.1 需求</h2><p>​ Yarn默认的容量调度器是一条单队列的调度器，在实际使用中会出现单个任务阻塞整个队列的情况。同时，随着业务的增长，公司需要分业务限制集群使用率。这就需要我们按照业务种类配置多条任务队列。</p><h2 id="2-2-配置多队列的容量调度器"><a href="#2-2-配置多队列的容量调度器" class="headerlink" title="2.2 配置多队列的容量调度器"></a>2.2 配置多队列的容量调度器</h2><p>默认Yarn的配置下，容量调度器只有一条Default队列。在capacity-scheduler.xml中可以配置多条队列，并降低default队列资源占比：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token comment" spellcheck="true">&lt;!-- 指定多队列，增加hive队列 --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.queues<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>default,hive<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>
      The queues at the this level (root is the root queue).
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token comment" spellcheck="true">&lt;!-- 降低default队列资源额定容量为40%，默认100% --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.capacity<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>40<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token comment" spellcheck="true">&lt;!-- 降低default队列资源最大容量为60%，默认100% --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.default.maximum-capacity<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>60<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
同时为新加队列添加必要属性：
<span class="token comment" spellcheck="true">&lt;!-- 指定hive队列的资源额定容量 --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.capacity<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>60<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.user-limit-factor<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token comment" spellcheck="true">&lt;!-- 指定hive队列的资源最大容量 --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.maximum-capacity<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.state<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>RUNNING<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.acl_submit_applications<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.acl_administer_queue<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.acl_application_max_priority<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.maximum-application-lifetime<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.scheduler.capacity.root.hive.default-application-lifetime<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在配置完成后，重启Yarn或者执行yarn rmadmin -refreshQueues刷新队列，就可以看到两条队列：</p><p><img src="/2020/11/12/bigdata-yarn-framework/1638343702545.png" alt="1638343702545"></p><p>在配置完成后，重启Yarn或者执行yarn rmadmin -refreshQueues刷新队列，就可以看到两条队列：</p><h2 id="2-3-向Hive队列提交任务"><a href="#2-3-向Hive队列提交任务" class="headerlink" title="2.3 向Hive队列提交任务"></a>2.3 向Hive队列提交任务</h2><p>​ 默认的任务提交都是提交到default队列的。如果希望向其他队列提交任务，需要在Driver中声明：</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WcDrvier</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
​    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> ClassNotFoundException<span class="token punctuation">,</span> InterruptedException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
​        Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

​    configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"mapreduce.job.queuename"</span><span class="token punctuation">,</span><span class="token string">"hive"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

​    <span class="token comment" spellcheck="true">//1. 获取一个Job实例</span>
​    Job job <span class="token operator">=</span> Job<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span>configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>

​    <span class="token comment" spellcheck="true">//2. 设置类路径</span>
​    job<span class="token punctuation">.</span><span class="token function">setJarByClass</span><span class="token punctuation">(</span>WcDrvier<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

​    <span class="token comment" spellcheck="true">//3. 设置Mapper和Reducer</span>
​    job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>WcMapper<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​    job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span>WcReducer<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

​    <span class="token comment" spellcheck="true">//4. 设置Mapper和Reducer的输出类型</span>
​    job<span class="token punctuation">.</span><span class="token function">setMapOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​    job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span>IntWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

​    job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​    job<span class="token punctuation">.</span><span class="token function">setOutputValueClass</span><span class="token punctuation">(</span>IntWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

​    job<span class="token punctuation">.</span><span class="token function">setCombinerClass</span><span class="token punctuation">(</span>WcReducer<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

​    <span class="token comment" spellcheck="true">//5. 设置输入输出文件</span>
​    FileInputFormat<span class="token punctuation">.</span><span class="token function">setInputPaths</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​    FileOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

​    <span class="token comment" spellcheck="true">//6. 提交Job</span>
​    <span class="token keyword">boolean</span> b <span class="token operator">=</span> job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
​    System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>b <span class="token operator">?</span> <span class="token number">0</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样，这个任务在集群提交时，就会提交到hive队列：</p><p>这样，这个任务在集群提交时，就会提交到hive队列：</p><p><img src="/2020/11/12/bigdata-yarn-framework/1638343722576.png" alt="1638343722576"></p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul></div><span class="post-count">总字数3.4k</span> <span class="post-count">预计阅读13分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-mapreduce1-setup" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/12/bigdata-mapreduce1-setup/" class="article-date"><time class="published" datetime="2020-11-12T07:04:15.000Z" itemprop="datePublished">2020-11-12 发布</time> <time class="updated" datetime="2021-12-01T10:37:14.592Z" itemprop="dateUpdated">2021-12-01 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/12/bigdata-mapreduce1-setup/">Hadoop 教程（四）mapreduce介绍</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="第1章-MapReduce概述"><a href="#第1章-MapReduce概述" class="headerlink" title="第1章 MapReduce概述"></a>第1章 MapReduce概述</h1><h2 id="1-1-MapReduce定义"><a href="#1-1-MapReduce定义" class="headerlink" title="1.1 MapReduce定义"></a>1.1 MapReduce定义</h2><p>MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。</p><p>MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并行运行在一个Hadoop集群上。</p><h2 id="1-2-MapReduce优缺点"><a href="#1-2-MapReduce优缺点" class="headerlink" title="1.2 MapReduce优缺点"></a>1.2 MapReduce优缺点</h2><h3 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h3><p>1）MapReduce 易于编程</p><p>它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。</p><p>2）良好的扩展性</p><p>当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。</p><p>3）高容错性</p><p>MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。</p><p>4）适合PB级以上海量数据的离线处理</p><p>可以实现上千台服务器集群并发工作，提供数据处理能力。</p><h3 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h3><p>1）不擅长实时计算</p><p>MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。</p><p>2）不擅长流式计算</p><p>流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。</p><p>3）不擅长DAG（有向无环图）计算</p><p>多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。</p><h2 id="1-3-MapReduce核心思想"><a href="#1-3-MapReduce核心思想" class="headerlink" title="1.3 MapReduce核心思想"></a>1.3 MapReduce核心思想</h2><p>​ <img src="/2020/11/12/bigdata-mapreduce1-setup/1638342393148.png" alt="1638342393148"></p><p>（1）分布式的运算程序往往需要分成至少2个阶段。</p><p>（2）第一个阶段的MapTask并发实例，完全并行运行，互不相干。</p><p>（3）第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。</p><p>（4）MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。</p><p>总结：分析WordCount数据流走向深入理解MapReduce核心思想。</p><h2 id="1-4-MapReduce进程"><a href="#1-4-MapReduce进程" class="headerlink" title="1.4 MapReduce进程"></a>1.4 MapReduce进程</h2><p>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p><p>（1）MrAppMaster：负责整个程序的过程调度及状态协调。</p><p>（2）MapTask：负责Map阶段的整个数据处理流程。</p><p>（3）ReduceTask：负责Reduce阶段的整个数据处理流程。</p><h2 id="1-5-官方WordCount源码"><a href="#1-5-官方WordCount源码" class="headerlink" title="1.5 官方WordCount源码"></a>1.5 官方WordCount源码</h2><p>采用反编译工具反编译源码，发现WordCount案例有Map类、Reduce类和驱动类。且数据的类型是Hadoop自身封装的序列化类型。</p><h2 id="1-6-常用数据序列化类型"><a href="#1-6-常用数据序列化类型" class="headerlink" title="1.6 常用数据序列化类型"></a>1.6 常用数据序列化类型</h2><table><thead><tr><th>Java类型</th><th>Hadoop Writable类型</th></tr></thead><tbody><tr><td>Boolean</td><td>BooleanWritable</td></tr><tr><td>Byte</td><td>ByteWritable</td></tr><tr><td>Int</td><td>IntWritable</td></tr><tr><td>Float</td><td>FloatWritable</td></tr><tr><td>Long</td><td>LongWritable</td></tr><tr><td>Double</td><td>DoubleWritable</td></tr><tr><td>String</td><td>Text</td></tr><tr><td>Map</td><td>MapWritable</td></tr><tr><td>Array</td><td>ArrayWritable</td></tr><tr><td>Null</td><td>NullWritable</td></tr></tbody></table><h2 id="1-7-MapReduce编程规范"><a href="#1-7-MapReduce编程规范" class="headerlink" title="1.7 MapReduce编程规范"></a>1.7 MapReduce编程规范</h2><p>用户编写的程序分成三个部分：Mapper、Reducer和Driver。</p><h3 id="1-7-1-Mapper阶段"><a href="#1-7-1-Mapper阶段" class="headerlink" title="1.7.1 Mapper阶段"></a>1.7.1 Mapper阶段</h3><p>（1）用户自定义的Mapper要继承自己的父类</p><p>（2）Mapper的输入数据是KV对的形式</p><p>（3）Mapper中的业务逻辑编写在map()方法中</p><p>（4）Mapper的输出数据是KV对的形式</p><p><strong>（5）map()方法（MapTask进程）对每一个&lt;K,V&gt;调用一次</strong></p><h3 id="1-7-2-Reducer阶段"><a href="#1-7-2-Reducer阶段" class="headerlink" title="1.7.2 Reducer阶段"></a>1.7.2 Reducer阶段</h3><p>（1）用户自定义的Reducer要继承自己的父类</p><p>（2）Reducer的输入数据是Mapper的输出数据类型</p><p>（3）Reducer中的业务逻辑编写在reduce()方法中</p><p>（4）Mapper的输出数据是KV对的形式</p><p><strong>（5）ReduceTask进程对每一组仙童的&lt;K,V&gt;调用reduce()一次</strong></p><h3 id="1-7-3-Driver阶段"><a href="#1-7-3-Driver阶段" class="headerlink" title="1.7.3 Driver阶段"></a>1.7.3 Driver阶段</h3><p>相当于YARN集群的客户端，用于提交我们整个程序到YARN，提交的是封装了MapReduce程序相关运行参数的job对象。</p><h2 id="1-8-WordCount案例实操"><a href="#1-8-WordCount案例实操" class="headerlink" title="1.8 WordCount案例实操"></a>1.8 WordCount案例实操</h2><p><strong>1）需求</strong></p><p>在给定的文本文件中统计输出每一个单词出现的总次数</p><p>（1）输入数据</p><p>​ <img src="/2020/11/12/bigdata-mapreduce1-setup/1638342710639.png" alt="1638342710639"></p><p>（2）期望输出数据</p><p>molly 2</p><p>banzhang 1</p><p>cls 2</p><p>hadoop 1</p><p>jiao 1</p><p>ss 2</p><p>xue 1</p><p><strong>2）需求分析</strong></p><p>按照MapReduce编程规范，分别编写Mapper，Reducer，Driver。</p><p><img src="/2020/11/12/bigdata-mapreduce1-setup/1638342700100.png" alt="1638342700100"><br>3）环境准备<br>（1）创建maven工程<br>（2）在pom.xml文件中添加如下依赖</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>junit<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>junit<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>4.12<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.logging.log4j<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>log4j-slf4j-impl<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.12.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>hadoop-client<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.1.3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）在项目的src/main/resources目录下，新建一个文件，命名为“log4j2.xml”，在文件中填入。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Configuration</span> <span class="token attr-name">status</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>error<span class="token punctuation">"</span></span> <span class="token attr-name">strict</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>XMLConfig<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Appenders</span><span class="token punctuation">></span></span>
        <span class="token comment" spellcheck="true">&lt;!-- 类型名为Console，名称为必须属性 --></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Appender</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>Console<span class="token punctuation">"</span></span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>STDOUT<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
            <span class="token comment" spellcheck="true">&lt;!-- 布局为PatternLayout的方式，
            输出样式为[INFO] [2018-01-22 17:34:01][org.test.Console]I'm here --></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Layout</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>PatternLayout<span class="token punctuation">"</span></span>
                    <span class="token attr-name">pattern</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>[%p] [%d&amp;#123;yyyy-MM-dd HH:mm:ss&amp;#125;][%c&amp;#123;10&amp;#125;]%m%n<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Appender</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Appenders</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Loggers</span><span class="token punctuation">></span></span>
        <span class="token comment" spellcheck="true">&lt;!-- 可加性为false --></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Logger</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>test<span class="token punctuation">"</span></span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>info<span class="token punctuation">"</span></span> <span class="token attr-name">additivity</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>false<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>AppenderRef</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>STDOUT<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Logger</span><span class="token punctuation">></span></span>

        <span class="token comment" spellcheck="true">&lt;!-- root loggerConfig设置 --></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Root</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>info<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>AppenderRef</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>STDOUT<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Root</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Loggers</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4）编写程序<br>（1）编写Mapper类</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>molly<span class="token punctuation">.</span>mapreduce<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IntWritable<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>LongWritable<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Mapper<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordcountMapper</span> <span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token operator">&lt;</span>LongWritable<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> IntWritable<span class="token operator">></span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    Text k <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    IntWritable v <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">IntWritable</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span>LongWritable key<span class="token punctuation">,</span> Text value<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span>    <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 1 获取一行</span>
        String line <span class="token operator">=</span> value<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 2 切割</span>
        String<span class="token punctuation">[</span><span class="token punctuation">]</span> words <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 3 输出</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>String word <span class="token operator">:</span> words<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

            k<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">;</span>
            context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> v<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）编写Reducer类</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>molly<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>wordcount<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IntWritable<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Reducer<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordcountReducer</span> <span class="token keyword">extends</span> <span class="token class-name">Reducer</span><span class="token operator">&lt;</span>Text<span class="token punctuation">,</span> IntWritable<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> IntWritable<span class="token operator">></span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

<span class="token keyword">int</span> sum<span class="token punctuation">;</span>
IntWritable v <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">IntWritable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span>Text key<span class="token punctuation">,</span> Iterable<span class="token operator">&lt;</span>IntWritable<span class="token operator">></span> values<span class="token punctuation">,</span>Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 1 累加求和</span>
        sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>IntWritable count <span class="token operator">:</span> values<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            sum <span class="token operator">+=</span> count<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 2 输出</span>
         v<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
        context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）编写Driver驱动类</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>molly<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>wordcount<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>conf<span class="token punctuation">.</span>Configuration<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>fs<span class="token punctuation">.</span>Path<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IntWritable<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Job<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>input<span class="token punctuation">.</span>FileInputFormat<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>output<span class="token punctuation">.</span>FileOutputFormat<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordcountDriver</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> ClassNotFoundException<span class="token punctuation">,</span> InterruptedException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 1 获取配置信息以及获取job对象</span>
        Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        Job job <span class="token operator">=</span> Job<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span>configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 2 关联本Driver程序的jar</span>
        job<span class="token punctuation">.</span><span class="token function">setJarByClass</span><span class="token punctuation">(</span>WordcountDriver<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 3 关联Mapper和Reducer的jar</span>
        job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>WordcountMapper<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span>WordcountReducer<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 4 设置Mapper输出的kv类型</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span>IntWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 5 设置最终输出kv类型</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputValueClass</span><span class="token punctuation">(</span>IntWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 6 设置输入和输出路径</span>
        FileInputFormat<span class="token punctuation">.</span><span class="token function">setInputPaths</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        FileOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 7 提交job</span>
        <span class="token keyword">boolean</span> result <span class="token operator">=</span> job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>result <span class="token operator">?</span> <span class="token number">0</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5）本地测试<br>（1）需要首先配置好HADOOP_HOME变量以及Windows运行依赖<br>（2）在IDEA/Eclipse上运行程序<br>6）集群上测试<br>（0）用maven打jar包，需要添加的打包插件依赖</p><pre class="line-numbers language-xml"><code class="language-xml">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-compiler-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.6.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>source</span><span class="token punctuation">></span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>source</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>target</span><span class="token punctuation">></span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>target</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-assembly-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRefs</span><span class="token punctuation">></span></span>
                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRef</span><span class="token punctuation">></span></span>jar-with-dependencies<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRef</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRefs</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>
                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>make-assembly<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>
                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>
                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>
                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>single<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>
                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意：如果工程上显示红叉。在项目上右键-&gt;maven-&gt;Reimport即可。<br>（1）将程序打成jar包，然后拷贝到Hadoop集群中<br>步骤详情：右键-&gt;Run as-&gt;maven install。等待编译完成就会在项目的target文件夹中生成jar包。如果看不到。在项目上右键-&gt;Refresh，即可看到。修改不带依赖的jar包名称为wc.jar，并拷贝该jar包到Hadoop集群。<br>（2）启动Hadoop集群<br>（3）执行WordCount程序</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 software<span class="token punctuation">]</span>$ hadoop jar  wc.jar
 com.molly.wordcount.WordcountDriver /user/molly/input /user/molly/output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>7）在Windows上向集群提交任务<br>（1）添加必要配置信息</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordcountDriver</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> ClassNotFoundException<span class="token punctuation">,</span> InterruptedException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 1 获取配置信息以及封装任务</span>
        Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">//设置HDFS NameNode的地址</span>
       configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"fs.defaultFS"</span><span class="token punctuation">,</span> <span class="token string">"hdfs://hadoop102:9820"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">// 指定MapReduce运行在Yarn上</span>
       configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"mapreduce.framework.name"</span><span class="token punctuation">,</span><span class="token string">"yarn"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">// 指定mapreduce可以在远程集群运行</span>
       configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"mapreduce.app-submission.cross-platform"</span><span class="token punctuation">,</span><span class="token string">"true"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">//指定Yarn resourcemanager的位置</span>
    configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"yarn.resourcemanager.hostname"</span><span class="token punctuation">,</span><span class="token string">"hadoop103"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        Job job <span class="token operator">=</span> Job<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span>configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 2 设置jar加载路径</span>
    job<span class="token punctuation">.</span><span class="token function">setJar</span><span class="token punctuation">(</span><span class="token string">"F:\\idea_project\\main\\bigdata1214\\MapReduce\\target\\MapReduce-1.0-SNAPSHOT.jar"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 3 设置map和reduce类</span>
        job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>WordcountMapper<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span>WordcountReducer<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 4 设置map输出</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span>IntWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 5 设置最终输出kv类型</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputValueClass</span><span class="token punctuation">(</span>IntWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 6 设置输入和输出路径</span>
        FileInputFormat<span class="token punctuation">.</span><span class="token function">setInputPaths</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        FileOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 7 提交</span>
        <span class="token keyword">boolean</span> result <span class="token operator">=</span> job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>result <span class="token operator">?</span> <span class="token number">0</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）编辑任务配置<br>1）检查第一个参数Main class是不是我们要运行的类的全类名，如果不是的话一定要修改！<br>2）在VM options后面加上 ：-DHADOOP_USER_NAME=molly<br>3）在Program arguments后面加上两个参数分别代表输入输出路径，两个参数之间用空格隔开。如：hdfs://hadoop102:9820/input hdfs://hadoop102:9820/output</p><p>（3）打包，并将Jar包设置到Driver中</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordcountDriver</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> ClassNotFoundException<span class="token punctuation">,</span> InterruptedException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 1 获取配置信息以及封装任务</span>
        Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

       configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"fs.defaultFS"</span><span class="token punctuation">,</span> <span class="token string">"hdfs://hadoop102:9820"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
       configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"mapreduce.framework.name"</span><span class="token punctuation">,</span><span class="token string">"yarn"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
       configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"mapreduce.app-submission.cross-platform"</span><span class="token punctuation">,</span><span class="token string">"true"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
       configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"yarn.resourcemanager.hostname"</span><span class="token punctuation">,</span><span class="token string">"hadoop103"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        Job job <span class="token operator">=</span> Job<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span>configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 2 设置jar加载路径</span>
<span class="token comment" spellcheck="true">//job.setJarByClass(WordCountDriver.class);</span>
        job<span class="token punctuation">.</span><span class="token function">setJar</span><span class="token punctuation">(</span><span class="token string">"D:\IdeaProjects\mapreduce\target\mapreduce-1.0-SNAPSHOT.jar"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 3 设置map和reduce类</span>
        job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>WordcountMapper<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span>WordcountReducer<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 4 设置map输出</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span>IntWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 5 设置最终输出kv类型</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputValueClass</span><span class="token punctuation">(</span>IntWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 6 设置输入和输出路径</span>
        FileInputFormat<span class="token punctuation">.</span><span class="token function">setInputPaths</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        FileOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment" spellcheck="true">// 7 提交</span>
        <span class="token keyword">boolean</span> result <span class="token operator">=</span> job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>result <span class="token operator">?</span> <span class="token number">0</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（4）提交并查看结果</p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul></div><span class="post-count">总字数2.7k</span> <span class="post-count">预计阅读12分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-bigdata-hdfs3-framework" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/12/bigdata-hdfs3-framework/" class="article-date"><time class="published" datetime="2020-11-12T06:43:49.000Z" itemprop="datePublished">2020-11-12 发布</time> <time class="updated" datetime="2021-12-01T07:02:15.881Z" itemprop="dateUpdated">2021-12-01 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/11/12/bigdata-hdfs3-framework/">Hadoop 教程（三）hdfs-架构解析</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="1-HDFS的数据流"><a href="#1-HDFS的数据流" class="headerlink" title="1 HDFS的数据流"></a>1 HDFS的数据流</h1><h2 id="1-1-HDFS写数据流程"><a href="#1-1-HDFS写数据流程" class="headerlink" title="1.1 HDFS写数据流程"></a>1.1 HDFS写数据流程</h2><h3 id="1-1-1-剖析文件写入"><a href="#1-1-1-剖析文件写入" class="headerlink" title="1.1.1 剖析文件写入"></a>1.1.1 剖析文件写入</h3><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638341373066.png" alt="1638341373066"></p><p>（1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。<br>（2）NameNode返回是否可以上传。<br>（3）客户端请求第一个 Block上传到哪几个DataNode服务器上。<br>（4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。<br>（5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。<br>（6）dn1、dn2、dn3逐级应答客户端。<br>（7）客户<strong>端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</strong><br>（8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。<br>源码解析：org.apache.hadoop.hdfs.DFSOutputStream</p><h3 id="1-1-2-网络拓扑-节点距离计算"><a href="#1-1-2-网络拓扑-节点距离计算" class="headerlink" title="1.1.2 网络拓扑-节点距离计算"></a>1.1.2 网络拓扑-节点距离计算</h3><p>​ 在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。那么这个最近距离怎么计算呢？<br>节点距离：两个节点到达最近的共同祖先的距离总和。</p><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638341407745.png" alt="1638341407745"></p><p>例如，假设有数据中心d1机架r1中的节点n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。<br>大家算一算每两个节点之间的距离。</p><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638341416287.png" alt="1638341416287"></p><h3 id="1-1-3-机架感知（副本存储节点选择）"><a href="#1-1-3-机架感知（副本存储节点选择）" class="headerlink" title="1.1.3 机架感知（副本存储节点选择）"></a>1.1.3 机架感知（副本存储节点选择）</h3><p><strong>1）官方IP地址</strong><br>机架感知说明<br><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication">http://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication</a><br>For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on the local machine if the writer is on a datanode, otherwise on a random datanode, another replica on a node in a different (remote) rack, and the last on a different node in the same remote rack. This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a file do not evenly distribute across the racks. One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks. This policy improves write performance without compromising data reliability or read performance.<br><strong>2）Hadoop3.1.3副本节点选择</strong></p><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638341452275.png" alt="1638341452275"></p><h2 id="1-2-HDFS读数据流程"><a href="#1-2-HDFS读数据流程" class="headerlink" title="1.2 HDFS读数据流程"></a>1.2 HDFS读数据流程</h2><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638341472416.png" alt="1638341472416"></p><p>（1）客户端通过DistributedFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。<br>（2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。<br>（3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。<br>（4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</p><h1 id="2-NameNode和SecondaryNameNode（面试开发重点）"><a href="#2-NameNode和SecondaryNameNode（面试开发重点）" class="headerlink" title="2 NameNode和SecondaryNameNode（面试开发重点）"></a>2 NameNode和SecondaryNameNode（面试开发重点）</h1><h2 id="2-1-NN和2NN工作机制"><a href="#2-1-NN和2NN工作机制" class="headerlink" title="2.1 NN和2NN工作机制"></a>2.1 NN和2NN工作机制</h2><p>思考：NameNode中的元数据是存储在哪里的？<br>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了<strong>。因此产生在磁盘中备份元数据的FsImage</strong>。<br>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。<br>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。<strong>因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并</strong>。</p><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638341524914.png" alt="1638341524914"></p><p><strong>1）第一阶段：NameNode启动</strong><br>（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br>（2）客户端对元数据进行增删改的请求。<br>（3）NameNode记录操作日志，更新滚动日志。<br>（4）NameNode在内存中对元数据进行增删改。<br><strong>2）第二阶段：Secondary NameNode工作</strong><br>（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。<br>（2）Secondary NameNode请求执行CheckPoint。<br>（3）NameNode滚动正在写的Edits日志。<br>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。<br>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。<br>（6）生成新的镜像文件fsimage.chkpoint。<br>（7）拷贝fsimage.chkpoint到NameNode。<br>（8）NameNode将fsimage.chkpoint重新命名成fsimage。</p><p><strong>NN和2NN工作机制详解：</strong><br>Fsimage：NameNode内存中元数据序列化后形成的文件。<br>Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。<br>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。<br>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。<br>SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</p><h2 id="2-2-Fsimage和Edits解析"><a href="#2-2-Fsimage和Edits解析" class="headerlink" title="2.2 Fsimage和Edits解析"></a>2.2 Fsimage和Edits解析</h2><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638341553613.png" alt="1638341553613"></p><p>1）oiv查看Fsimage文件<br>（1）查看oiv和oev命令</p><p>（2）基本语法<br>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径<br>（3）案例实操</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 current<span class="token punctuation">]</span>$ <span class="token function">pwd</span>
/opt/module/hadoop-3.1.3/data/dfs/name/current

<span class="token punctuation">[</span>molly@hadoop102 current<span class="token punctuation">]</span>$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-3.1.3/fsimage.xml

<span class="token punctuation">[</span>molly@hadoop102 current<span class="token punctuation">]</span>$ <span class="token function">cat</span> /opt/module/hadoop-3.1.3/fsimage.xml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>将显示的xml文件内容拷贝到IDEA中创建的xml文件中，并格式化。部分显示结果如下。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>inode</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>16386<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>type</span><span class="token punctuation">></span></span>DIRECTORY<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>type</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>user<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>mtime</span><span class="token punctuation">></span></span>1512722284477<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>mtime</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>permission</span><span class="token punctuation">></span></span>molly:supergroup:rwxr-xr-x<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>permission</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>nsquota</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>nsquota</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dsquota</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dsquota</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>inode</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>inode</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>16387<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>type</span><span class="token punctuation">></span></span>DIRECTORY<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>type</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>molly<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>mtime</span><span class="token punctuation">></span></span>1512790549080<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>mtime</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>permission</span><span class="token punctuation">></span></span>molly:supergroup:rwxr-xr-x<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>permission</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>nsquota</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>nsquota</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dsquota</span><span class="token punctuation">></span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dsquota</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>inode</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>inode</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>16389<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>type</span><span class="token punctuation">></span></span>FILE<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>type</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>wc.input<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replication</span><span class="token punctuation">></span></span>3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replication</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>mtime</span><span class="token punctuation">></span></span>1512722322219<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>mtime</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>atime</span><span class="token punctuation">></span></span>1512722321610<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>atime</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>perferredBlockSize</span><span class="token punctuation">></span></span>134217728<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>perferredBlockSize</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>permission</span><span class="token punctuation">></span></span>molly:supergroup:rw-r--r--<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>permission</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>blocks</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>block</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>1073741825<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>genstamp</span><span class="token punctuation">></span></span>1001<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>genstamp</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>numBytes</span><span class="token punctuation">></span></span>59<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>numBytes</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>block</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>blocks</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>inode</span> <span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>思考：可以看出，Fsimage中没有记录块所对应DataNode，为什么？<br>在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。<br>2）oev查看Edits文件<br>（1）基本语法<br>hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径<br>（2）案例实操</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 current<span class="token punctuation">]</span>$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-3.1.3/edits.xml

<span class="token punctuation">[</span>molly@hadoop102 current<span class="token punctuation">]</span>$ <span class="token function">cat</span> /opt/module/hadoop-3.1.3/edits.xml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>将显示的xml文件内容拷贝到Eclipse中创建的xml文件中，并格式化。显示结果如下。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>EDITS</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>EDITS_VERSION</span><span class="token punctuation">></span></span>-63<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>EDITS_VERSION</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RECORD</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OPCODE</span><span class="token punctuation">></span></span>OP_START_LOG_SEGMENT<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OPCODE</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>DATA</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TXID</span><span class="token punctuation">></span></span>129<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>TXID</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>DATA</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RECORD</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RECORD</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OPCODE</span><span class="token punctuation">></span></span>OP_ADD<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OPCODE</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>DATA</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TXID</span><span class="token punctuation">></span></span>130<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>TXID</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>LENGTH</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>LENGTH</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>INODEID</span><span class="token punctuation">></span></span>16407<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>INODEID</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PATH</span><span class="token punctuation">></span></span>/hello7.txt<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PATH</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>REPLICATION</span><span class="token punctuation">></span></span>2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>REPLICATION</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>MTIME</span><span class="token punctuation">></span></span>1512943607866<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>MTIME</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ATIME</span><span class="token punctuation">></span></span>1512943607866<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ATIME</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCKSIZE</span><span class="token punctuation">></span></span>134217728<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCKSIZE</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>CLIENT_NAME</span><span class="token punctuation">></span></span>DFSClient_NONMAPREDUCE_-1544295051_1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>CLIENT_NAME</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>CLIENT_MACHINE</span><span class="token punctuation">></span></span>192.168.1.5<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>CLIENT_MACHINE</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OVERWRITE</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OVERWRITE</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PERMISSION_STATUS</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>USERNAME</span><span class="token punctuation">></span></span>molly<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>USERNAME</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>GROUPNAME</span><span class="token punctuation">></span></span>supergroup<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>GROUPNAME</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>MODE</span><span class="token punctuation">></span></span>420<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>MODE</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PERMISSION_STATUS</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RPC_CLIENTID</span><span class="token punctuation">></span></span>908eafd4-9aec-4288-96f1-e8011d181561<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RPC_CLIENTID</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RPC_CALLID</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RPC_CALLID</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>DATA</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RECORD</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RECORD</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OPCODE</span><span class="token punctuation">></span></span>OP_ALLOCATE_BLOCK_ID<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OPCODE</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>DATA</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TXID</span><span class="token punctuation">></span></span>131<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>TXID</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCK_ID</span><span class="token punctuation">></span></span>1073741839<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCK_ID</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>DATA</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RECORD</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RECORD</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OPCODE</span><span class="token punctuation">></span></span>OP_SET_GENSTAMP_V2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OPCODE</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>DATA</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TXID</span><span class="token punctuation">></span></span>132<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>TXID</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>GENSTAMPV2</span><span class="token punctuation">></span></span>1016<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>GENSTAMPV2</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>DATA</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RECORD</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RECORD</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OPCODE</span><span class="token punctuation">></span></span>OP_ADD_BLOCK<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OPCODE</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>DATA</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TXID</span><span class="token punctuation">></span></span>133<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>TXID</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PATH</span><span class="token punctuation">></span></span>/hello7.txt<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PATH</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCK</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCK_ID</span><span class="token punctuation">></span></span>1073741839<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCK_ID</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>NUM_BYTES</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>NUM_BYTES</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>GENSTAMP</span><span class="token punctuation">></span></span>1016<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>GENSTAMP</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCK</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RPC_CLIENTID</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RPC_CLIENTID</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RPC_CALLID</span><span class="token punctuation">></span></span>-2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RPC_CALLID</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>DATA</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RECORD</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RECORD</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OPCODE</span><span class="token punctuation">></span></span>OP_CLOSE<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OPCODE</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>DATA</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TXID</span><span class="token punctuation">></span></span>134<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>TXID</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>LENGTH</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>LENGTH</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>INODEID</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>INODEID</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PATH</span><span class="token punctuation">></span></span>/hello7.txt<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PATH</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>REPLICATION</span><span class="token punctuation">></span></span>2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>REPLICATION</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>MTIME</span><span class="token punctuation">></span></span>1512943608761<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>MTIME</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ATIME</span><span class="token punctuation">></span></span>1512943607866<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ATIME</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCKSIZE</span><span class="token punctuation">></span></span>134217728<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCKSIZE</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>CLIENT_NAME</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>CLIENT_NAME</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>CLIENT_MACHINE</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>CLIENT_MACHINE</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OVERWRITE</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OVERWRITE</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCK</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCK_ID</span><span class="token punctuation">></span></span>1073741839<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCK_ID</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>NUM_BYTES</span><span class="token punctuation">></span></span>25<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>NUM_BYTES</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>GENSTAMP</span><span class="token punctuation">></span></span>1016<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>GENSTAMP</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCK</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PERMISSION_STATUS</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>USERNAME</span><span class="token punctuation">></span></span>molly<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>USERNAME</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>GROUPNAME</span><span class="token punctuation">></span></span>supergroup<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>GROUPNAME</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>MODE</span><span class="token punctuation">></span></span>420<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>MODE</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PERMISSION_STATUS</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>DATA</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RECORD</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>EDITS</span> <span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>思考：NameNode如何确定下次开机启动的时候合并哪些Edits？</p><h2 id="2-3-CheckPoint时间设置"><a href="#2-3-CheckPoint时间设置" class="headerlink" title="2.3 CheckPoint时间设置"></a>2.3 CheckPoint时间设置</h2><p>1）通常情况下，SecondaryNameNode每隔一小时执行一次。<br>[hdfs-default.xml]</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.checkpoint.period<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>3600s<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.checkpoint.txns<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>1000000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>操作动作次数<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.checkpoint.check.period<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>60s<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span> 1分钟检查一次操作次数<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span> <span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-4-NameNode故障处理（扩展）"><a href="#2-4-NameNode故障处理（扩展）" class="headerlink" title="2.4 NameNode故障处理（扩展）"></a>2.4 NameNode故障处理（扩展）</h2><p>NameNode故障后，可以采用如下两种方法恢复数据。<br>1）将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；<br>（1）kill -9 NameNode进程<br>（2）删除NameNode存储的数据（/opt/module/hadoop-3.1.3/data/tmp/dfs/name）</p><pre class="line-numbers language-basn"><code class="language-basn">[molly@hadoop102 hadoop-3.1.3]$ rm -rf /opt/module/hadoop-3.1.3/data/dfs/name/*<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）拷贝SecondaryNameNode中数据到原NameNode存储数据目录</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 dfs<span class="token punctuation">]</span>$ <span class="token function">scp</span> -r molly@hadoop104:/opt/module/hadoop-3.1.3/data/dfs/namesecondary/* ./name/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）重新启动NameNode</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hdfs --daemon start namenode<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。<br>（1）修改hdfs-site.xml中的</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.checkpoint.period<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>120<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.name.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/opt/module/hadoop-3.1.3/data/dfs/name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）kill -9 NameNode进程<br>（3）删除NameNode存储的数据（/opt/module/hadoop-3.1.3/data/dfs/name）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">rm</span> -rf /opt/module/hadoop-3.1.3/data/dfs/name/*<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 dfs<span class="token punctuation">]</span>$ <span class="token function">scp</span> -r molly@hadoop104:/opt/module/hadoop-3.1.3/data/dfs/namesecondary ./

<span class="token punctuation">[</span>molly@hadoop102 namesecondary<span class="token punctuation">]</span>$ <span class="token function">rm</span> -rf in_use.lock

<span class="token punctuation">[</span>molly@hadoop102 dfs<span class="token punctuation">]</span>$ <span class="token function">pwd</span>
/opt/module/hadoop-3.1.3/data/dfs

<span class="token punctuation">[</span>molly@hadoop102 dfs<span class="token punctuation">]</span>$ <span class="token function">ls</span>
data  name  namesecondary<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（5）导入检查点数据（等待一会ctrl+c结束掉）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ bin/hdfs namenode -importCheckpoint<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（6）启动NameNode</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hdfs --daemon start namenode<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-5-集群安全模式"><a href="#2-5-集群安全模式" class="headerlink" title="2.5 集群安全模式"></a>2.5 集群安全模式</h2><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638341980899.png" alt="1638341980899"></p><p>1）基本语法<br>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。<br>（1）bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态）<br>（2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态）<br>（3）bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态）<br>（4）bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态）<br>2）案例<br>模拟等待安全模式<br>3）查看当前模式</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hdfs dfsadmin -safemode get
Safe mode is OFF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>4）先进入安全模式</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ bin/hdfs dfsadmin -safemode enter<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）创建并执行下面的脚本<br>在/opt/module/hadoop-3.1.3路径上，编辑一个脚本safemode.sh</p><pre class="line-numbers language-sh"><code class="language-sh">[molly@hadoop102 hadoop-3.1.3]$ touch safemode.sh
[molly@hadoop102 hadoop-3.1.3]$ vim safemode.sh

#!/bin/bash
hdfs dfsadmin -safemode wait
hdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">chmod</span> 777 safemode.sh

<span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ ./safemode.sh <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>6）再打开一个窗口，执行</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ bin/hdfs dfsadmin -safemode leave<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>7）观察<br>8）再观察上一个窗口<br>Safe mode is OFF<br>9）HDFS集群上已经有上传的数据了。</p><h2 id="2-6-NameNode多目录配置-了解"><a href="#2-6-NameNode多目录配置-了解" class="headerlink" title="2.6 NameNode多目录配置(了解)"></a>2.6 NameNode多目录配置(了解)</h2><p>1）NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性<br>2）具体配置如下<br>（1）在hdfs-site.xml文件中添加如下内容</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.name.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>file://$<span class="token entity" title="&#123;">&amp;#123;</span>hadoop.tmp.dir<span class="token entity" title="&#125;">&amp;#125;</span>/dfs/name1,file://$<span class="token entity" title="&#123;">&amp;#123;</span>hadoop.tmp.dir<span class="token entity" title="&#125;">&amp;#125;</span>/dfs/name2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>（2）停止集群，删除三台节点的data和logs中所有数据。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">rm</span> -rf data/ logs/
<span class="token punctuation">[</span>molly@hadoop103 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">rm</span> -rf data/ logs/
<span class="token punctuation">[</span>molly@hadoop104 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">rm</span> -rf data/ logs/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（3）格式化集群并启动。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ bin/hdfs namenode –format
<span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-dfs.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（4）查看结果</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 dfs<span class="token punctuation">]</span>$ ll
总用量 12
drwx------. 3 molly molly 4096 12月 11 08:03 data
drwxrwxr-x. 3 molly molly 4096 12月 11 08:03 name1
drwxrwxr-x. 3 molly molly 4096 12月 11 08:03 name2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-DataNode（面试开发重点）"><a href="#3-DataNode（面试开发重点）" class="headerlink" title="3 DataNode（面试开发重点）"></a>3 DataNode（面试开发重点）</h1><h2 id="3-1-DataNode工作机制"><a href="#3-1-DataNode工作机制" class="headerlink" title="3.1 DataNode工作机制"></a>3.1 DataNode工作机制</h2><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638342000541.png" alt="1638342000541"></p><p>（1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。<br>（2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。<br>（3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。<br>（4）集群运行中可以安全加入和退出一些机器。</p><h2 id="3-2-数据完整性"><a href="#3-2-数据完整性" class="headerlink" title="3.2 数据完整性"></a>3.2 数据完整性</h2><p>思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？<br>如下是DataNode节点保证数据完整性的方法。<br>（1）当DataNode读取Block的时候，它会计算CheckSum。<br>（2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。<br>（3）Client读取其他DataNode上的Block。<br>（4）常见的校验算法 crc（32），md5（128），sha1（160）<br>（5）DataNode在其文件创建后周期验证CheckSum。</p><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638342020071.png" alt="1638342020071"></p><h2 id="3-3-掉线时限参数设置"><a href="#3-3-掉线时限参数设置" class="headerlink" title="3.3 掉线时限参数设置"></a>3.3 掉线时限参数设置</h2><p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638342040415.png" alt="1638342040415"></p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.heartbeat.recheck-interval<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>300000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.heartbeat.interval<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-4-服役新数据节点"><a href="#3-4-服役新数据节点" class="headerlink" title="3.4 服役新数据节点"></a>3.4 服役新数据节点</h2><p>1）需求<br>随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。<br>2）环境准备<br>（1）在hadoop104主机上再克隆一台hadoop105主机<br>（2）修改IP地址和主机名称<br>（3）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-3.1.3/data和logs）<br>（4）source一下配置文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop105 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">source</span> /etc/profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）服役新节点具体步骤<br>（1）直接启动DataNode，即可关联到集群</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop105 hadoop-3.1.3<span class="token punctuation">]</span>$ hdfs --daemon start datanode
<span class="token punctuation">[</span>molly@hadoop105 hadoop-3.1.3<span class="token punctuation">]</span>$ yarn --daemon start nodemanager<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638342075539.png" alt="1638342075539"></p><p>（2）在hadoop105上上传文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop105 hadoop-3.1.3<span class="token punctuation">]</span>$ hadoop fs -put /opt/module/hadoop-3.1.3/LICENSE.txt /<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）如果数据不均衡，可以用命令实现集群的再平衡</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 sbin<span class="token punctuation">]</span>$ ./start-balancer.sh
starting balancer, logging to /opt/module/hadoop-3.1.3/logs/hadoop-molly-balancer-hadoop102.out
Time Stamp               Iteration<span class="token comment" spellcheck="true">#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="3-5-退役旧数据节点"><a href="#3-5-退役旧数据节点" class="headerlink" title="3.5 退役旧数据节点"></a>3.5 退役旧数据节点</h2><h3 id="3-5-1-添加白名单和黑名单"><a href="#3-5-1-添加白名单和黑名单" class="headerlink" title="3.5.1 添加白名单和黑名单"></a>3.5.1 添加白名单和黑名单</h3><p>白名单和黑名单是hadoop管理集群主机的一种机制。<br>添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。添加到黑名单的主机节点，不允许访问NameNode，会在数据迁移后退出。<br>实际情况下，白名单用于确定允许访问NameNode的DataNode节点，内容配置一般与workers文件内容一致。 黑名单用于在集群运行过程中退役DataNode节点。<br>配置白名单和黑名单的具体步骤如下：<br>1）在NameNode节点的/opt/module/hadoop-3.1.3/etc/hadoop目录下分别创建whitelist 和blacklist文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop<span class="token punctuation">]</span>$ <span class="token function">pwd</span>
/opt/module/hadoop-3.1.3/etc/hadoop
<span class="token punctuation">[</span>molly@hadoop102 hadoop<span class="token punctuation">]</span>$ <span class="token function">touch</span> whitelist
<span class="token punctuation">[</span>molly@hadoop102 hadoop<span class="token punctuation">]</span>$ <span class="token function">touch</span> blacklist
<span class="token comment" spellcheck="true">#在whitelist中添加如下主机名称,假如集群正常工作的节点为102 103 104 105</span>
hadoop102
hadoop103
hadoop104
hadoop105<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>黑名单暂时为空。<br>2）在hdfs-site.xml配置文件中增加dfs.hosts和 dfs.hosts.exclude配置参数</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token comment" spellcheck="true">&lt;!-- 白名单 --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.hosts<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!-- 黑名单 --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.hosts.exclude<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）分发配置文件whitelist，blacklist，hdfs-site.xml (注意：105节点也要发一份)</p><pre class="line-numbers language-xml"><code class="language-xml">[molly@hadoop102 etc]$ xsync hadoop/ 
[molly@hadoop102 etc]$ rsync -av hadoop/ molly@hadoop105:/opt/module/hadoop-3.1.3/etc/hadoop/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>4）重新启动集群(注意：105节点没有添加到workers，因此要单独起停)</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ stop-dfs.sh
<span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ start-dfs.sh
<span class="token punctuation">[</span>molly@hadoop105 hadoop-3.1.3<span class="token punctuation">]</span>$ hdfs –daemon start datanode<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>5）在web浏览器上查看目前正常工作的DN节点</p><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638342091569.png" alt="1638342091569"></p><h3 id="3-5-2-黑名单退役"><a href="#3-5-2-黑名单退役" class="headerlink" title="3.5.2 黑名单退役"></a>3.5.2 黑名单退役</h3><p>1）编辑/opt/module/hadoop-3.1.3/etc/hadoop目录下的blacklist文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop<span class="token punctuation">]</span> vim blacklist<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加如下主机名称（要退役的节点）</p><pre class="line-numbers language-bash"><code class="language-bash">hadoop105<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）分发blacklist到所有节点</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 etc<span class="token punctuation">]</span>$ xsync hadoop/ 
<span class="token punctuation">[</span>molly@hadoop102 etc<span class="token punctuation">]</span>$ <span class="token function">rsync</span> -av hadoop/ molly@hadoop105:/opt/module/hadoop-3.1.3/etc/hadoop/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>3）刷新NameNode、刷新ResourceManager</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ hdfs dfsadmin -refreshNodes
Refresh nodes successful

<span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ yarn rmadmin -refreshNodes
17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4）检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点</p><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638342103632.png" alt="1638342103632"></p><p>5）等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役</p><p><img src="/2020/11/12/bigdata-hdfs3-framework/1638342110735.png" alt="1638342110735"></p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop105 hadoop-3.1.3<span class="token punctuation">]</span>$ hdfs --daemon stop datanode
stopping datanode
<span class="token punctuation">[</span>molly@hadoop105 hadoop-3.1.3<span class="token punctuation">]</span>$ yarn --daemon stop nodemanager
stopping nodemanager<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>6）如果数据不均衡，可以用命令实现集群的再平衡</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-balancer.sh 
starting balancer, logging to /opt/module/hadoop-3.1.3/logs/hadoop-molly-balancer-hadoop102.out
Time Stamp               Iteration<span class="token comment" spellcheck="true">#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>注意：不允许白名单和黑名单中同时出现同一个主机名称，既然使用了黑名单blacklist成功退役了hadoop105节点，因此要将白名单whitelist里面的hadoop105去掉。</p><h2 id="3-6-DataNode多目录配置"><a href="#3-6-DataNode多目录配置" class="headerlink" title="3.6 DataNode多目录配置"></a>3.6 DataNode多目录配置</h2><p>1）DataNode可以配置成多个目录，<strong>每个目录存储的数据不一样</strong>。即：数据不是副本<br>2）具体配置如下<br>（1）在hdfs-site.xml文件中添加如下内容</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.datanode.data.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>file://$<span class="token entity" title="&#123;">&amp;#123;</span>hadoop.tmp.dir<span class="token entity" title="&#125;">&amp;#125;</span>/dfs/data1,file://$<span class="token entity" title="&#123;">&amp;#123;</span>hadoop.tmp.dir<span class="token entity" title="&#125;">&amp;#125;</span>/dfs/data2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>（2）停止集群，删除三台节点的data和logs中所有数据。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">rm</span> -rf data/ logs/
<span class="token punctuation">[</span>molly@hadoop103 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">rm</span> -rf data/ logs/
<span class="token punctuation">[</span>molly@hadoop104 hadoop-3.1.3<span class="token punctuation">]</span>$ <span class="token function">rm</span> -rf data/ logs/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（3）格式化集群并启动。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ bin/hdfs namenode –format
<span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-dfs.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（4）查看结果</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 dfs<span class="token punctuation">]</span>$ ll
总用量 12
drwx------. 3 molly molly 4096 4月   4 14:22 data1
drwx------. 3 molly molly 4096 4月   4 14:22 data2
drwxrwxr-x. 3 molly molly 4096 12月 11 08:03 name1
drwxrwxr-x. 3 molly molly 4096 12月 11 08:03 name2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div><div class="article-info article-info-index"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul></div><span class="post-count">总字数5.4k</span> <span class="post-count">预计阅读24分钟</span></div><div class="clearfix"></div></div></div></article><article id="post-cipher-certificate-format" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/10/10/cipher-certificate-format/" class="article-date"><time class="published" datetime="2020-10-10T06:56:33.000Z" itemprop="datePublished">2020-10-10 发布</time> <time class="updated" datetime="2021-02-19T08:09:25.012Z" itemprop="dateUpdated">2021-02-19 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/10/10/cipher-certificate-format/">证书的各种格式</a></h1></header><div class="article-entry" itemprop="articleBody"><p>待完善</p></div><div class="article-info article-info-index"><div class="article-count"><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/" rel="tag">密码学</a></li></ul></div><span class="post-count">总字数98</span> <span class="post-count">预计阅读1分钟</span></div><p class="article-more-link"><a href="/2020/10/10/cipher-certificate-format/#more">阅读全文 >></a></p><div class="clearfix"></div></div></div></article><article id="post-docker-guide" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/09/28/docker-guide/" class="article-date"><time class="published" datetime="2020-09-28T07:22:20.000Z" itemprop="datePublished">2020-09-28 发布</time> <time class="updated" datetime="2021-11-23T09:11:41.252Z" itemprop="dateUpdated">2021-11-23 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 itemprop="name"><a class="article-title" href="/2020/09/28/docker-guide/">docker使用大全</a></h1></header><div class="article-entry" itemprop="articleBody"><p>自己检索方便</p></div><div class="article-info article-info-index"><div class="article-count"><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8%E6%95%99%E7%A8%8B/" rel="tag">容器教程</a></li></ul></div><span class="post-count">总字数1.1k</span> <span class="post-count">预计阅读4分钟</span></div><p class="article-more-link"><a href="/2020/09/28/docker-guide/#more">阅读全文 >></a></p><div class="clearfix"></div></div></div></article><nav id="page-nav"><a class="extend prev" rel="prev" href="/page/4/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/6/">Next &amp;raquo;</a></nav></div><footer id="footer"><div class="outer"><div id="footer-info"><div class="footer-left"><i class="fa fa-copyright"></i> 2017-2021 冀-18010769-1</div><div class="visit"><span id="busuanzi_container_site_pv" style="display:none"><span id="site-visit" title="本站到访人数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span> </span></span><span>| </span><span id="busuanzi_container_page_pv" style="display:none"><span id="page-visit" title="本站总访问量"><i class="fa fa-eye" aria-hidden="true"></i><span id="busuanzi_value_site_pv"></span></span></span></div><div class="footer-right"><i class="fa fa-heart"></i><a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架"> Hexo</a> Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a></div></div></div></footer></div><script type="application/javascript">var leftWidth,hide=!1;$(".hide-left-col").click(function(){hide=hide?($(".left-col").css("width",leftWidth),$(".left-col .intrude-less").fadeIn(200),$("#tocButton").fadeIn(200),"block"===$("#switch-btn").css("display")&&"block"===$("#switch-area").css("display")||$("#toc").fadeIn(200),$(".hide-left-col").css("left",leftWidth).html('<i class="fa fa-angle-double-left"></i>'),$(".mid-col").css("left",leftWidth),$("#post-nav-button").css("left",leftWidth),$("#post-nav-button > a:nth-child(2)").css("display","block"),!1):(leftWidth=$(".left-col")[0].style.width,$(".left-col").css("width",0),$(".left-col .intrude-less").fadeOut(200),$("#toc").fadeOut(100),$("#tocButton").fadeOut(100),$(".hide-left-col").css("left",0).html('<i class="fa fa-angle-double-right"></i>'),$(".mid-col").css("left",0),$("#post-nav-button").css("left",0),$("#post-nav-button > a:nth-child(2)").css("display","none"),$(".post-list").is(":visible")&&($("#post-nav-button .fa-bars,#post-nav-button .fa-times").toggle(),$(".post-list").toggle()),!0)})</script><script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.3.5/require.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[["$$","$$"],["$","$"],["\\(","\\)"]],processEscapes:!0,skipTags:["script","noscript","style","textarea","pre","code"]}}),MathJax.Hub.Queue(function(){var a,e=MathJax.Hub.getAllJax();for(a=0;a<e.length;a+=1)e[a].SourceElement().parentNode.className+=" has-jax"})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script><div class="scroll" id="scroll"><a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a> <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a> <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a></div><script>var oOpenInNew={post:".copyright a[href]",friends:"#js-friends a",socail:".social a"};for(var x in oOpenInNew)$(oOpenInNew[x]).attr("target","_blank")</script><script>var titleTime,originTitle=document.title;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="(つェ⊂)"+originTitle,clearTimeout(titleTime)):(document.title="(*´∇｀*)~ "+originTitle,titleTime=setTimeout(function(){document.title=originTitle},2e3))})</script><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><link href="//cdn.bootcss.com/aos/2.2.0/aos.css" rel="stylesheet"><script type="text/javascript">AOS.init({easing:"ease-out-back",once:!0})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"live2d_models/live2d-widget-model-izumi"},"display":{"position":"right","width":100,"height":200,"hOffset":-50,"vOffset":-85},"mobile":{"show":false},"react":{"opacityDefault":0.9,"opacityOnHover":0.3},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false});</script></body></html>