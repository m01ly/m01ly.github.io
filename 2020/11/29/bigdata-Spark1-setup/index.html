<!DOCTYPE html><html lang="zh-Hans"><head><!--[if IE]><style>body{display:none;}</style><script>alert('IE浏览器下无法展示效果，请更换浏览器！');var headNode=document.getElementsByTagName('head')[0];var refresh=document.createElement('meta');refresh.setAttribute('http-equiv','Refresh');refresh.setAttribute('Content','0; url=http://outdatedbrowser.com/');headNode.appendChild(refresh);</script><![endif]--><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="m01ly"><meta name="description" content="1 Spark概述1.1 spark or haddopMR框架主要应用于数据的一次性计算：存存储介质中读取数据，然后进行过处理后，再存储到文件中。IO读取多，效率低。 1）Spark是基于MR框架的，但是优化了其中的计算过程，使用内存来代替计算结果。减少了磁盘IO，因此快。（MR多作业之间会多次磁盘的IO，因此慢） 2）Spark基于Scala语言开发的，更适合迭代计算和数据挖掘计算 3） Sp"><meta property="og:type" content="article"><meta property="og:title" content="Spark学习笔记（一） 搭建Spark"><meta property="og:url" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/index.html"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="1 Spark概述1.1 spark or haddopMR框架主要应用于数据的一次性计算：存存储介质中读取数据，然后进行过处理后，再存储到文件中。IO读取多，效率低。 1）Spark是基于MR框架的，但是优化了其中的计算过程，使用内存来代替计算结果。减少了磁盘IO，因此快。（MR多作业之间会多次磁盘的IO，因此慢） 2）Spark基于Scala语言开发的，更适合迭代计算和数据挖掘计算 3） Sp"><meta property="og:locale"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638156481825.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176912041.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176561427.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176731823.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176805506.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176855314.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176879056.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176475372.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638173742050.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176427950.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176440182.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638174301463.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638174700872.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638174744996.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638175491608.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638175504305.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638175185282.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638175383495.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638175397500.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638175436542.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638175451854.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638175863107.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638175901609.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638175915913.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638175935231.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176022879.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176037995.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176059473.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176071216.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176091921.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176106974.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176202265.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638176219502.png"><meta property="article:published_time" content="2020-11-29T03:15:49.000Z"><meta property="article:modified_time" content="2021-12-02T02:41:58.591Z"><meta property="article:author" content="m01ly"><meta property="article:tag" content="Spark"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/1638156481825.png"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="shortcut icon" href="/favicon.ico"><link href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css" rel="stylesheet"><link rel="stylesheet" href="/css/style.css"><title>Spark学习笔记（一） 搭建Spark | Hexo</title><script src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"><script src="//cdn.bootcss.com/aos/2.2.0/aos.js"></script><script>var yiliaConfig={fancybox:!0,isHome:!1,isPost:!0,isArchive:!1,isTag:!1,isCategory:!1,fancybox_js:"//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js",search:!0}</script><script>yiliaConfig.jquery_ui=[!1]</script><script>yiliaConfig.rootUrl="/"</script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/rss+xml">
<link rel="stylesheet" href="/css/prism-a11y-dark.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="container"><div class="left-col"><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><form id="search-form"><input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false"> <i class="fa fa-times" onclick="resetSearch()"></i></form><div id="local-search-result"></div><p class="no-result">No results found <i class="fa fa-spinner fa-pulse"></i></p><div id="switch-btn" class="switch-btn"><div class="icon"><div class="icon-ctn"><div class="icon-wrap icon-house" data-idx="0"><div class="birdhouse"></div><div class="birdhouse_holes"></div></div><div class="icon-wrap icon-ribbon hide" data-idx="1"><div class="ribbon"></div></div><div class="icon-wrap icon-link hide" data-idx="2"><div class="loopback_l"></div><div class="loopback_r"></div></div><div class="icon-wrap icon-me hide" data-idx="3"><div class="user"></div><div class="shoulder"></div></div></div></div><div class="tips-box hide"><div class="tips-arrow"></div><ul class="tips-inner"><li>菜单</li><li>标签</li><li>友情链接</li><li>目标</li></ul></div></div><div id="switch-area" class="switch-area"><div class="switch-wrap"><section class="switch-part switch-part1"><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" rel="noopener" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" rel="noopener" href="https://music.163.com/" title="网易云音乐"></a></ul><ul class="social"><div class="donateIcon-position"><p style="display:block"><a class="donateIcon" href="javascript:void(0)" onmouseout='var qr=document.getElementById("donate");qr.style.display="none"' onmouseenter='var qr=document.getElementById("donate");qr.style.display="block"'>赏</a></p><div id="donate"><img id="multipay" src="/img/multipay.png" width="250px" alt="m01ly Multipay"><div class="triangle"></div></div></div></ul></nav></section><section class="switch-part switch-part2"><div class="widget tagcloud" id="js-tagcloud"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TLS/" rel="tag">TLS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zookeeper/" rel="tag">Zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/" rel="tag">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/" rel="tag">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BC%81%E4%B8%9A%E5%AE%89%E5%85%A8%E5%BB%BA%E8%AE%BE/" rel="tag">企业安全建设</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E5%85%A8%E6%89%AB%E6%8F%8F/" rel="tag">安全扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" rel="tag">安装教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8%E6%95%99%E7%A8%8B/" rel="tag">容器教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/" rel="tag">密码学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" rel="tag">插件开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E4%BB%93%E9%87%87%E9%9B%86%E9%A1%B9%E7%9B%AE/" rel="tag">数仓采集项目</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/" rel="tag">日志管理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/" rel="tag">流量分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" rel="tag">渗透测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%8F%E6%B4%9E%E6%89%AB%E6%8F%8F/" rel="tag">漏洞扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%BB%E5%8A%A8%E5%AE%89%E5%85%A8/" rel="tag">移动安全</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%B6%E5%9C%BAwriteup/" rel="tag">靶场writeup</a></li></ul></div></section><section class="switch-part switch-part3"><div id="js-friends"><a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/TechCatsLab">TechCatsLab</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://yangchenglong11.github.io">YangChengLong</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://jsharkc.github.io">LiuJiaChang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://blog.yusank.space">YusanKurban</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.lizebang.top">Lizebang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/sunanxiang">SunAnXiang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/DoubleWoodH">LinHao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.littlechao.top">ShiChao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/Txiaozhe">TangXiaoJi</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/LLLeon">JiaChenHui</a></div></section><section class="switch-part switch-part4"><div id="js-aboutme">不悲不喜，不卑不亢，努力成为一个更好的程序猿！</div></section></div></div></header></div></div><div class="hide-left-col" title="隐藏侧栏"><i class="fa fa-angle-double-left"></i></div><div class="mid-col"><nav id="mobile-nav"><div class="overlay"><div class="slider-trigger"></div><h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">m01ly</a></h1></div><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/" title="回到主页">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li><div class="clearfix"></div></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" href="https://music.163.com/" title="网易云音乐"></a></ul></nav></header></div><link class="menu-list" tags="标签" friends="友情链接" about="目标"></nav><div class="body-wrap"><article id="post-bigdata-Spark1-setup" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/29/bigdata-Spark1-setup/" class="article-date"><time class="published" datetime="2020-11-29T03:15:49.000Z" itemprop="datePublished">2020-11-29 发布</time> <time class="updated" datetime="2021-12-02T02:41:58.591Z" itemprop="dateUpdated">2021-12-02 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 class="article-title" itemprop="name">Spark学习笔记（一） 搭建Spark</h1></header><div class="article-info article-info-post"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li></ul></div><span class="post-count">总字数4.8k</span> <span class="post-count">预计阅读20分钟</span></div><div class="clearfix"></div></div><div class="article-entry" itemprop="articleBody"><h1 id="1-Spark概述"><a href="#1-Spark概述" class="headerlink" title="1 Spark概述"></a>1 Spark概述</h1><h2 id="1-1-spark-or-haddop"><a href="#1-1-spark-or-haddop" class="headerlink" title="1.1 spark or haddop"></a>1.1 spark or haddop</h2><p>MR框架主要应用于数据的一次性计算：存存储介质中读取数据，然后进行过处理后，再存储到文件中。IO读取多，效率低。</p><p>1）Spark是基于MR框架的，但是优化了其中的计算过程，使用内存来代替计算结果。减少了磁盘IO，因此快。（MR多作业之间会多次磁盘的IO，因此慢）</p><p>2）Spark基于Scala语言开发的，更适合迭代计算和数据挖掘计算</p><p>3） Spark中计算模型非常丰富（MR中只有两个计算模型：mapper和reducer）;spark的计算模型有：map,filter,groupby,sortby。</p><p>工作中：是Spark中和Yarn联合使用：资源用的是Yarn,计算用的是spark。</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638156481825.png" alt="1638156481825"></p><h2 id="1-2-Spark核心模块"><a href="#1-2-Spark核心模块" class="headerlink" title="1.2 Spark核心模块"></a>1.2 Spark核心模块</h2><h2 id="1-1-Spark-核心模块"><a href="#1-1-Spark-核心模块" class="headerlink" title="1.1 Spark 核心模块"></a>1.1 Spark 核心模块</h2><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176912041.png" alt="1638176912041"></p><p>1）<strong>Spark Core</strong></p><p>Spark Core中提供了Spark最基础与最核心的功能，Spark其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib都是在Spark Core的基础上进行扩展的</p><p>2） <strong>Spark SQL</strong></p><p>Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言（HQL）来查询数据。</p><ol start="3"><li><strong>Spark Streaming</strong></li></ol><p>Spark Streaming是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。</p><p>4） <strong>Spark MLlib</strong></p><p>MLlib是Spark提供的一个机器学习算法库。MLlib不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。</p><p>5）<strong>Spark GraphX</strong></p><p>GraphX是Spark面向图计算提供的框架与算法库。</p><h1 id="2-Spark快速上手"><a href="#2-Spark快速上手" class="headerlink" title="2 Spark快速上手"></a>2 Spark快速上手</h1><p>在大数据早期的课程中我们已经学习了MapReduce框架的原理及基本使用，并了解了其底层数据处理的实现方式。接下来，就让咱们走进Spark的世界，了解一下它是如何带领我们完成数据处理的。</p><h2 id="2-1-创建Maven项目"><a href="#2-1-创建Maven项目" class="headerlink" title="2.1  创建Maven项目"></a>2.1 创建Maven项目</h2><h3 id="2-1-1-增加Scala插件"><a href="#2-1-1-增加Scala插件" class="headerlink" title="2.1.1 增加Scala插件"></a>2.1.1 增加Scala插件</h3><p>Spark由Scala语言开发的，所以本课件接下来的开发所使用的语言也为Scala，咱们当前使用的Spark版本为3.0.0，默认采用的Scala编译版本为2.12，所以后续开发时。我们依然采用这个版本。开发前请保证IDEA开发工具中含有Scala开发插件。</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176561427.png" alt="1638176561427"></p><h3 id="2-1-2-增加依赖关系"><a href="#2-1-2-增加依赖关系" class="headerlink" title="2.1.2 增加依赖关系"></a>2.1.2 增加依赖关系</h3><p>修改Maven项目中的POM文件，增加Spark框架的依赖关系。本课件基于Spark3.0版本，使用时请注意对应版本。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.spark<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>spark-core_2.12<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.0.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>
        <span class="token comment" spellcheck="true">&lt;!-- 该插件用于将Scala代码编译成class文件 --></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>net.alchim31.maven<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>scala-maven-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.2.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>
                    <span class="token comment" spellcheck="true">&lt;!-- 声明绑定到maven的compile阶段 --></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>
                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>testCompile<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-assembly-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.1.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRefs</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRef</span><span class="token punctuation">></span></span>jar-with-dependencies<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRef</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRefs</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>make-assembly<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>
                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>single<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-1-3-WordCount"><a href="#2-1-3-WordCount" class="headerlink" title="2.1.3 WordCount"></a>2.1.3 WordCount</h3><p>为了能直观地感受Spark框架的效果，接下来我们实现一个大数据学科中最常见的教学案例WordCount</p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token comment" spellcheck="true">// 创建Spark运行配置对象</span>
<span class="token keyword">val</span> sparkConf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"WordCount"</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 创建Spark上下文环境对象（连接对象）</span>
<span class="token keyword">val</span> sc <span class="token operator">:</span> SparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 读取文件数据</span>
<span class="token keyword">val</span> fileRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"input/word.txt"</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 将文件中的数据进行分词</span>
<span class="token keyword">val</span> wordRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> fileRDD<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span> _<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span> <span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 转换数据结构 word => (word, 1)</span>
<span class="token keyword">val</span> word2OneRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> wordRDD<span class="token punctuation">.</span>map<span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 将转换结构后的数据按照相同的单词进行分组聚合</span>
<span class="token keyword">val</span> word2CountRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> word2OneRDD<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>_<span class="token operator">+</span>_<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 将数据聚合结果采集到内存中</span>
<span class="token keyword">val</span> word2Count<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> word2CountRDD<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 打印结果</span>
word2Count<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">//关闭Spark连接</span>
sc<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>执行过程中，会产生大量的执行日志，如果为了能够更好的查看程序的执行结果，可以在项目的resources目录中创建log4j.properties文件，并添加日志配置信息：</p><pre class="line-numbers language-properties"><code class="language-properties"><span class="token attr-name">log4j.rootCategory</span><span class="token punctuation">=</span><span class="token attr-value">ERROR, console</span>
<span class="token attr-name">log4j.appender.console</span><span class="token punctuation">=</span><span class="token attr-value">org.apache.log4j.ConsoleAppender</span>
<span class="token attr-name">log4j.appender.console.target</span><span class="token punctuation">=</span><span class="token attr-value">System.err</span>
<span class="token attr-name">log4j.appender.console.layout</span><span class="token punctuation">=</span><span class="token attr-value">org.apache.log4j.PatternLayout</span>
<span class="token attr-name">log4j.appender.console.layout.ConversionPattern</span><span class="token punctuation">=</span><span class="token attr-value">%d&amp;#123;yy/MM/dd HH:mm:ss&amp;#125; %p %c&amp;#123;1&amp;#125;: %m%n</span>

<span class="token comment" spellcheck="true"># Set the default spark-shell log level to ERROR. When running the spark-shell, the</span>
<span class="token comment" spellcheck="true"># log level for this class is used to overwrite the root logger's log level, so that</span>
<span class="token comment" spellcheck="true"># the user can have different defaults for the shell and regular Spark apps.</span>
<span class="token attr-name">log4j.logger.org.apache.spark.repl.Main</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>

<span class="token comment" spellcheck="true"># Settings to quiet third party logs that are too verbose</span>
<span class="token attr-name">log4j.logger.org.spark_project.jetty</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>
<span class="token attr-name">log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>
<span class="token attr-name">log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>
<span class="token attr-name">log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>
<span class="token attr-name">log4j.logger.org.apache.parquet</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>
<span class="token attr-name">log4j.logger.parquet</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span>

<span class="token comment" spellcheck="true"># SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support</span>
<span class="token attr-name">log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler</span><span class="token punctuation">=</span><span class="token attr-value">FATAL</span>
<span class="token attr-name">log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry</span><span class="token punctuation">=</span><span class="token attr-value">ERROR</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-1-4-异常处理"><a href="#2-1-4-异常处理" class="headerlink" title="2.1.4 异常处理"></a>2.1.4 异常处理</h3><p>如果本机操作系统是Windows，在程序中使用了Hadoop相关的东西，比如写入文件到HDFS，则会遇到如下异常：</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176731823.png" alt="1638176731823"></p><p>出现这个问题的原因，并不是程序的错误，而是windows系统用到了hadoop相关的服务，解决办法是通过配置关联到windows的系统依赖就可以了</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176805506.png" alt="1638176805506"></p><p>在IDEA中配置Run Configuration，添加HADOOP_HOME变量</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176855314.png" alt="1638176855314"></p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176879056.png" alt="1638176879056"></p><h1 id="3-Spark运行环境"><a href="#3-Spark运行环境" class="headerlink" title="3 Spark运行环境"></a>3 Spark运行环境</h1><p>Spark作为一个数据处理框架和计算引擎，被设计在所有常见的集群环境中运行, 在国内工作中主流的环境为Yarn，不过逐渐<strong>容器式环境</strong>也慢慢流行起来。接下来，我们就分别看看不同环境下Spark的运行。</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176475372.png" alt="1638176475372"></p><h2 id="3-1-Local模式"><a href="#3-1-Local模式" class="headerlink" title="3.1  Local模式"></a>3.1 Local模式</h2><p>所谓的<strong>Local模式，就是不需要其他任何节点资源就可以在本地执行Spark代码的环境</strong>，一般用于教学，调试，演示等，之前在IDEA中运行代码的环境我们称之为<strong>开发环境</strong>，不太一样。（<strong>Local=本机提供资源+spark提供计算</strong>）</p><h3 id="3-1-1-解压缩文件"><a href="#3-1-1-解压缩文件" class="headerlink" title="3.1.1 解压缩文件"></a>3.1.1 解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到Linux并解压缩，放置在指定位置，路径中不要包含中文或空格，课件后续如果涉及到解压缩操作，不再强调。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">tar</span> -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
<span class="token function">cd</span> /opt/module 
<span class="token function">mv</span> spark-3.0.0-bin-hadoop3.2 spark-local<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="3-1-2-启动Local环境"><a href="#3-1-2-启动Local环境" class="headerlink" title="3.1.2 启动Local环境"></a>3.1.2 启动Local环境</h3><ol><li>进入解压缩后的路径，执行如下指令</li></ol><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-shell<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638173742050.png" alt="1638173742050"></p><ol start="2"><li>启动成功后，可以输入网址进行Web UI<strong>监控页面</strong>访问</li></ol><p>http://虚拟机地址:4040</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176427950.png" alt="1638176427950"></p><h3 id="3-1-3-命令行工具"><a href="#3-1-3-命令行工具" class="headerlink" title="3.1.3 命令行工具"></a>3.1.3 命令行工具</h3><p>在解压缩文件夹下的data目录中，添加word.txt文件。在命令行工具中执行如下代码指令（和IDEA中代码简化版一致）</p><pre class="line-numbers language-scala"><code class="language-scala">sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"data/word.txt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>_<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>_<span class="token operator">+</span>_<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176440182.png" alt="1638176440182"></p><h3 id="3-1-4-退出本地模式"><a href="#3-1-4-退出本地模式" class="headerlink" title="3.1.4 退出本地模式"></a>3.1.4 退出本地模式</h3><p>按键Ctrl+C或输入Scala指令</p><pre class="line-numbers language-bash"><code class="language-bash">:quit<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-1-5-提交应用"><a href="#3-1-5-提交应用" class="headerlink" title="3.1.5 提交应用"></a>3.1.5 提交应用</h3><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master local<span class="token punctuation">[</span>2<span class="token punctuation">]</span> \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>–class表示要执行程序的主类，此处可以更换为咱们自己写的应用程序</li><li>–master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟CPU核数量;不知道可以用*</li><li>spark-examples_2.12-3.0.0.jar 运行的应用类所在的jar包，实际使用时，可以设定为咱们自己打的jar包</li><li>数字10表示程序的入口参数，用于设定当前应用的任务数量</li></ol><h2 id="3-2-Standalone模式"><a href="#3-2-Standalone模式" class="headerlink" title="3.2  Standalone模式"></a>3.2 Standalone模式</h2><p>local本地模式毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，这里我们来看看只使用<strong>Spark自身节点运行的集群模式</strong>，也就是我们所谓的独立部署（Standalone）模式。Spark的Standalone模式体现了经典的master-slave模式。（<strong>Standalone=spark提供资源+spark提供计算</strong>）</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638174301463.png" alt="1638174301463"></p><p>集群规划:</p><table><thead><tr><th></th><th>Linux1</th><th>Linux2</th><th>Linux3</th></tr></thead><tbody><tr><td>Spark</td><td>Worker Master</td><td>Worker</td><td>Worker</td></tr></tbody></table><h3 id="3-2-1-解压缩文件"><a href="#3-2-1-解压缩文件" class="headerlink" title="3.2.1 解压缩文件"></a>3.2.1 解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到Linux并解压缩在指定位置</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">tar</span> -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
<span class="token function">cd</span> /opt/module 
<span class="token function">mv</span> spark-3.0.0-bin-hadoop3.2 spark-standalone<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="3-2-2-修改配置文件"><a href="#3-2-2-修改配置文件" class="headerlink" title="3.2.2 修改配置文件"></a>3.2.2 修改配置文件</h3><ol><li>进入解压缩后路径的conf目录，修改slaves.template文件名为slaves</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">mv</span> slaves.template slaves 修改slaves文件，添加worker节点<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-bash"><code class="language-bash">linux1
linux2
linux3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="2"><li>修改spark-env.sh.template文件名为spark-env.sh</li></ol><pre><code>mv spark-env.sh.template spark-env.sh</code></pre><p>修改spark-env.sh文件，添加JAVA_HOME环境变量和集群对应的master节点。指定master机器，和集群间spark通信的端口。</p><pre class="line-numbers language-sh"><code class="language-sh">export JAVA_HOME=/opt/module/jdk1.8.0_144
SPARK_MASTER_HOST=linux1
SPARK_MASTER_PORT=7077<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>注意：7077端口，相当于hadoop3内部通信的8020端口，此处的端口需要确认自己的Hadoop配置</p><ol start="5"><li>分发spark-standalone目录</li></ol><pre class="line-numbers language-bash"><code class="language-bash">xsync spark-standalone<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-2-3-启动集群"><a href="#3-2-3-启动集群" class="headerlink" title="3.2.3 启动集群"></a>3.2.3 启动集群</h3><ol><li>执行脚本命令：</li></ol><pre class="line-numbers language-bash"><code class="language-bash">sbin/start-all.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638174700872.png" alt="1638174700872"></p><ol start="2"><li>查看三台服务器运行进程</li></ol><pre class="line-numbers language-bash"><code class="language-bash">xcall jps
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>linux1<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>
3330 Jps
3238 Worker
3163 Master
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>linux2<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>
2966 Jps
2908 Worker
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>linux3<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>
2978 Worker
3036 Jps<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="3"><li>查看Master资源监控Web UI界面: <a target="_blank" rel="noopener" href="http://linux1:8080/">http://linux1:8080</a></li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638174744996.png" alt="1638174744996"></p><h3 id="3-2-4-提交应用"><a href="#3-2-4-提交应用" class="headerlink" title="3.2.4 提交应用"></a>3.2.4 提交应用</h3><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://linux1:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>–class表示要执行程序的主</li><li>–master spark://linux1:7077 独立部署模式，连接到Spark集群</li><li>spark-examples_2.12-3.0.0.jar 运行类所在的jar包</li><li>数字10表示程序的入口参数，用于设定当前应用的任务数量</li></ol><p>执行任务时，会产生多个Java进程</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175491608.png" alt="1638175491608"></p><p>执行任务时，默认采用服务器集群节点的总核数，每个节点内存1024M。</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175504305.png" alt="1638175504305"></p><h3 id="3-2-5-提交参数说明"><a href="#3-2-5-提交参数说明" class="headerlink" title="3.2.5 提交参数说明"></a>3.2.5 提交参数说明</h3><p>在提交应用中，一般会同时一些提交参数</p><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-submit \
--class <span class="token operator">&lt;</span>main-class<span class="token operator">></span>
--master <span class="token operator">&lt;</span>master-url<span class="token operator">></span> \
<span class="token punctuation">..</span>. <span class="token comment" spellcheck="true"># other options</span>
<span class="token operator">&lt;</span>application-jar<span class="token operator">></span> \
<span class="token punctuation">[</span>application-arguments<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><table><thead><tr><th>参数</th><th>解释</th><th>可选值举例</th></tr></thead><tbody><tr><td>–class</td><td>Spark程序中包含主函数的类</td><td></td></tr><tr><td>–master</td><td>Spark程序运行的模式(环境)</td><td>*<em>模式：local[</em>]、spark://linux1:7077、 Yarn**</td></tr><tr><td>–executor-memory 1G</td><td>指定每个executor可用内存为1G</td><td>符合集群内存配置即可，具体情况具体分析。</td></tr><tr><td>–total-executor-cores 2</td><td>指定所有executor使用的cpu核数为2个</td><td></td></tr><tr><td>–executor-cores</td><td>指定每个executor使用的cpu核数</td><td></td></tr><tr><td>application-jar</td><td>打包好的应用jar，包含依赖。这个URL在集群中全局可见。 比如hdfs:// 共享存储系统，如果是file:// path，那么所有的节点的path都包含同样的jar</td><td></td></tr><tr><td>application-arguments</td><td>传给main()方法的参数</td><td></td></tr></tbody></table><h3 id="3-2-6-配置历史服务"><a href="#3-2-6-配置历史服务" class="headerlink" title="3.2.6 配置历史服务"></a>3.2.6 配置历史服务</h3><p>由于spark-shell停止掉后，集群监控linux1:4040页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况。**(历史信息存到了HDFS中)**</p><ol><li>修改spark-defaults.conf.template文件名为spark-defaults.conf</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">mv</span> spark-defaults.conf.template spark-defaults.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="2"><li>修改spark-default.conf文件，配置日志存储路径</li></ol><pre class="line-numbers language-sh"><code class="language-sh">spark.eventLog.enabled     true
spark.eventLog.dir        hdfs://linux1:8020/directory<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>注意：需要启动hadoop集群，HDFS上的directory目录需要提前存在。</strong></p><pre class="line-numbers language-bash"><code class="language-bash">sbin/start-dfs.sh
hadoop fs -mkdir /directory<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="3"><li>修改spark-env.sh文件, 添加日志配置</li></ol><pre class="line-numbers language-sh"><code class="language-sh">export SPARK_HISTORY_OPTS="
-Dspark.history.ui.port=18080 
-Dspark.history.fs.logDirectory=hdfs://linux1:8020/directory 
-Dspark.history.retainedApplications=30"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>l 参数1含义：WEB UI访问的端口号为18080<br>l 参数2含义：指定历史服务器日志存储路径<br>l 参数3含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。</p><ol start="4"><li>分发配置文件</li></ol><pre class="line-numbers language-bash"><code class="language-bash">xsync conf <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="5"><li>重新启动集群和历史服务</li></ol><pre><code>sbin/start-all.sh

sbin/start-history-server.sh</code></pre><ol start="6"><li>重新执行任务</li></ol><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://linux1:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="7"><li>查看历史服务：<a target="_blank" rel="noopener" href="http://linux1:18080/">http://linux1:18080</a></li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175185282.png" alt="1638175185282"></p><h3 id="3-2-7-配置高可用（HA）"><a href="#3-2-7-配置高可用（HA）" class="headerlink" title="3.2.7 配置高可用（HA）"></a>3.2.7 配置高可用（HA）</h3><p>所谓的高可用是因为当前集群中的Master节点只有一个，所以会存在单点故障问题。<strong>所以为了解决单点故障问题，需要在集群中配置多个Master节点</strong>，一旦处于活动状态的Master发生故障时，由备用Master提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper设置。</p><p><strong>集群规划</strong>:</p><table><thead><tr><th></th><th>Linux1</th><th>Linux2</th><th>Linux3</th></tr></thead><tbody><tr><td>Spark</td><td>Master Zookeeper Worker</td><td>Master Zookeeper Worker</td><td>Zookeeper Worker</td></tr></tbody></table><ol><li>停止集群</li></ol><pre class="line-numbers language-bash"><code class="language-bash">sbin/stop-all.sh <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="2"><li>启动Zookeeper</li></ol><pre class="line-numbers language-bash"><code class="language-bash">xstart zk <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="3"><li>修改spark-env.sh文件添加如下配置</li></ol><p>注释如下内容：</p><p>#SPARK_MASTER_HOST=linux1</p><p>#SPARK_MASTER_PORT=7077</p><p>添加如下内容:</p><p>#Master监控页面默认访问端口为8080，但是可能会和Zookeeper冲突，所以改成8989，也可以自定义，访问UI监控页面时请注意</p><pre class="line-numbers language-sh"><code class="language-sh">SPARK_MASTER_WEBUI_PORT=8989
export SPARK_DAEMON_JAVA_OPTS="
-Dspark.deploy.recoveryMode=ZOOKEEPER 
-Dspark.deploy.zookeeper.url=linux1,linux2,linux3 
-Dspark.deploy.zookeeper.dir=/spark"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="4"><li>分发配置文件</li></ol><pre class="line-numbers language-bash"><code class="language-bash">xsync conf/ <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="5"><li>启动集群</li></ol><pre class="line-numbers language-bash"><code class="language-bash">sbin/start-all.sh <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175383495.png" alt="1638175383495"></p><ol start="6"><li>启动linux2的单独Master节点，此时linux2节点Master状态处于备用状态</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>root@linux2 spark-standalone<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># sbin/start-master.sh </span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175397500.png" alt="1638175397500"></p><ol start="7"><li>提交应用到高可用集群</li></ol><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://linux1:7077,linux2:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="8"><li>停止linux1的Master资源监控进程</li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175436542.png" alt="1638175436542"></p><ol start="9"><li>查看linux2的Master 资源监控Web UI，稍等一段时间后，linux2节点的Master状态提升为活动状态</li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175451854.png" alt="1638175451854"></p><h2 id="3-3-Yarn模式"><a href="#3-3-Yarn模式" class="headerlink" title="3.3  Yarn模式"></a>3.3 Yarn模式</h2><p>独立部署（Standalone）模式由Spark自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是你也要记住，Spark主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的Yarn环境下Spark是如何工作的（其实是因为在国内工作中，Yarn使用的非常多）。</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175863107.png" alt="1638175863107"></p><h3 id="3-3-1-解压缩文件"><a href="#3-3-1-解压缩文件" class="headerlink" title="3.3.1 解压缩文件"></a>3.3.1 解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到linux并解压缩，放置在指定位置。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">tar</span> -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
<span class="token function">cd</span> /opt/module 
<span class="token function">mv</span> spark-3.0.0-bin-hadoop3.2 spark-yarn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="3-3-2-修改配置文件"><a href="#3-3-2-修改配置文件" class="headerlink" title="3.3.2 修改配置文件"></a>3.3.2 修改配置文件</h3><ol><li>修改hadoop配置文件/opt/module/hadoop/etc/hadoop/yarn-site.xml, 并分发</li></ol><pre class="line-numbers language-xml"><code class="language-xml"><span class="token comment" spellcheck="true">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.pmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>

<span class="token comment" spellcheck="true">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.vmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>修改conf/spark-env.sh，添加JAVA_HOME和YARN_CONF_DIR配置</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">mv</span> spark-env.sh.template spark-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查找yarn的位置</p><pre class="line-numbers language-sh"><code class="language-sh">export JAVA_HOME=/opt/module/jdk1.8.0_144
YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="3-3-3-启动HDFS以及YARN集群"><a href="#3-3-3-启动HDFS以及YARN集群" class="headerlink" title="3.3.3 启动HDFS以及YARN集群"></a>3.3.3 启动HDFS以及YARN集群</h3><p>瞅啥呢，自己启动去！</p><h3 id="3-3-4-提交应用"><a href="#3-3-4-提交应用" class="headerlink" title="3.3.4 提交应用"></a>3.3.4 提交应用</h3><p>master指定为yarn，部署模式为集群模式。</p><pre class="line-numbers language-bash"><code class="language-bash">bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode cluster \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175901609.png" alt="1638175901609"></p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175915913.png" alt="1638175915913"></p><p>查看 <a target="_blank" rel="noopener" href="http://linux2:8088/">http://linux2:8088</a> 页面，点击History，查看历史页面</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638175935231.png" alt="1638175935231"></p><h3 id="3-3-5-配置历史服务器"><a href="#3-3-5-配置历史服务器" class="headerlink" title="3.3.5 配置历史服务器"></a>3.3.5 配置历史服务器</h3><ol><li>修改spark-defaults.conf.template文件名为spark-defaults.conf</li></ol><pre><code>mv spark-defaults.conf.template spark-defaults.conf</code></pre><ol start="2"><li>修改spark-default.conf文件，配置日志存储路径</li></ol><pre><code>spark.eventLog.enabled     true
spark.eventLog.dir        hdfs://linux1:8020/directory</code></pre><p>注意：需要启动hadoop集群，HDFS上的目录需要提前存在。</p><pre><code>[root@linux1 hadoop]# sbin/start-dfs.sh
[root@linux1 hadoop]# hadoop fs -mkdir /directory</code></pre><ol start="3"><li>修改spark-env.sh文件, 添加日志配置</li></ol><pre><code>export SPARK_HISTORY_OPTS=&quot;
-Dspark.history.ui.port=18080 
-Dspark.history.fs.logDirectory=hdfs://linux1:8020/directory 
-Dspark.history.retainedApplications=30&quot;</code></pre><p>l 参数1含义：WEB UI访问的端口号为18080</p><p>l 参数2含义：指定历史服务器日志存储路径</p><p>l 参数3含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。</p><ol start="4"><li>修改spark-defaults.conf</li></ol><pre><code>spark.yarn.historyServer.address=linux1:18080
spark.history.ui.port=18080</code></pre><ol start="5"><li>启动历史服务</li></ol><pre><code>sbin/start-history-server.sh </code></pre><ol start="6"><li>重新提交应用</li></ol><pre><code>bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10</code></pre><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176022879.png" alt="1638176022879"></p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176037995.png" alt="1638176037995"></p><ol start="7"><li>Web页面查看日志：<a target="_blank" rel="noopener" href="http://linux2:8088/">http://linux2:8088</a></li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176059473.png" alt="1638176059473"></p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176071216.png" alt="1638176071216"></p><h2 id="3-4-K8S-amp-Mesos模式"><a href="#3-4-K8S-amp-Mesos模式" class="headerlink" title="3.4  K8S &amp; Mesos模式"></a>3.4 K8S &amp; Mesos模式</h2><p>Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核,在Twitter得到广泛使用,管理着Twitter超过30,0000台服务器上的应用部署，但是在国内，依然使用着传统的Hadoop大数据框架，所以国内使用Mesos框架的并不多，但是原理其实都差不多，这里我们就不做过多讲解了。</p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176091921.png" alt="1638176091921"></p><p>容器化部署是目前业界很流行的一项技术，基于Docker镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes（k8s），而Spark也在最近的版本中支持了k8s部署模式。这里我们也不做过多的讲解。给个链接大家自己感受一下：<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/running-on-kubernetes.html">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176106974.png" alt="1638176106974"></p><h2 id="3-5-Windows模式"><a href="#3-5-Windows模式" class="headerlink" title="3.5  Windows模式"></a>3.5 Windows模式</h2><p>在同学们自己学习时，每次都需要启动虚拟机，启动集群，这是一个比较繁琐的过程，并且会占大量的系统资源，导致系统执行变慢，不仅仅影响学习效果，也影响学习进度，Spark非常暖心地提供了可以在windows系统下启动本地集群的方式，这样，在不使用虚拟机的情况下，也能学习Spark的基本使用！</p><p>在后续的教学中，为了能够给同学们更加流畅的教学效果和教学体验，我们一般情况下都会采用windows系统的集群来学习Spark。</p><h3 id="3-5-1-解压缩文件"><a href="#3-5-1-解压缩文件" class="headerlink" title="3.5.1 解压缩文件"></a>3.5.1 解压缩文件</h3><p>将文件spark-3.0.0-bin-hadoop3.2.tgz解压缩到无中文无空格的路径中</p><h3 id="3-5-2-启动本地环境"><a href="#3-5-2-启动本地环境" class="headerlink" title="3.5.2 启动本地环境"></a>3.5.2 启动本地环境</h3><ol><li>执行解压缩文件路径下bin目录中的spark-shell.cmd文件，启动Spark本地环境</li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176202265.png" alt="1638176202265"></p><ol start="2"><li>在bin目录中创建input目录，并添加word.txt文件, 在命令行中输入脚本代码</li></ol><p><img src="/2020/11/29/bigdata-Spark1-setup/1638176219502.png" alt="1638176219502"></p><h3 id="3-5-3-命令行提交应用"><a href="#3-5-3-命令行提交应用" class="headerlink" title="3.5.3 命令行提交应用"></a>3.5.3 命令行提交应用</h3><p>在DOS命令行窗口中执行提交指令</p><pre class="line-numbers language-bash"><code class="language-bash">spark-submit --class org.apache.spark.examples.SparkPi --master local<span class="token punctuation">[</span>2<span class="token punctuation">]</span> <span class="token punctuation">..</span>/examples/jars/spark-examples_2.12-3.0.0.jar 10<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="3-6-部署模式对比"><a href="#3-6-部署模式对比" class="headerlink" title="3.6  部署模式对比"></a>3.6 部署模式对比</h2><table><thead><tr><th>模式</th><th>Spark安装机器数</th><th>需启动的进程</th><th>所属者</th><th>应用场景</th></tr></thead><tbody><tr><td>Local</td><td>1</td><td>无</td><td>Spark</td><td>测试</td></tr><tr><td>Standalone</td><td>3</td><td>Master及Worker</td><td>Spark</td><td>单独部署</td></tr><tr><td>Yarn</td><td>1</td><td>Yarn及HDFS</td><td>Hadoop</td><td>混合部署</td></tr></tbody></table><h2 id="3-7-端口号"><a href="#3-7-端口号" class="headerlink" title="3.7  端口号"></a>3.7 端口号</h2><ul><li><p>Spark查看当前Spark-shell运行任务情况端口号：4040（计算）</p></li><li><p>Spark Master内部通信服务端口号：7077</p></li><li><p>Standalone模式下，Spark Master Web端口号：8080（资源）</p></li><li><p>Spark历史服务器端口号：18080</p></li><li><p>Hadoop YARN任务运行情况查看端口号：8088</p></li></ul><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div></div><div class="copyright"><p><span>本文标题:</span><a href="/2020/11/29/bigdata-Spark1-setup/">Spark学习笔记（一） 搭建Spark</a></p><p><span>文章作者:</span><a href="/" title="回到主页">m01ly</a></p><p><span>发布时间:</span>2020-11-29, 11:15:49</p><p><span>最后更新:</span>2021-12-02, 10:41:58</p><p><span>原始链接:</span><a class="post-url" href="/2020/11/29/bigdata-Spark1-setup/" title="Spark学习笔记（一） 搭建Spark">https://m01ly.github.io/2020/11/29/bigdata-Spark1-setup/</a></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target="_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。</p></div><nav id="article-nav"><div id="article-nav-newer" class="article-nav-title"><a href="/2020/11/29/bigdata-Spark2-framework/">Spark学习笔记（二） 架构解析和RDD编程</a></div><div id="article-nav-older" class="article-nav-title"><a href="/2020/11/25/bigdata-HBase2-framework/">HBase学习笔记（二） HBase架构解析</a></div></nav></article><div id="toc" class="toc-article"><strong class="toc-title">文章目录</strong><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Spark%E6%A6%82%E8%BF%B0"><span class="toc-text">1 Spark概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-spark-or-haddop"><span class="toc-text">1.1 spark or haddop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-Spark%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97"><span class="toc-text">1.2 Spark核心模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-Spark-%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97"><span class="toc-text">1.1 Spark 核心模块</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Spark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B"><span class="toc-text">2 Spark快速上手</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%88%9B%E5%BB%BAMaven%E9%A1%B9%E7%9B%AE"><span class="toc-text">2.1 创建Maven项目</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1-%E5%A2%9E%E5%8A%A0Scala%E6%8F%92%E4%BB%B6"><span class="toc-text">2.1.1 增加Scala插件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-%E5%A2%9E%E5%8A%A0%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="toc-text">2.1.2 增加依赖关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-3-WordCount"><span class="toc-text">2.1.3 WordCount</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-4-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86"><span class="toc-text">2.1.4 异常处理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Spark%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83"><span class="toc-text">3 Spark运行环境</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Local%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.1 Local模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-%E8%A7%A3%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6"><span class="toc-text">3.1.1 解压缩文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E5%90%AF%E5%8A%A8Local%E7%8E%AF%E5%A2%83"><span class="toc-text">3.1.2 启动Local环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-3-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7"><span class="toc-text">3.1.3 命令行工具</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-4-%E9%80%80%E5%87%BA%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.1.4 退出本地模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-5-%E6%8F%90%E4%BA%A4%E5%BA%94%E7%94%A8"><span class="toc-text">3.1.5 提交应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Standalone%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.2 Standalone模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-%E8%A7%A3%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6"><span class="toc-text">3.2.1 解压缩文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">3.2.2 修改配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4"><span class="toc-text">3.2.3 启动集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-4-%E6%8F%90%E4%BA%A4%E5%BA%94%E7%94%A8"><span class="toc-text">3.2.4 提交应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-5-%E6%8F%90%E4%BA%A4%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="toc-text">3.2.5 提交参数说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-6-%E9%85%8D%E7%BD%AE%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1"><span class="toc-text">3.2.6 配置历史服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-7-%E9%85%8D%E7%BD%AE%E9%AB%98%E5%8F%AF%E7%94%A8%EF%BC%88HA%EF%BC%89"><span class="toc-text">3.2.7 配置高可用（HA）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-Yarn%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.3 Yarn模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-%E8%A7%A3%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6"><span class="toc-text">3.3.1 解压缩文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">3.3.2 修改配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-3-%E5%90%AF%E5%8A%A8HDFS%E4%BB%A5%E5%8F%8AYARN%E9%9B%86%E7%BE%A4"><span class="toc-text">3.3.3 启动HDFS以及YARN集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-4-%E6%8F%90%E4%BA%A4%E5%BA%94%E7%94%A8"><span class="toc-text">3.3.4 提交应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-5-%E9%85%8D%E7%BD%AE%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-text">3.3.5 配置历史服务器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-K8S-amp-Mesos%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.4 K8S &amp; Mesos模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-Windows%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.5 Windows模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-1-%E8%A7%A3%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6"><span class="toc-text">3.5.1 解压缩文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-2-%E5%90%AF%E5%8A%A8%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83"><span class="toc-text">3.5.2 启动本地环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-3-%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%8F%90%E4%BA%A4%E5%BA%94%E7%94%A8"><span class="toc-text">3.5.3 命令行提交应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6-%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F%E5%AF%B9%E6%AF%94"><span class="toc-text">3.6 部署模式对比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-7-%E7%AB%AF%E5%8F%A3%E5%8F%B7"><span class="toc-text">3.7 端口号</span></a></li></ol></li></ol></div><style>.left-col .switch-area,.left-col .switch-btn{display:none}.toc-level-6 i,.toc-level-6 ol{display:none!important}</style><input type="button" id="tocButton" value="隐藏目录" title="点击按钮隐藏或者显示文章目录"><script>yiliaConfig.toc=["隐藏目录","显示目录",!0],$(".left-col").is(":hidden")&&$("#tocButton").attr("value",yiliaConfig.toc[1])</script><div class="share"><link rel="stylesheet" type="text/css" href="/share/iconfont.css"><link rel="stylesheet" href="/share/spongebob.min.css" type="text/css" media="all"><div class="social_share"><ul id="social_list" class="social_icon_list"></ul></div><script>var shareConfig={title:"Spark学习笔记（一） 搭建Spark",url:window.location.href,author:"m01ly",img:"https:/img/avatar.jpg"}</script><script src="/share/qrcode.min.js"></script><script src="/share/spongebob.min.js"></script></div><section id="comments" style="margin:2em;padding:2em;background:rgba(255,255,255,.5)"><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script><div id="gitalk-container"></div><script type="text/javascript">var gitalk=new Gitalk({clientID:"41a199ade404435645c4",clientSecret:"1de34fbb95212de986a29fea6d6f22bb57b2d473",repo:"m01ly.github.io",owner:"m01ly",admin:["m01ly"],id:window.location.pathname});gitalk.render("gitalk-container")</script></section><div class="scroll" id="post-nav-button"><a href="/2020/11/29/bigdata-Spark2-framework/" title="上一篇: Spark学习笔记（二） 架构解析和RDD编程"><i class="fa fa-angle-left"></i> </a><a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a> <a href="/2020/11/25/bigdata-HBase2-framework/" title="下一篇: HBase学习笔记（二） HBase架构解析"><i class="fa fa-angle-right"></i></a></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/12/01/bigdata-mapreduce2-framework/">Hadoop 教程（五）mapreduce架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/29/leetcode-binarytree/">leetcode-binarytree</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/28/scan-nessus-compliance/">nessus扫描合规性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/22/cert-letsencrypt/">cert-letsencrypt</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/16/leetcode-stackandqueue/">栈和队列相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/06/htps-recommend/">安全的TLS协议</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/07/leetcode-slidewindow/">滑动窗口相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/07/leetcode-list/">链表相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/26/leetcode-binary/">二分查找相关的题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/24/burpsuite-develop-detect-nginx/">开发burpsuite插件-识别nginx版本并列出已知CVE</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/21/burpsuite-develop/">从0开发burpsuite插件（Java）</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/26/machine-learning-classify-knn/">机器学习算法之KNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/07/openvas-develop/">openvas插件开发</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/06/leetcode-daily/">leetcode每日一题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/30/pt-antSword/">渗透测试工具之蚁剑</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/29/leetcode-sort/">排序算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/11/linux-disk/">centos7把/mnt空间合并到/(根目录)</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/19/install-guide-elk-filebeats/">elk笔记三--利用elk+filebeat搭建SIEM系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/18/linux-jdk8/">linux安装jdk1.8</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/01/21/esc-DefectDojo/">DefectDojo安装与使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper3-API/">Zookeeper学习笔记（三） API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper2-framework/">Zookeeper学习笔记（二） 架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper1-setup/">Zookeeper学习笔记（一） 搭建教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/29/bigdata-Spark2-framework/">Spark学习笔记（二） 架构解析和RDD编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/29/bigdata-Spark1-setup/">Spark学习笔记（一） 搭建Spark</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase2-framework/">HBase学习笔记（二） HBase架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase4-phoenix/">HBase学习笔记（四） HBase整合phoenix和Hive</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase3-API/">HBase学习笔记（三） HBase的API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase1-setup/">HBase学习笔记（一） 安装教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/23/bigdata-datacollect1-userbehavior/">大数据实践（一）数仓采集项目</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/22/bigdata-sqoop/">sqoop安装教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/18/bigdata-kafka4-test/">kafka学习笔记（四） kafka面试集锦</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/17/bigdata-kafka3-API/">kafka学习笔记（三） kafka的API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/16/bigdata-flume3-monitor/">flume学习笔记（三） flum数据流监控及面试题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/16/bigdata-kafka2-framework/">kafka学习笔记（二） kafka框架深入</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-flume2-framework/">flume学习笔记（二） flum事务和部署架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-flume1-setup/">flume学习笔记（一） flume搭建</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive3/">Hive学习笔记（三） Hive的分区表和分桶表</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive5-example/">Hive学习笔记（五） Hive实战</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive4-optimize/">Hive学习笔记（四） Hive的企业级调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive2/">Hive学习笔记（二） Hive对数据基本操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-kafka1-setup/">kafka学习笔记（一） kafka搭建</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/14/bigdata-hive1/">Hive学习笔记（一） Hive安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs1/">Hadoop 教程（二）安装hadoop集群-完全分布式部署及API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs/">Hadoop 教程（一）hadoop介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-yarn-framework/">Hadoop 教程（六）yarn-架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-mapreduce1-setup/">Hadoop 教程（四）mapreduce介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs3-framework/">Hadoop 教程（三）hdfs-架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/10/cipher-certificate-format/">证书的各种格式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/docker-guide/">docker使用大全</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/linux-cmd/">linux命令大全</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-metosploitInAliyun/">在阿里云主机反弹metosploit</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-info-collection/">信息收集</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-tools/">最佳网络安全和黑客软件</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/18/mobilesecurity-experience/">小白如何在三天一步步逆向app，找到私钥</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/elk-login/">elk笔记二--通过X-Pack权限控制设置elk登录</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-centosInvm/">vm 安装centos 7教程详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/writeup-sqli-labs/">writeup-sqli-labs</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-elk-suricata/">elk笔记一---suricata+elk搭建入侵检测系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-suricata/">centos7中安装suricata</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/10/pt-sqlbypass/">sql关键词绕过</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/04/pt-portinfo/">常见端口说明和攻击汇总</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/03/m01ly-wiki/">m01ly-wiki</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/03/htps-attack-heartbleed/">TLS攻击之心脏滴血</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/01/htps-attack-paddingoracle/">TLS 攻击之POODLE</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/31/blockcipher-padding/">分组密码--填充模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/31/blockcipher-operation-mode/">分组密码--工作模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/hexo-guide/">Hexo踩坑</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/scan-awvs-nessus/">AWVS和Nessus镜像安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/scan-zap/">ZAP的安装和使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/pt-tiquan/">提权</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/htps-tools/">TLS安全检测小工具</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/htps-build/">搭建https网站</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/apple/">竟然有人能把https/TLS1.2协议讲的这么详细</a></li></ul><script></script></div><footer id="footer"><div class="outer"><div id="footer-info"><div class="footer-left"><i class="fa fa-copyright"></i> 2017-2021 冀-18010769-1</div><div class="visit"><span id="busuanzi_container_site_pv" style="display:none"><span id="site-visit" title="本站到访人数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span> </span></span><span>| </span><span id="busuanzi_container_page_pv" style="display:none"><span id="page-visit" title="本页访问次数"><i class="fa fa-eye" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span></span></span></div><div class="footer-right"><i class="fa fa-heart"></i><a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架"> Hexo</a> Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a></div></div></div></footer></div><script type="application/javascript">var leftWidth,hide=!1;$(".hide-left-col").click(function(){hide=hide?($(".left-col").css("width",leftWidth),$(".left-col .intrude-less").fadeIn(200),$("#tocButton").fadeIn(200),"block"===$("#switch-btn").css("display")&&"block"===$("#switch-area").css("display")||$("#toc").fadeIn(200),$(".hide-left-col").css("left",leftWidth).html('<i class="fa fa-angle-double-left"></i>'),$(".mid-col").css("left",leftWidth),$("#post-nav-button").css("left",leftWidth),$("#post-nav-button > a:nth-child(2)").css("display","block"),!1):(leftWidth=$(".left-col")[0].style.width,$(".left-col").css("width",0),$(".left-col .intrude-less").fadeOut(200),$("#toc").fadeOut(100),$("#tocButton").fadeOut(100),$(".hide-left-col").css("left",0).html('<i class="fa fa-angle-double-right"></i>'),$(".mid-col").css("left",0),$("#post-nav-button").css("left",0),$("#post-nav-button > a:nth-child(2)").css("display","none"),$(".post-list").is(":visible")&&($("#post-nav-button .fa-bars,#post-nav-button .fa-times").toggle(),$(".post-list").toggle()),!0)})</script><script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.3.5/require.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[["$$","$$"],["$","$"],["\\(","\\)"]],processEscapes:!0,skipTags:["script","noscript","style","textarea","pre","code"]}}),MathJax.Hub.Queue(function(){var a,e=MathJax.Hub.getAllJax();for(a=0;a<e.length;a+=1)e[a].SourceElement().parentNode.className+=" has-jax"})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script><div class="scroll" id="scroll"><a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a> <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a> <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a></div><script>var oOpenInNew={post:".copyright a[href]",friends:"#js-friends a",socail:".social a"};for(var x in oOpenInNew)$(oOpenInNew[x]).attr("target","_blank")</script><script>var titleTime,originTitle=document.title;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="(つェ⊂)"+originTitle,clearTimeout(titleTime)):(document.title="(*´∇｀*)~ "+originTitle,titleTime=setTimeout(function(){document.title=originTitle},2e3))})</script><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><link href="//cdn.bootcss.com/aos/2.2.0/aos.css" rel="stylesheet"><script type="text/javascript">AOS.init({easing:"ease-out-back",once:!0})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"live2d_models/live2d-widget-model-izumi"},"display":{"position":"right","width":100,"height":200,"hOffset":-50,"vOffset":-85},"mobile":{"show":false},"react":{"opacityDefault":0.9,"opacityOnHover":0.3},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false});</script></body></html>