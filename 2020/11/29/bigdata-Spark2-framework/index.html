<!DOCTYPE html><html lang="zh-Hans"><head><!--[if IE]><style>body{display:none;}</style><script>alert('IE浏览器下无法展示效果，请更换浏览器！');var headNode=document.getElementsByTagName('head')[0];var refresh=document.createElement('meta');refresh.setAttribute('http-equiv','Refresh');refresh.setAttribute('Content','0; url=http://outdatedbrowser.com/');headNode.appendChild(refresh);</script><![endif]--><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="m01ly"><meta name="description" content="1 Spark运行架构1.1 运行架构Spark框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。 如下图所示，它展示了一个 Spark执行时的基本结构。图形中的Driver表示master，负责管理整个集群中的作业任务调度。图形中的Executor 则是 slave，负责实际执行任务。  1.2 核心组件由上图可以看出，对于Spark框架有两个核心组件： 1.2"><meta property="og:type" content="article"><meta property="og:title" content="Spark学习笔记（二） 架构解析和RDD编程"><meta property="og:url" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/index.html"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="1 Spark运行架构1.1 运行架构Spark框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。 如下图所示，它展示了一个 Spark执行时的基本结构。图形中的Driver表示master，负责管理整个集群中的作业任务调度。图形中的Executor 则是 slave，负责实际执行任务。  1.2 核心组件由上图可以看出，对于Spark框架有两个核心组件： 1.2"><meta property="og:locale"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/1638183145356.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/1638183179425.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/1638183211849.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/1638181914698.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/clip_image086.jpg"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/1638182223714.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/1638182323787.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/1638182335328.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/1638182347091.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/1638182359929.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/clip_image106.jpg"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/clip_image107.jpg"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/clip_image108.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/clip_image110.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/clip_image112.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/clip_image113.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image115.jpg"><meta property="og:image" content="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image117.jpg"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/clip_image119.gif"><meta property="og:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/clip_image121.gif"><meta property="article:published_time" content="2020-11-29T09:29:54.000Z"><meta property="article:modified_time" content="2021-11-30T02:12:28.930Z"><meta property="article:author" content="m01ly"><meta property="article:tag" content="Spark"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/1638183145356.png"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="shortcut icon" href="/favicon.ico"><link href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css" rel="stylesheet"><link rel="stylesheet" href="/css/style.css"><title>Spark学习笔记（二） 架构解析和RDD编程 | Hexo</title><script src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"><script src="//cdn.bootcss.com/aos/2.2.0/aos.js"></script><script>var yiliaConfig={fancybox:!0,isHome:!1,isPost:!0,isArchive:!1,isTag:!1,isCategory:!1,fancybox_js:"//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js",search:!0}</script><script>yiliaConfig.jquery_ui=[!1]</script><script>yiliaConfig.rootUrl="/"</script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/rss+xml">
<link rel="stylesheet" href="/css/prism-a11y-dark.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="container"><div class="left-col"><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><form id="search-form"><input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false"> <i class="fa fa-times" onclick="resetSearch()"></i></form><div id="local-search-result"></div><p class="no-result">No results found <i class="fa fa-spinner fa-pulse"></i></p><div id="switch-btn" class="switch-btn"><div class="icon"><div class="icon-ctn"><div class="icon-wrap icon-house" data-idx="0"><div class="birdhouse"></div><div class="birdhouse_holes"></div></div><div class="icon-wrap icon-ribbon hide" data-idx="1"><div class="ribbon"></div></div><div class="icon-wrap icon-link hide" data-idx="2"><div class="loopback_l"></div><div class="loopback_r"></div></div><div class="icon-wrap icon-me hide" data-idx="3"><div class="user"></div><div class="shoulder"></div></div></div></div><div class="tips-box hide"><div class="tips-arrow"></div><ul class="tips-inner"><li>菜单</li><li>标签</li><li>友情链接</li><li>目标</li></ul></div></div><div id="switch-area" class="switch-area"><div class="switch-wrap"><section class="switch-part switch-part1"><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" rel="noopener" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" rel="noopener" href="https://music.163.com/" title="网易云音乐"></a></ul><ul class="social"><div class="donateIcon-position"><p style="display:block"><a class="donateIcon" href="javascript:void(0)" onmouseout='var qr=document.getElementById("donate");qr.style.display="none"' onmouseenter='var qr=document.getElementById("donate");qr.style.display="block"'>赏</a></p><div id="donate"><img id="multipay" src="/img/multipay.png" width="250px" alt="m01ly Multipay"><div class="triangle"></div></div></div></ul></nav></section><section class="switch-part switch-part2"><div class="widget tagcloud" id="js-tagcloud"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TLS/" rel="tag">TLS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zookeeper/" rel="tag">Zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/" rel="tag">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/" rel="tag">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BC%81%E4%B8%9A%E5%AE%89%E5%85%A8%E5%BB%BA%E8%AE%BE/" rel="tag">企业安全建设</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E5%85%A8%E5%B7%A5%E5%85%B7/" rel="tag">安全工具</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" rel="tag">安装教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8%E6%95%99%E7%A8%8B/" rel="tag">容器教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/" rel="tag">密码学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" rel="tag">插件开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E4%BB%93%E9%87%87%E9%9B%86%E9%A1%B9%E7%9B%AE/" rel="tag">数仓采集项目</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/" rel="tag">日志管理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/" rel="tag">流量分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" rel="tag">渗透测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%8F%E6%B4%9E%E5%A4%8D%E7%8E%B0/" rel="tag">漏洞复现</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%8F%E6%B4%9E%E6%89%AB%E6%8F%8F/" rel="tag">漏洞扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%BB%E5%8A%A8%E5%AE%89%E5%85%A8/" rel="tag">移动安全</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%B6%E5%9C%BAwriteup/" rel="tag">靶场writeup</a></li></ul></div></section><section class="switch-part switch-part3"><div id="js-friends"><a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/TechCatsLab">TechCatsLab</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://yangchenglong11.github.io">YangChengLong</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://jsharkc.github.io">LiuJiaChang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://blog.yusank.space">YusanKurban</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.lizebang.top">Lizebang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/sunanxiang">SunAnXiang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/DoubleWoodH">LinHao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.littlechao.top">ShiChao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/Txiaozhe">TangXiaoJi</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/LLLeon">JiaChenHui</a></div></section><section class="switch-part switch-part4"><div id="js-aboutme">不悲不喜，不卑不亢，努力成为一个更好的程序猿！</div></section></div></div></header></div></div><div class="hide-left-col" title="隐藏侧栏"><i class="fa fa-angle-double-left"></i></div><div class="mid-col"><nav id="mobile-nav"><div class="overlay"><div class="slider-trigger"></div><h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">m01ly</a></h1></div><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/" title="回到主页">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li><div class="clearfix"></div></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" href="https://music.163.com/" title="网易云音乐"></a></ul></nav></header></div><link class="menu-list" tags="标签" friends="友情链接" about="目标"></nav><div class="body-wrap"><article id="post-bigdata-Spark2-framework" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/29/bigdata-Spark2-framework/" class="article-date"><time class="published" datetime="2020-11-29T09:29:54.000Z" itemprop="datePublished">2020-11-29 发布</time> <time class="updated" datetime="2021-11-30T02:12:28.930Z" itemprop="dateUpdated">2021-11-30 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 class="article-title" itemprop="name">Spark学习笔记（二） 架构解析和RDD编程</h1></header><div class="article-info article-info-post"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li></ul></div><span class="post-count">总字数11.9k</span> <span class="post-count">预计阅读50分钟</span></div><div class="clearfix"></div></div><div class="article-entry" itemprop="articleBody"><h1 id="1-Spark运行架构"><a href="#1-Spark运行架构" class="headerlink" title="1 Spark运行架构"></a>1 Spark运行架构</h1><h2 id="1-1-运行架构"><a href="#1-1-运行架构" class="headerlink" title="1.1 运行架构"></a>1.1 运行架构</h2><p>Spark框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。</p><p>如下图所示，它展示了一个 Spark执行时的基本结构。图形中的Driver表示master，负责管理整个集群中的作业任务调度。图形中的Executor 则是 slave，负责实际执行任务。</p><p><img src="/2020/11/29/bigdata-Spark2-framework/1638183145356.png" alt="1638183145356"></p><h2 id="1-2-核心组件"><a href="#1-2-核心组件" class="headerlink" title="1.2 核心组件"></a>1.2 核心组件</h2><p>由上图可以看出，对于Spark框架有两个核心组件：</p><h3 id="1-2-1-Driver"><a href="#1-2-1-Driver" class="headerlink" title="1.2.1 Driver"></a>1.2.1 Driver</h3><p>Spark驱动器节点，用于执行Spark任务中的main方法，负责实际代码的执行工作。Driver在Spark作业执行时主要负责：</p><p>Ø 将用户程序转化为作业（job）</p><p>Ø 在Executor之间调度任务(task)</p><p>Ø 跟踪Executor的执行情况</p><p>Ø 通过UI展示查询运行情况</p><p>实际上，我们无法准确地描述Driver的定义，因为在整个的编程过程中没有看到任何有关Driver的字眼。所以简单理解，所谓的Driver就是驱使整个应用运行起来的程序，也称之为Driver类。</p><h3 id="1-2-2-Executor"><a href="#1-2-2-Executor" class="headerlink" title="1.2.2 Executor"></a>1.2.2 Executor</h3><p>Spark Executor是集群中工作节点（Worker）中的一个JVM进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。</p><p>Executor有两个核心功能：</p><p>Ø 负责运行组成Spark应用的任务，并将结果返回给驱动器进程</p><p>Ø 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</p><h3 id="1-2-3-Master-amp-Worker"><a href="#1-2-3-Master-amp-Worker" class="headerlink" title="1.2.3 Master &amp; Worker"></a>1.2.3 Master &amp; Worker</h3><p>Spark集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master和Worker，这里的Master是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于Yarn环境中的RM, 而Worker呢，也是进程，一个Worker运行在集群中的一台服务器上，由Master分配资源对数据进行并行的处理和计算，类似于Yarn环境中NM。</p><h3 id="1-2-4-ApplicationMaster"><a href="#1-2-4-ApplicationMaster" class="headerlink" title="1.2.4 ApplicationMaster"></a>1.2.4 ApplicationMaster</h3><p>Hadoop用户向YARN集群提交应用程序时,提交程序中应该包含ApplicationMaster，用于向资源调度器申请执行任务的资源容器Container，运行用户自己的程序任务job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。</p><p>说的简单点就是，ResourceManager（资源）和Driver（计算）之间的解耦合靠的就是ApplicationMaster。</p><h2 id="1-3-核心概念"><a href="#1-3-核心概念" class="headerlink" title="1.3 核心概念"></a>1.3 核心概念</h2><h3 id="1-3-1-Executor与Core（核）"><a href="#1-3-1-Executor与Core（核）" class="headerlink" title="1.3.1 Executor与Core（核）"></a>1.3.1 Executor与Core（核）</h3><p>Spark Executor是集群中运行在工作节点（Worker）中的一个JVM进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点Executor的内存大小和使用的虚拟CPU核（Core）数量。</p><p>应用程序相关启动参数如下：</p><table><thead><tr><th>名称</th><th>说明</th></tr></thead><tbody><tr><td>–num-executors</td><td>配置Executor的数量</td></tr><tr><td>–executor-memory</td><td>配置每个Executor的内存大小</td></tr><tr><td>–executor-cores</td><td>配置每个Executor的虚拟CPU core数量</td></tr></tbody></table><h3 id="1-3-2-并行度（Parallelism）"><a href="#1-3-2-并行度（Parallelism）" class="headerlink" title="1.3.2 并行度（Parallelism）"></a>1.3.2 并行度（Parallelism）</h3><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们将整个集群并行执行任务的数量称之为并行度。那么一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。</p><h3 id="1-3-3-有向无环图（DAG）"><a href="#1-3-3-有向无环图（DAG）" class="headerlink" title="1.3.3 有向无环图（DAG）"></a>1.3.3 有向无环图（DAG）</h3><p><img src="/2020/11/29/bigdata-Spark2-framework/1638183179425.png" alt="1638183179425"></p><p>大数据计算引擎框架我们根据使用方式的不同一般会分为四类，其中第一类就是Hadoop所承载的MapReduce,它将计算分为两个阶段，分别为 Map阶段 和 Reduce阶段。对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算。 由于这样的弊端，催生了支持 DAG 框架的产生。因此，支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的 Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 Oozie 来说，大多还是批处理的任务。接下来就是以 Spark 为代表的第三代的计算引擎。第三代计算引擎的特点主要是 Job 内部的 DAG 支持（不跨越 Job），以及实时计算。</p><p>这里所谓的有向无环图，并不是真正意义的图形，而是由Spark程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来,这样更直观，更便于理解，可以用于表示程序的拓扑结构。</p><p>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。</p><h2 id="1-4-提交流程"><a href="#1-4-提交流程" class="headerlink" title="1.4 提交流程"></a>1.4 提交流程</h2><p>所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过Spark客户端提交给Spark运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又有细微的区别，我们这里不进行详细的比较，但是因为国内工作中，将Spark引用部署到Yarn环境中会更多一些，所以本课程中的提交流程是基于Yarn环境的。</p><p><img src="/2020/11/29/bigdata-Spark2-framework/1638183211849.png" alt="1638183211849"></p><p>Spark应用程序提交到Yarn环境中执行的时候，一般会有两种部署执行的方式：Client和Cluster。两种模式主要区别在于：Driver程序的运行节点位置。</p><h3 id="1-2-1-Yarn-Client模式"><a href="#1-2-1-Yarn-Client模式" class="headerlink" title="1.2.1 Yarn Client模式"></a>1.2.1 Yarn Client模式</h3><p>Client模式将用于监控和调度的Driver模块在客户端执行，而不是在Yarn中，所以一般用于测试。</p><p>Ø Driver在任务提交的本地机器上运行</p><p>Ø Driver启动后会和ResourceManager通讯申请启动ApplicationMaster</p><p>Ø ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，负责向ResourceManager申请Executor内存</p><p>Ø ResourceManager接到ApplicationMaster的资源申请后会分配container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程</p><p>Ø Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数</p><p>Ø 之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生成对应的TaskSet，之后将task分发到各个Executor上执行。</p><h3 id="1-2-2-Yarn-Cluster模式"><a href="#1-2-2-Yarn-Cluster模式" class="headerlink" title="1.2.2 Yarn Cluster模式"></a>1.2.2 Yarn Cluster模式</h3><p>Cluster模式将用于监控和调度的Driver模块启动在Yarn集群资源中执行。一般应用于实际生产环境。</p><p>Ø 在YARN Cluster模式下，任务提交后会和ResourceManager通讯申请启动ApplicationMaster，</p><p>Ø 随后ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver。</p><p>Ø Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配container，然后在合适的NodeManager上启动Executor进程</p><p>Ø Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数，</p><p>Ø 之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生成对应的TaskSet，之后将task分发到各个Executor上执行。</p><h1 id="2-Spark核心编程"><a href="#2-Spark核心编程" class="headerlink" title="2  Spark核心编程"></a>2 Spark核心编程</h1><p>Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p><p>Ø RDD : 弹性分布式数据集</p><p>Ø 累加器：分布式共享只写变量</p><p>Ø 广播变量：分布式共享只读变量</p><p>接下来我们一起看看这三大数据结构是如何在数据处理中使用的。</p><h2 id="2-1-RDD"><a href="#2-1-RDD" class="headerlink" title="2.1 RDD"></a>2.1 RDD</h2><h3 id="2-1-1-什么是RDD"><a href="#2-1-1-什么是RDD" class="headerlink" title="2.1.1 什么是RDD"></a>2.1.1 什么是RDD</h3><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。<strong>（所谓的RDD，其实就是要给数据结构，类似于链表中的Node，RDD中有适合并行计算的分区操作；RDD中封装了最小的计算单元，目的是更适合重复使用；Spark主要就是通过组合RDD的操作完成业务需求。）</strong></p><p>那Spark 怎么组合RDD？</p><p><strong>RDD的扩展功能采用的也是装饰者设计模式；RDD中的collect方法类似于IO中的read方法。RDD不存储任何数据，只封装逻辑。</strong></p><p><img src="/2020/11/29/bigdata-Spark2-framework/1638181914698.png" alt="1638181914698"></p><p>Ø 弹性</p><p>l 存储的弹性：内存与磁盘的自动切换；</p><p>l 容错的弹性：数据丢失可以自动恢复；</p><p>l 计算的弹性：计算出错重试机制；</p><p>l 分片的弹性：可根据需要重新分片。</p><p>Ø 分布式：数据存储在大数据集群不同节点上</p><p>Ø 数据集：<strong>RDD封装了计算逻辑，并不保存数据</strong></p><p>Ø 数据抽象：RDD是一个抽象类，需要子类具体实现</p><p>Ø 不可变：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑</p><p>Ø 可分区、并行计算</p><h3 id="2-1-2-核心属性"><a href="#2-1-2-核心属性" class="headerlink" title="2.1.2 核心属性"></a>2.1.2 核心属性</h3><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image086.jpg" alt="img"></p><p><img src="/2020/11/29/bigdata-Spark2-framework/1638182223714.png" alt="1638182223714"></p><p>1）分区列表</p><p>RDD数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。</p><p>2） 分区计算函数</p><p>Spark在计算时，是使用分区函数对每一个分区进行计算</p><p>3）RDD之间的依赖关系</p><p>RDD是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个RDD建立依赖关系</p><p>4） 分区器（可选）</p><p>当数据为KV类型数据时，可以通过设定分区器自定义数据的分区</p><p>5）首选位置（可选）</p><p>计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算</p><h3 id="2-1-3-执行原理"><a href="#2-1-3-执行原理" class="headerlink" title="2.1.3 执行原理"></a>2.1.3 执行原理</h3><p>从计算的角度来讲，数据处理过程中需要<strong>计算资源（内存 &amp; CPU）</strong>和<strong>计算模型（逻辑）</strong>。执行时，需<strong>要将计算资源和计算模型进行协调和整合</strong>。</p><p>Spark框架在执行时，先申请资源，然后将应用程序的数据处理逻辑<strong>分解成一个一个的计算任务</strong>。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。</p><p>RDD是Spark框架中用于数据处理的核心模型，接下来我们看看，在Yarn环境中，RDD的工作原理:</p><ol><li>启动Yarn集群环境</li></ol><p><img src="/2020/11/29/bigdata-Spark2-framework/1638182323787.png" alt="1638182323787"></p><ol start="2"><li>Spark通过申请资源创建调度节点和计算节点</li></ol><p><img src="/2020/11/29/bigdata-Spark2-framework/1638182335328.png" alt="1638182335328"></p><ol start="3"><li>Spark框架根据需求将计算逻辑根据分区划分成不同的任务</li></ol><p><img src="/2020/11/29/bigdata-Spark2-framework/1638182347091.png" alt="1638182347091"></p><ol start="4"><li>调度节点将任务根据计算节点状态发送到对应的计算节点进行计算</li></ol><p><img src="/2020/11/29/bigdata-Spark2-framework/1638182359929.png" alt="1638182359929"></p><p>从以上流程可以看出RDD在整个流程中主要用于将逻辑进行封装，并生成Task发送给Executor节点执行计算，接下来我们就一起看看Spark框架中RDD是具体是如何进行数据处理的。</p><h3 id="2-1-4-基础编程"><a href="#2-1-4-基础编程" class="headerlink" title="2.1.4 基础编程"></a>2.1.4 基础编程</h3><h4 id="2-1-4-1-RDD创建"><a href="#2-1-4-1-RDD创建" class="headerlink" title="2.1.4.1 RDD创建"></a>2.1.4.1 RDD创建</h4><p>在Spark中创建RDD的创建方式可以分为<strong>四种</strong>：</p><p><strong>1) 从集合（内存）中创建RDD</strong></p><p>从集合中创建RDD，<a target="_blank" rel="noopener" href="https://www.iteblog.com/archives/tag/spark/">Spark</a>主要提供了两个方法：parallelize和<strong>makeRDD(推荐使用)</strong></p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> sparkConf <span class="token operator">=</span>
    <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"spark"</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>
<span class="token keyword">val</span> rdd1 <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>
    List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
<span class="token keyword">val</span> rdd2 <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>
    List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
rdd1<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
rdd2<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>从外部存储（文件）创建RDD</li></ol><p>由外部存储系统的数据集创建RDD(<strong>textFile函数</strong>)包括：本地的文件系统，所有Hadoop支持的数据集，比如HDFS、HBase等。。</p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> sparkConf <span class="token operator">=</span>
    <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"spark"</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>
<span class="token keyword">val</span> fileRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"data/input.txt"</span><span class="token punctuation">)</span>
fileRDD<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>3) 从其他RDD创建</strong></p><p>主要是通过一个RDD运算完后，再产生新的RDD。详情请参考后续章节</p><p><strong>4) 直接创建RDD（new）</strong></p><p>使用new的方式直接构造RDD，一般由Spark框架自身使用。</p><h4 id="2-1-4-2-RDD并行度与分区"><a href="#2-1-4-2-RDD并行度与分区" class="headerlink" title="2.1.4.2 RDD并行度与分区"></a>2.1.4.2 RDD并行度与分区</h4><p>默认情况下，Spark可以将一个作业切分多个任务后，发送给Executor节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建RDD时指定。<strong>记住，这里的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。</strong></p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> sparkConf <span class="token operator">=</span>
    <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"spark"</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>
<span class="token keyword">val</span> dataRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">Int</span><span class="token punctuation">]</span> <span class="token operator">=</span>
    sparkContext<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>
        List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token number">4</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> fileRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span>
    sparkContext<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span>
        <span class="token string">"input"</span><span class="token punctuation">,</span>
        <span class="token number">2</span><span class="token punctuation">)</span>
fileRDD<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>l 读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark核心源码如下：</p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">def</span> positions<span class="token punctuation">(</span>length<span class="token operator">:</span> <span class="token builtin">Long</span><span class="token punctuation">,</span> numSlices<span class="token operator">:</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token operator">:</span> Iterator<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">Int</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
  <span class="token punctuation">(</span><span class="token number">0</span> until numSlices<span class="token punctuation">)</span><span class="token punctuation">.</span>iterator<span class="token punctuation">.</span>map <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> i <span class="token keyword">=></span>
    <span class="token keyword">val</span> start <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>i <span class="token operator">*</span> length<span class="token punctuation">)</span> <span class="token operator">/</span> numSlices<span class="token punctuation">)</span><span class="token punctuation">.</span>toInt
    <span class="token keyword">val</span> end <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> length<span class="token punctuation">)</span> <span class="token operator">/</span> numSlices<span class="token punctuation">)</span><span class="token punctuation">.</span>toInt
    <span class="token punctuation">(</span>start<span class="token punctuation">,</span> end<span class="token punctuation">)</span>
  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>l 读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异，具体Spark核心源码如下</p><pre class="line-numbers language-scala"><code class="language-scala">public InputSplit<span class="token punctuation">[</span><span class="token punctuation">]</span> getSplits<span class="token punctuation">(</span>JobConf job<span class="token punctuation">,</span> int numSplits<span class="token punctuation">)</span>
    throws IOException <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    long totalSize <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>                           <span class="token comment" spellcheck="true">// compute total size</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span>FileStatus file<span class="token operator">:</span> files<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// check we have valid files</span>
      <span class="token keyword">if</span> <span class="token punctuation">(</span>file<span class="token punctuation">.</span>isDirectory<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token keyword">throw</span> <span class="token keyword">new</span> IOException<span class="token punctuation">(</span><span class="token string">"Not a file: "</span><span class="token operator">+</span> file<span class="token punctuation">.</span>getPath<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
      <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
      totalSize <span class="token operator">+=</span> file<span class="token punctuation">.</span>getLen<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    long goalSize <span class="token operator">=</span> totalSize <span class="token operator">/</span> <span class="token punctuation">(</span>numSplits <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">?</span> <span class="token number">1</span> <span class="token operator">:</span> numSplits<span class="token punctuation">)</span><span class="token punctuation">;</span>
    long minSize <span class="token operator">=</span> Math<span class="token punctuation">.</span>max<span class="token punctuation">(</span>job<span class="token punctuation">.</span>getLong<span class="token punctuation">(</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>input<span class="token punctuation">.</span>
      FileInputFormat<span class="token punctuation">.</span>SPLIT_MINSIZE<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> minSplitSize<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

    <span class="token keyword">for</span> <span class="token punctuation">(</span>FileStatus file<span class="token operator">:</span> files<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

    <span class="token keyword">if</span> <span class="token punctuation">(</span>isSplitable<span class="token punctuation">(</span>fs<span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
          long blockSize <span class="token operator">=</span> file<span class="token punctuation">.</span>getBlockSize<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
          long splitSize <span class="token operator">=</span> computeSplitSize<span class="token punctuation">(</span>goalSize<span class="token punctuation">,</span> minSize<span class="token punctuation">,</span> blockSize<span class="token punctuation">)</span><span class="token punctuation">;</span>

          <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
  <span class="token keyword">protected</span> long computeSplitSize<span class="token punctuation">(</span>long goalSize<span class="token punctuation">,</span> long minSize<span class="token punctuation">,</span>
                                       long blockSize<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> Math<span class="token punctuation">.</span>max<span class="token punctuation">(</span>minSize<span class="token punctuation">,</span> Math<span class="token punctuation">.</span>min<span class="token punctuation">(</span>goalSize<span class="token punctuation">,</span> blockSize<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-4-3-RDD转换算子"><a href="#2-1-4-3-RDD转换算子" class="headerlink" title="2.1.4.3 RDD转换算子"></a>2.1.4.3 RDD转换算子</h4><p>RDD根据数据处理方式的不同将算子整体上分为Value类型、双Value类型和Key-Value类型</p><p><strong>l Value类型</strong></p><h5 id="1-map"><a href="#1-map" class="headerlink" title="1)   map"></a>1) map</h5><p>Ø 函数签名</p><p>def map[U: ClassTag](f: T =&gt; U): RDD[U]</p><p>Ø 函数说明</p><p>将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。</p><p>val dataRDD: RDD[Int] = sparkContext.makeRDD(List(1,2,3,4))</p><p>val dataRDD1: RDD[Int] = dataRDD.map(</p><p>num =&gt; {</p><p>​ num * 2</p><p>}</p><p>)</p><p>val dataRDD2: RDD[String] = dataRDD1.map(</p><p>num =&gt; {</p><p>​ “” + num</p><p>}</p><p>)</p><p>v 小功能：从服务器日志数据apache.log中获取用户请求URL资源路径</p><h5 id="2-mapPartitions"><a href="#2-mapPartitions" class="headerlink" title="2)   mapPartitions"></a>2) mapPartitions</h5><p>Ø 函数签名</p><p>def mapPartitions[U: ClassTag](</p><p>f: Iterator[T] =&gt; Iterator[U],</p><p>preservesPartitioning: Boolean = false): RDD[U]</p><p>Ø 函数说明</p><p>将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。</p><p>val dataRDD1: RDD[Int] = dataRDD.mapPartitions(</p><p>datas =&gt; {</p><p>​ datas.filter(_==2)</p><p>}</p><p>)</p><p>v 小功能：获取每个数据分区的最大值</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image106.jpg" alt="img"> 思考一个问题：map和mapPartitions的区别？</p><p>Ø 数据处理角度</p><p>Map算子是分区内一个数据一个数据的执行，类似于串行操作。而mapPartitions算子是以分区为单位进行批处理操作。</p><p>Ø 功能的角度</p><p>Map算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。MapPartitions算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据</p><p>Ø 性能的角度</p><p>Map算子因为类似于串行操作，所以性能比较低，而是mapPartitions算子类似于批处理，所以性能较高。但是mapPartitions算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用map操作。</p><p>完成比完美更重要</p><h5 id="3-mapPartitionsWithIndex"><a href="#3-mapPartitionsWithIndex" class="headerlink" title="3)   mapPartitionsWithIndex"></a>3) mapPartitionsWithIndex</h5><p>Ø 函数签名</p><p>def mapPartitionsWithIndex[U: ClassTag](</p><p>f: (Int, Iterator[T]) =&gt; Iterator[U],</p><p>preservesPartitioning: Boolean = false): RDD[U]</p><p>Ø 函数说明</p><p>将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。</p><p>val dataRDD1 = dataRDD.mapPartitionsWithIndex(</p><p>(index, datas) =&gt; {</p><p>​ datas.map(index, _)</p><p>}</p><p>)</p><p>v 小功能：获取第二个数据分区的数据</p><h5 id="4-flatMap"><a href="#4-flatMap" class="headerlink" title="4)   flatMap"></a>4) flatMap</h5><p>Ø 函数签名</p><p>def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U]</p><p>Ø 函数说明</p><p>将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>List(1,2),List(3,4)</p><p>),1)</p><p>val dataRDD1 = dataRDD.flatMap(</p><p>list =&gt; list</p><p>)</p><p>v 小功能：将List(List(1,2),3,List(4,5))进行扁平化操作</p><h5 id="5-glom"><a href="#5-glom" class="headerlink" title="5)   glom"></a>5) glom</h5><p>Ø 函数签名</p><p>def glom(): RDD[Array[T]]</p><p>Ø 函数说明</p><p>将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4</p><p>),1)</p><p>val dataRDD1:RDD[Array[Int]] = dataRDD.glom()</p><p>v 小功能：计算所有分区最大值求和（分区内取最大值，分区间最大值求和）</p><h5 id="6-groupBy"><a href="#6-groupBy" class="headerlink" title="6)   groupBy"></a>6) groupBy</h5><p>Ø 函数签名</p><p>def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])]</p><p>Ø 函数说明</p><p>将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为shuffle。极限情况下，数据可能被分在同一个分区中</p><p>一个组的数据在一个分区中，但是并不是说一个分区中只有一个组</p><p>val dataRDD = sparkContext.makeRDD(List(1,2,3,4),1)</p><p>val dataRDD1 = dataRDD.groupBy(</p><p>_%2</p><p>)</p><p>v 小功能：将List(“Hello”, “hive”, “hbase”, “Hadoop”)根据单词首写字母进行分组。</p><p>v 小功能：从服务器日志数据apache.log中获取每个时间段访问量。</p><p>v 小功能：WordCount。</p><h5 id="7-filter"><a href="#7-filter" class="headerlink" title="7)   filter"></a>7) filter</h5><p>Ø 函数签名</p><p>def filter(f: T =&gt; Boolean): RDD[T]</p><p>Ø 函数说明</p><p>将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。</p><p>当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜。</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4</p><p>),1)</p><p>val dataRDD1 = dataRDD.filter(_%2 == 0)</p><p>v 小功能：从服务器日志数据apache.log中获取2015年5月17日的请求路径</p><h5 id="8-sample"><a href="#8-sample" class="headerlink" title="8)   sample"></a>8) sample</h5><p>Ø 函数签名</p><p>def sample(</p><p>withReplacement: Boolean,</p><p>fraction: Double,</p><p>seed: Long = Utils.random.nextLong): RDD[T]</p><p>Ø 函数说明</p><p>根据指定的规则从数据集中抽取数据</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4</p><p>),1)</p><p>// 抽取数据不放回（伯努利算法）</p><p>// 伯努利算法：又叫0、1分布。例如扔硬币，要么正面，要么反面。</p><p>// 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不要</p><p>// 第一个参数：抽取的数据是否放回，false：不放回</p><p>// 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；</p><p>// 第三个参数：随机数种子</p><p>val dataRDD1 = dataRDD.sample(false, 0.5)</p><p>// 抽取数据放回（泊松算法）</p><p>// 第一个参数：抽取的数据是否放回，true：放回；false：不放回</p><p>// 第二个参数：重复数据的几率，范围大于等于0.表示每一个元素被期望抽取到的次数</p><p>// 第三个参数：随机数种子</p><p>val dataRDD2 = dataRDD.sample(true, 2)</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image107.jpg" alt="img">思考一个问题：有啥用，抽奖吗？</p><h5 id="9-distinct"><a href="#9-distinct" class="headerlink" title="9)   distinct"></a>9) distinct</h5><p>Ø 函数签名</p><p>def distinct()(implicit ord: Ordering[T] = null): RDD[T]</p><p>def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]</p><p>Ø 函数说明</p><p>将数据集中重复的数据去重</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4,1,2</p><p>),1)</p><p>val dataRDD1 = dataRDD.distinct()</p><p>val dataRDD2 = dataRDD.distinct(2)</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image108.jpg" alt="img">思考一个问题：如果不用该算子，你有什么办法实现数据去重？</p><h5 id="10-coalesce"><a href="#10-coalesce" class="headerlink" title="10)  coalesce"></a>10) coalesce</h5><p>Ø 函数签名</p><p>def coalesce(numPartitions: Int, shuffle: Boolean = false,</p><p>​ partitionCoalescer: Option[PartitionCoalescer] = Option.empty)</p><p>​ (implicit ord: Ordering[T] = null)</p><p>: RDD[T]</p><p>Ø 函数说明</p><p>根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率</p><p>当spark程序中，存在过多的小任务的时候，可以通过coalesce方法，收缩合并分区，减少分区的个数，减小任务调度成本</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4,1,2</p><p>),6)</p><p>val dataRDD1 = dataRDD.coalesce(2)</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg" alt="img">思考一个问题：我想要扩大分区，怎么办？</p><h5 id="11-repartition"><a href="#11-repartition" class="headerlink" title="11)  repartition"></a>11) repartition</h5><p>Ø 函数签名</p><p>def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]</p><p>Ø 函数说明</p><p>该操作内部其实执行的是coalesce操作，参数shuffle的默认值为true。无论是将分区数多的RDD转换为分区数少的RDD，还是将分区数少的RDD转换为分区数多的RDD，repartition操作都可以完成，因为无论如何都会经shuffle过程。</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4,1,2</p><p>),2)</p><p>val dataRDD1 = dataRDD.repartition(4)</p><p>思考一个问题：coalesce和repartition区别？</p><h5 id="12-sortBy"><a href="#12-sortBy" class="headerlink" title="12)  sortBy"></a>12) sortBy</h5><p>Ø 函数签名</p><p>def sortBy[K](</p><p>f: (T) =&gt; K,</p><p>ascending: Boolean = true,</p><p>numPartitions: Int = this.partitions.length)</p><p>(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]</p><p>Ø 函数说明</p><p>该操作用于排序数据。在排序之前，可以将数据通过f函数进行处理，之后按照f函数处理的结果进行排序，默认为升序排列。排序后新产生的RDD的分区数与原RDD的分区数一致。中间存在shuffle的过程</p><p>val dataRDD = sparkContext.makeRDD(List(</p><p>1,2,3,4,1,2</p><p>),2)</p><p>val dataRDD1 = dataRDD.sortBy(num=&gt;num, false, 4)</p><p>l 双Value类型</p><h5 id="13-intersection"><a href="#13-intersection" class="headerlink" title="13)  intersection"></a>13) intersection</h5><p>Ø 函数签名</p><p>def intersection(other: RDD[T]): RDD[T]</p><p>Ø 函数说明</p><p>对源RDD和参数RDD求交集后返回一个新的RDD</p><p>val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4))</p><p>val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6))</p><p>val dataRDD = dataRDD1.intersection(dataRDD2)</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image110.jpg" alt="img">思考一个问题：如果两个RDD数据类型不一致怎么办？</p><h5 id="14-union"><a href="#14-union" class="headerlink" title="14)  union"></a>14) union</h5><p>Ø 函数签名</p><p>def union(other: RDD[T]): RDD[T]</p><p>Ø 函数说明</p><p>对源RDD和参数RDD求并集后返回一个新的RDD</p><p>val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4))</p><p>val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6))</p><p>val dataRDD = dataRDD1.union(dataRDD2)</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg" alt="img">思考一个问题：如果两个RDD数据类型不一致怎么办？</p><h5 id="15-subtract"><a href="#15-subtract" class="headerlink" title="15)  subtract"></a>15) subtract</h5><p>Ø 函数签名</p><p>def subtract(other: RDD[T]): RDD[T]</p><p>Ø 函数说明</p><p>以一个RDD元素为主，去除两个RDD中重复元素，将其他元素保留下来。求差集</p><p>val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4))</p><p>val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6))</p><p>val dataRDD = dataRDD1.subtract(dataRDD2)</p><p>思考一个问题：如果两个RDD数据类型不一致怎么办？</p><h5 id="16-zip"><a href="#16-zip" class="headerlink" title="16)  zip"></a>16) zip</h5><p>Ø 函数签名</p><p>def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]</p><p>Ø 函数说明</p><p>将两个RDD中的元素，以键值对的形式进行合并。其中，键值对中的Key为第1个RDD中的元素，Value为第2个RDD中的相同位置的元素。</p><p>val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4))</p><p>val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6))</p><p>val dataRDD = dataRDD1.zip(dataRDD2)</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg" alt="img">思考一个问题：如果两个RDD数据类型不一致怎么办？</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg" alt="img">思考一个问题：如果两个RDD数据分区不一致怎么办？</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg" alt="img">思考一个问题：如果两个RDD分区数据数量不一致怎么办？</p><p>l Key - Value类型</p><h5 id="17-partitionBy"><a href="#17-partitionBy" class="headerlink" title="17)  partitionBy"></a>17) partitionBy</h5><p>Ø 函数签名</p><p>def partitionBy(partitioner: Partitioner): RDD[(K, V)]</p><p>Ø 函数说明</p><p>将数据按照指定Partitioner重新进行分区。Spark默认的分区器是HashPartitioner</p><p>val rdd: RDD[(Int, String)] =</p><p>sc.makeRDD(Array((1,”aaa”),(2,”bbb”),(3,”ccc”)),3)</p><p>import org.apache.spark.HashPartitioner</p><p>val rdd2: RDD[(Int, String)] =</p><p>rdd.partitionBy(new HashPartitioner(2))</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg" alt="img">思考一个问题：如果重分区的分区器和当前RDD的分区器一样怎么办？</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg" alt="img">思考一个问题：Spark还有其他分区器吗？</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg" alt="img">思考一个问题：如果想按照自己的方法进行数据分区怎么办？</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg" alt="img">思考一个问题：哪那么多问题？</p><h5 id="18-reduceByKey"><a href="#18-reduceByKey" class="headerlink" title="18)  reduceByKey"></a>18) reduceByKey</h5><p>Ø 函数签名</p><p>def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]</p><p>def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]</p><p>Ø 函数说明</p><p>可以将数据按照相同的Key对Value进行聚合</p><p>val dataRDD1 = sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val dataRDD2 = dataRDD1.reduceByKey(<em>+</em>)</p><p>val dataRDD3 = dataRDD1.reduceByKey(<em>+</em>, 2)</p><p>v 小功能：WordCount</p><h5 id="19-groupByKey"><a href="#19-groupByKey" class="headerlink" title="19)  groupByKey"></a>19) groupByKey</h5><p>Ø 函数签名</p><p>def groupByKey(): RDD[(K, Iterable[V])]</p><p>def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]</p><p>def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]</p><p>Ø 函数说明</p><p>将数据源的数据根据key对value进行分组</p><p>val dataRDD1 =</p><p>sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val dataRDD2 = dataRDD1.groupByKey()</p><p>val dataRDD3 = dataRDD1.groupByKey(2)</p><p>val dataRDD4 = dataRDD1.groupByKey(new HashPartitioner(2))</p><p>考一个问题：reduceByKey和groupByKey的区别？</p><p>从shuffle的角度：reduceByKey和groupByKey都存在shuffle的操作，但是reduceByKey可以在shuffle前对分区内相同key的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而groupByKey只是进行分组，不存在数据量减少的问题，reduceByKey性能比较高。</p><p>从功能的角度：reduceByKey其实包含分组和聚合的功能。groupByKey只能分组，不能聚合，所以在分组聚合的场合下，推荐使用reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用groupByKey</p><p>v 小功能：WordCount</p><h5 id="20-aggregateByKey"><a href="#20-aggregateByKey" class="headerlink" title="20)  aggregateByKey"></a>20) aggregateByKey</h5><p>Ø 函数签名</p><p>def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) =&gt; U,</p><p>combOp: (U, U) =&gt; U): RDD[(K, U)]</p><p>Ø 函数说明</p><p>将数据根据不同的规则进行分区内计算和分区间计算</p><p>val dataRDD1 =</p><p>sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val dataRDD2 =</p><p>dataRDD1.aggregateByKey(0)(<em>+</em>,<em>+</em>)</p><p>v 取出每个分区内相同key的最大值然后分区间相加</p><p>// TODO : 取出每个分区内相同key的最大值然后分区间相加</p><p>// aggregateByKey算子是函数柯里化，存在两个参数列表</p><p>// 1. 第一个参数列表中的参数表示初始值</p><p>// 2. 第二个参数列表中含有两个参数</p><p>// 2.1 第一个参数表示分区内的计算规则</p><p>// 2.2 第二个参数表示分区间的计算规则</p><p>val rdd =</p><p>sc.makeRDD(List(</p><p>​ (“a”,1),(“a”,2),(“c”,3),</p><p>​ (“b”,4),(“c”,5),(“c”,6)</p><p>),2)</p><p>// 0:(“a”,1),(“a”,2),(“c”,3) =&gt; (a,10)(c,10)</p><p>// =&gt; (a,10)(b,10)(c,20)</p><p>// 1:(“b”,4),(“c”,5),(“c”,6) =&gt; (b,10)(c,10)</p><p>val resultRDD =</p><p>rdd.aggregateByKey(10)(</p><p>​ (x, y) =&gt; math.max(x,y),</p><p>​ (x, y) =&gt; x + y</p><p>)</p><p>resultRDD.collect().foreach(println)</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg" alt="img">思考一个问题：分区内计算规则和分区间计算规则相同怎么办？（WordCount）</p><h5 id="21-foldByKey"><a href="#21-foldByKey" class="headerlink" title="21)  foldByKey"></a>21) foldByKey</h5><p>Ø 函数签名</p><p>def foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]</p><p>Ø 函数说明</p><p>当分区内计算规则和分区间计算规则相同时，aggregateByKey就可以简化为foldByKey</p><p>val dataRDD1 = sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val dataRDD2 = dataRDD1.foldByKey(0)(<em>+</em>)</p><h5 id="22-combineByKey"><a href="#22-combineByKey" class="headerlink" title="22)  combineByKey"></a>22) combineByKey</h5><p>Ø 函数签名</p><p>def combineByKey[C](</p><p>createCombiner: V =&gt; C,</p><p>mergeValue: (C, V) =&gt; C,</p><p>mergeCombiners: (C, C) =&gt; C): RDD[(K, C)]</p><p>Ø 函数说明</p><p>最通用的对key-value型rdd进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致。</p><p>小练习：将数据List((“a”, 88), (“b”, 95), (“a”, 91), (“b”, 93), (“a”, 95), (“b”, 98))求每个key的平均值</p><p>val list: List[(String, Int)] = List((“a”, 88), (“b”, 95), (“a”, 91), (“b”, 93), (“a”, 95), (“b”, 98))</p><p>val input: RDD[(String, Int)] = sc.makeRDD(list, 2)</p><p>val combineRdd: RDD[(String, (Int, Int))] = input.combineByKey(</p><p>(_, 1),</p><p>(acc: (Int, Int), v) =&gt; (acc._1 + v, acc._2 + 1),</p><p>(acc1: (Int, Int), acc2: (Int, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</p><p>)</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image112.jpg" alt="img">思考一个问题：reduceByKey、foldByKey、aggregateByKey、combineByKey的区别？</p><p>reduceByKey: 相同key的第一个数据不进行任何计算，分区内和分区间计算规则相同</p><p>foldByKey: 相同key的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同</p><p>aggregateByKey：相同key的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同</p><p>combineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同。</p><h5 id="23-sortByKey"><a href="#23-sortByKey" class="headerlink" title="23)  sortByKey"></a>23) sortByKey</h5><p>Ø 函数签名</p><p>def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)</p><p>: RDD[(K, V)]</p><p>Ø 函数说明</p><p>在一个(K,V)的RDD上调用，K必须实现Ordered接口(特质)，返回一个按照key进行排序的</p><p>val dataRDD1 = sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(true)</p><p>val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(false)</p><p>v 小功能：设置key为自定义类User</p><h5 id="24-join"><a href="#24-join" class="headerlink" title="24)  join"></a>24) join</h5><p>Ø 函数签名</p><p>def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]</p><p>Ø 函数说明</p><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素连接在一起的(K,(V,W))的RDD</p><p>val rdd: RDD[(Int, String)] = sc.makeRDD(Array((1, “a”), (2, “b”), (3, “c”)))</p><p>val rdd1: RDD[(Int, Int)] = sc.makeRDD(Array((1, 4), (2, 5), (3, 6)))</p><p>rdd.join(rdd1).collect().foreach(println)</p><p><img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg" alt="img">思考一个问题：如果key存在不相等呢？</p><h5 id="25-leftOuterJoin"><a href="#25-leftOuterJoin" class="headerlink" title="25)  leftOuterJoin"></a>25) leftOuterJoin</h5><p>Ø 函数签名</p><p>def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]</p><p>Ø 函数说明</p><p>类似于SQL语句的左外连接</p><p>val dataRDD1 = sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val dataRDD2 = sparkContext.makeRDD(List((“a”,1),(“b”,2),(“c”,3)))</p><p>val rdd: RDD[(String, (Int, Option[Int]))] = dataRDD1.leftOuterJoin(dataRDD2)</p><h5 id="26-cogroup"><a href="#26-cogroup" class="headerlink" title="26)  cogroup"></a>26) cogroup</h5><p>Ø 函数签名</p><p>def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]</p><p>Ø 函数说明</p><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<v>,Iterable<w>))类型的RDD</w></v></p><p>val dataRDD1 = sparkContext.makeRDD(List((“a”,1),(“a”,2),(“c”,3)))</p><p>val dataRDD2 = sparkContext.makeRDD(List((“a”,1),(“c”,2),(“c”,3)))</p><p>val value: RDD[(String, (Iterable[Int], Iterable[Int]))] =</p><p>dataRDD1.cogroup(dataRDD2)</p><h4 id="2-1-4-4-案例实操"><a href="#2-1-4-4-案例实操" class="headerlink" title="2.1.4.4 案例实操"></a>2.1.4.4 案例实操</h4><ol><li>数据准备</li></ol><p>agent.log：时间戳，省份，城市，用户，广告，中间字段使用空格分隔。</p><ol start="2"><li>需求描述</li></ol><p>统计出每一个省份每个广告被点击数量排行的Top3</p><ol start="3"><li><p>需求分析</p></li><li><p>功能实现</p></li></ol><h4 id="2-1-4-5-RDD行动算子"><a href="#2-1-4-5-RDD行动算子" class="headerlink" title="2.1.4.5 RDD行动算子"></a>2.1.4.5 RDD行动算子</h4><h5 id="1-reduce"><a href="#1-reduce" class="headerlink" title="1)   reduce"></a>1) reduce</h5><p>Ø 函数签名</p><p>def reduce(f: (T, T) =&gt; T): T</p><p>Ø 函数说明</p><p><strong>聚集RDD中的所有元素，先聚合分区内数据，再聚合分区间数据</strong></p><p>val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))</p><p>// 聚合数据</p><p>val reduceResult: Int = rdd.reduce(<em>+</em>)</p><h5 id="2-collect"><a href="#2-collect" class="headerlink" title="2)   collect"></a>2) collect</h5><p>Ø 函数签名</p><p>def collect(): Array[T]</p><p>Ø 函数说明</p><p>在驱动程序（Driver）中，以数组Array的形式返回数据集的所有元素</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))</p><p>// 收集数据到Driver</p><p>rdd.collect().foreach(println)</p><h5 id="3-count"><a href="#3-count" class="headerlink" title="3)   count"></a>3) count</h5><p>Ø 函数签名</p><p>def count(): Long</p><p>Ø 函数说明</p><p>返回RDD中元素的个数</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))</p><p>// 返回RDD中元素的个数</p><p>val countResult: Long = rdd.count()</p><h5 id="4-first"><a href="#4-first" class="headerlink" title="4)   first"></a>4) first</h5><p>Ø 函数签名</p><p>def first(): T</p><p>Ø 函数说明</p><p>返回RDD中的第一个元素</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))</p><p>// 返回RDD中元素的个数</p><p>val firstResult: Int = rdd.first()</p><p>println(firstResult)</p><h5 id="5-take"><a href="#5-take" class="headerlink" title="5)   take"></a>5) take</h5><p>Ø 函数签名</p><p>def take(num: Int): Array[T]</p><p>Ø 函数说明</p><p>返回一个由RDD的前n个元素组成的数组</p><p>vval rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))</p><p>// 返回RDD中元素的个数</p><p>val takeResult: Array[Int] = rdd.take(2)</p><p>println(takeResult.mkString(“,”))</p><h5 id="6-takeOrdered"><a href="#6-takeOrdered" class="headerlink" title="6)   takeOrdered"></a>6) takeOrdered</h5><p>Ø 函数签名</p><p>def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]</p><p>Ø 函数说明</p><p>返回该RDD排序后的前n个元素组成的数组</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1,3,2,4))</p><p>// 返回RDD中元素的个数</p><p>val result: Array[Int] = rdd.takeOrdered(2)</p><h5 id="7-aggregate"><a href="#7-aggregate" class="headerlink" title="7)   aggregate"></a>7) aggregate</h5><p>Ø 函数签名</p><p>def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U</p><p>Ø 函数说明</p><p>分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4), 8)</p><p>// 将该RDD所有元素相加得到结果</p><p>//val result: Int = rdd.aggregate(0)(_ + _, _ + _)</p><p>val result: Int = rdd.aggregate(10)(_ + _, _ + _)</p><h5 id="8-fold"><a href="#8-fold" class="headerlink" title="8)   fold"></a>8) fold</h5><p>Ø 函数签名</p><p>def fold(zeroValue: T)(op: (T, T) =&gt; T): T</p><p>Ø 函数说明</p><p>折叠操作，aggregate的简化版操作</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))</p><p>val foldResult: Int = rdd.fold(0)(<em>+</em>)</p><h5 id="9-countByKey"><a href="#9-countByKey" class="headerlink" title="9)   countByKey"></a>9) countByKey</h5><p>Ø 函数签名</p><p>def countByKey(): Map[K, Long]</p><p>Ø 函数说明</p><p>统计每种key的个数</p><p>val rdd: RDD[(Int, String)] = sc.makeRDD(List((1, “a”), (1, “a”), (1, “a”), (2, “b”), (3, “c”), (3, “c”)))</p><p>// 统计每种key的个数</p><p>val result: collection.Map[Int, Long] = rdd.countByKey()</p><h5 id="10-save相关算子"><a href="#10-save相关算子" class="headerlink" title="10)  save相关算子"></a>10) save相关算子</h5><p>Ø 函数签名</p><p>def saveAsTextFile(path: String): Unit</p><p>def saveAsObjectFile(path: String): Unit</p><p>def saveAsSequenceFile(</p><p>path: String,</p><p>codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit</p><p>Ø 函数说明</p><p>将数据保存到不同格式的文件中</p><p>// 保存成Text文件</p><p>rdd.saveAsTextFile(“output”)</p><p>// 序列化成对象保存到文件</p><p>rdd.saveAsObjectFile(“output1”)</p><p>// 保存成Sequencefile文件</p><p>rdd.map((_,1)).saveAsSequenceFile(“output2”)</p><h5 id="11-foreach"><a href="#11-foreach" class="headerlink" title="11)  foreach"></a>11) foreach</h5><p>Ø 函数签名</p><p>def foreach(f: T =&gt; Unit): Unit = withScope {</p><p>val cleanF = sc.clean(f)</p><p>sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF))</p><p>}</p><p>Ø 函数说明</p><p>分布式遍历RDD中的每一个元素，调用指定函数</p><p>val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))</p><p>// 收集后打印</p><p>rdd.map(num=&gt;num).collect().foreach(println)</p><p>println(“<strong><strong><strong>****</strong></strong></strong>“)</p><p>// 分布式打印</p><p>rdd.foreach(println)</p><h4 id="2-1-4-6-RDD序列化"><a href="#2-1-4-6-RDD序列化" class="headerlink" title="2.1.4.6 RDD序列化"></a>2.1.4.6 RDD序列化</h4><ol><li>闭包检查</li></ol><p>从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行。那么在scala的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12版本后闭包编译方式发生了改变</p><ol start="2"><li>序列化方法和属性</li></ol><p>从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行，看如下代码：</p><p>object serializable02_function {</p><p>def main(args: Array[String]): Unit = {</p><p>​ //1.创建SparkConf并设置App名称</p><p>​ val conf: SparkConf = new SparkConf().setAppName(“SparkCoreTest”).setMaster(“local[*]”)</p><p>​ //2.创建SparkContext，该对象是提交Spark App的入口</p><p>​ val sc: SparkContext = new SparkContext(conf)</p><p>​ //3.创建一个RDD</p><p>​ val rdd: RDD[String] = sc.makeRDD(Array(“hello world”, “hello spark”, “hive”, “atguigu”))</p><p>​ //3.1创建一个Search对象</p><p>​ val search = new Search(“hello”)</p><p>​ //3.2 函数传递，打印：ERROR Task not serializable</p><p>​ search.getMatch1(rdd).collect().foreach(println)</p><p>​ //3.3 属性传递，打印：ERROR Task not serializable</p><p>​ search.getMatch2(rdd).collect().foreach(println)</p><p>​ //4.关闭连接</p><p>​ sc.stop()</p><p>}</p><p>}</p><p>class Search(query:String) extends Serializable {</p><p>def isMatch(s: String): Boolean = {</p><p>​ s.contains(query)</p><p>}</p><p>// 函数序列化案例</p><p>def getMatch1 (rdd: RDD[String]): RDD[String] = {</p><p>​ //rdd.filter(this.isMatch)</p><p>​ rdd.filter(isMatch)</p><p>}</p><p>// 属性序列化案例</p><p>def getMatch2(rdd: RDD[String]): RDD[String] = {</p><p>​ //rdd.filter(x =&gt; x.contains(this.query))</p><p>​ rdd.filter(x =&gt; x.contains(query))</p><p>​ //val q = query</p><p>​ //rdd.filter(x =&gt; x.contains(q))</p><p>}</p><p>}</p><ol start="3"><li>Kryo序列化框架</li></ol><p>参考地址: <a target="_blank" rel="noopener" href="https://github.com/EsotericSoftware/kryo">https://github.com/EsotericSoftware/kryo</a></p><p>Java的序列化能够序列化任何的类。但是比较重（字节多），序列化后，对象的提交也比较大。Spark出于性能的考虑，Spark2.0开始支持另外一种Kryo序列化机制。Kryo速度是Serializable的10倍。当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kryo来序列化。</p><p>注意：即使使用Kryo序列化，也要继承Serializable接口。</p><p>object serializable_Kryo {</p><p>def main(args: Array[String]): Unit = {</p><p>​ val conf: SparkConf = new SparkConf()</p><p>​ .setAppName(“SerDemo”)</p><p>​ .setMaster(“local[*]”)</p><p>​ // 替换默认的序列化机制</p><p>​ .set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”)</p><p>​ // 注册需要使用 kryo 序列化的自定义类</p><p>​ .registerKryoClasses(Array(classOf[Searcher]))</p><p>​ val sc = new SparkContext(conf)</p><p>​ val rdd: RDD[String] = sc.makeRDD(Array(“hello world”, “hello atguigu”, “atguigu”, “hahah”), 2)</p><p>​ val searcher = new Searcher(“hello”)</p><p>​ val result: RDD[String] = searcher.getMatchedRDD1(rdd)</p><p>​ result.collect.foreach(println)</p><p>}</p><p>}</p><p>case class Searcher(val query: String) {</p><p>def isMatch(s: String) = {</p><p>​ s.contains(query)</p><p>}</p><p>def getMatchedRDD1(rdd: RDD[String]) = {</p><p>​ rdd.filter(isMatch)</p><p>}</p><p>def getMatchedRDD2(rdd: RDD[String]) = {</p><p>​ val q = query</p><p>​ rdd.filter(_.contains(q))</p><p>}</p><p>}</p><h4 id="2-1-4-7-RDD依赖关系"><a href="#2-1-4-7-RDD依赖关系" class="headerlink" title="2.1.4.7 RDD依赖关系"></a>2.1.4.7 RDD依赖关系</h4><ol><li>RDD 血缘关系</li></ol><p>RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p><p>val fileRDD: RDD[String] = sc.textFile(“input/1.txt”)</p><p>println(fileRDD.toDebugString)</p><p>println(“———————-“)</p><p>val wordRDD: RDD[String] = fileRDD.flatMap(_.split(“ “))</p><p>println(wordRDD.toDebugString)</p><p>println(“———————-“)</p><p>val mapRDD: RDD[(String, Int)] = wordRDD.map((_,1))</p><p>println(mapRDD.toDebugString)</p><p>println(“———————-“)</p><p>val resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(<em>+</em>)</p><p>println(resultRDD.toDebugString)</p><p>resultRDD.collect()</p><ol start="2"><li>RDD 依赖关系</li></ol><p>这里所谓的依赖关系，其实就是两个相邻RDD之间的关系</p><p>val sc: SparkContext = new SparkContext(conf)</p><p>val fileRDD: RDD[String] = sc.textFile(“input/1.txt”)</p><p>println(fileRDD.dependencies)</p><p>println(“———————-“)</p><p>val wordRDD: RDD[String] = fileRDD.flatMap(_.split(“ “))</p><p>println(wordRDD.dependencies)</p><p>println(“———————-“)</p><p>val mapRDD: RDD[(String, Int)] = wordRDD.map((_,1))</p><p>println(mapRDD.dependencies)</p><p>println(“———————-“)</p><p>val resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(<em>+</em>)</p><p>println(resultRDD.dependencies)</p><p>resultRDD.collect()</p><ol start="3"><li>RDD 窄依赖</li></ol><p>窄依赖表示每一个父(上游)RDD的Partition最多被子（下游）RDD的一个Partition使用，窄依赖我们形象的比喻为独生子女。</p><p>class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency<a href="rdd">T</a></p><ol start="4"><li>RDD 宽依赖</li></ol><p>宽依赖表示同一个父（上游）RDD的Partition被多个子（下游）RDD的Partition依赖，会引起Shuffle，总结：宽依赖我们形象的比喻为多生。</p><p>class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](</p><p>@transient private val <em>rdd: RDD[</em> &lt;: Product2[K, V]],</p><p>val partitioner: Partitioner,</p><p>val serializer: Serializer = SparkEnv.get.serializer,</p><p>val keyOrdering: Option[Ordering[K]] = None,</p><p>val aggregator: Option[Aggregator[K, V, C]] = None,</p><p>val mapSideCombine: Boolean = false)</p><p>extends Dependency[Product2[K, V]]</p><ol start="5"><li>RDD 阶段划分</li></ol><p>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。例如，DAG记录了RDD的转换过程和任务的阶段。</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image113.jpg" alt="img"> <img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image115.jpg" alt="img"> <img src="file:///C:/Users/MENGLI~1.ZHA/AppData/Local/Temp/msohtmlclip1/01/clip_image117.jpg" alt="img"></p><ol start="6"><li>RDD 阶段划分源码</li></ol><p>try {</p><p>// New stage creation may throw an exception if, for example, jobs are run on a</p><p>// HadoopRDD whose underlying HDFS files have been deleted.</p><p>finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</p><p>} catch {</p><p>case e: Exception =&gt;</p><p>logWarning(“Creating new stage failed due to exception - job: “ + jobId, e)</p><p>listener.jobFailed(e)</p><p>return</p><p>}</p><p>……</p><p>private def createResultStage(</p><p>rdd: RDD[_],</p><p>func: (TaskContext, Iterator[_]) =&gt; _,</p><p>partitions: Array[Int],</p><p>jobId: Int,</p><p>callSite: CallSite): ResultStage = {</p><p>val parents = getOrCreateParentStages(rdd, jobId)</p><p>val id = nextStageId.getAndIncrement()</p><p>val stage = new ResultStage(id, rdd, func, partitions, parents, jobId, callSite)</p><p>stageIdToStage(id) = stage</p><p>updateJobIdStageIdMaps(jobId, stage)</p><p>stage</p><p>}</p><p>……</p><p>private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {</p><p>getShuffleDependencies(rdd).map { shuffleDep =&gt;</p><p>getOrCreateShuffleMapStage(shuffleDep, firstJobId)</p><p>}.toList</p><p>}</p><p>……</p><p>private[scheduler] def getShuffleDependencies(</p><p>rdd: RDD[<em>]): HashSet[ShuffleDependency[</em>, _, _]] = {</p><p>val parents = new HashSet[ShuffleDependency[_, _, _]]</p><p>val visited = new HashSet[RDD[_]]</p><p>val waitingForVisit = new Stack[RDD[_]]</p><p>waitingForVisit.push(rdd)</p><p>while (waitingForVisit.nonEmpty) {</p><p>val toVisit = waitingForVisit.pop()</p><p>if (!visited(toVisit)) {</p><p>visited += toVisit</p><p>toVisit.dependencies.foreach {</p><p>case shuffleDep: ShuffleDependency[_, _, _] =&gt;</p><p>​ parents += shuffleDep</p><p>case dependency =&gt;</p><p>​ waitingForVisit.push(dependency.rdd)</p><p>}</p><p>}</p><p>}</p><p>parents</p><p>}</p><ol start="7"><li>RDD 任务划分</li></ol><p>RDD任务切分中间分为：Application、Job、Stage和Task</p><p>l Application：初始化一个SparkContext即生成一个Application；</p><p>l Job：一个Action算子就会生成一个Job；</p><p>l Stage：Stage等于宽依赖(ShuffleDependency)的个数加1；</p><p>l Task：一个Stage阶段中，最后一个RDD的分区个数就是Task的个数。</p><p>注意：Application-&gt;Job-&gt;Stage-&gt;Task每一层都是1对n的关系。</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image119.gif" alt="img"></p><ol start="8"><li>RDD 任务划分源码</li></ol><p>val tasks: Seq[Task[_]] = try {</p><p>stage match {</p><p>case stage: ShuffleMapStage =&gt;</p><p>partitionsToCompute.map { id =&gt;</p><p>​ val locs = taskIdToLocations(id)</p><p>​ val part = stage.rdd.partitions(id)</p><p>​ new ShuffleMapTask(stage.id, stage.latestInfo.attemptId,</p><p>​ taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, Option(jobId),</p><p>​ Option(sc.applicationId), sc.applicationAttemptId)</p><p>}</p><p>case stage: ResultStage =&gt;</p><p>partitionsToCompute.map { id =&gt;</p><p>​ val p: Int = stage.partitions(id)</p><p>​ val part = stage.rdd.partitions(p)</p><p>​ val locs = taskIdToLocations(id)</p><p>​ new ResultTask(stage.id, stage.latestInfo.attemptId,</p><p>​ taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics,</p><p>​ Option(jobId), Option(sc.applicationId), sc.applicationAttemptId)</p><p>}</p><p>}</p><p>……</p><p>val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()</p><p>……</p><p>override def findMissingPartitions(): Seq[Int] = {</p><p>mapOutputTrackerMaster</p><p>.findMissingPartitions(shuffleDep.shuffleId)</p><p>.getOrElse(0 until numPartitions)</p><p>}</p><h4 id="2-1-4-8-RDD持久化"><a href="#2-1-4-8-RDD持久化" class="headerlink" title="2.1.4.8 RDD持久化"></a>2.1.4.8 RDD持久化</h4><ol><li>RDD Cache缓存</li></ol><p>RDD通过Cache或者Persist方法将前面的计算结果缓存，默认情况下会把数据以缓存在JVM的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action算子时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p><p>// cache操作会增加血缘关系，不改变原有的血缘关系</p><p>println(wordToOneRdd.toDebugString)</p><p>// 数据缓存。</p><p>wordToOneRdd.cache()</p><p>// 可以更改存储级别</p><p>//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)</p><p>存储级别</p><p>object StorageLevel {</p><p>val NONE = new StorageLevel(false, false, false, false)</p><p>val DISK_ONLY = new StorageLevel(true, false, false, false)</p><p>val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)</p><p>val MEMORY_ONLY = new StorageLevel(false, true, false, true)</p><p>val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)</p><p>val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)</p><p>val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)</p><p>val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)</p><p>val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)</p><p>val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)</p><p>val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</p><p>val OFF_HEAP = new StorageLevel(true, true, true, false, 1)</p><p><img src="/2020/11/29/bigdata-Spark2-framework/clip_image121.gif" alt="img"></p><p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p><p>Spark会自动对一些Shuffle操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点Shuffle失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用persist或cache。</p><ol start="2"><li>RDD CheckPoint检查点</li></ol><p>所谓的检查点其实就是通过将RDD中间结果写入磁盘</p><p>由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。</p><p>对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p><p>// 设置检查点路径</p><p>sc.setCheckpointDir(“./checkpoint1”)</p><p>// 创建一个RDD，读取指定位置文件:hello atguigu atguigu</p><p>val lineRdd: RDD[String] = sc.textFile(“input/1.txt”)</p><p>// 业务逻辑</p><p>val wordRdd: RDD[String] = lineRdd.flatMap(line =&gt; line.split(“ “))</p><p>val wordToOneRdd: RDD[(String, Long)] = wordRdd.map {</p><p>word =&gt; {</p><p>​ (word, System.currentTimeMillis())</p><p>}</p><p>}</p><p>// 增加缓存,避免再重新跑一个job做checkpoint</p><p>wordToOneRdd.cache()</p><p>// 数据检查点：针对wordToOneRdd做检查点计算</p><p>wordToOneRdd.checkpoint()</p><p>// 触发执行逻辑</p><p>wordToOneRdd.collect().foreach(println)</p><ol start="3"><li>缓存和检查点区别</li></ol><p>1）Cache缓存只是将数据保存起来，不切断血缘依赖。Checkpoint检查点切断血缘依赖。</p><p>2）Cache缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint的数据通常存储在HDFS等容错、高可用的文件系统，可靠性高。</p><p>3）建议对checkpoint()的RDD使用Cache缓存，这样checkpoint的job只需从Cache缓存中读取数据即可，否则需要再从头计算一次RDD。</p><h4 id="2-1-4-9-RDD分区器"><a href="#2-1-4-9-RDD分区器" class="headerlink" title="2.1.4.9 RDD分区器"></a>2.1.4.9 RDD分区器</h4><p>Spark目前支持Hash分区和Range分区，和用户自定义分区。Hash分区为当前的默认分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle后进入哪个分区，进而决定了Reduce的个数。</p><p>Ø 只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None</p><p>Ø 每个RDD的分区ID范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。</p><ol><li>Hash分区：对于给定的key，计算其hashCode,并除以分区个数取余</li></ol><p>class HashPartitioner(partitions: Int) extends Partitioner {</p><p>require(partitions &gt;= 0, s”Number of partitions ($partitions) cannot be negative.”)</p><p>def numPartitions: Int = partitions</p><p>def getPartition(key: Any): Int = key match {</p><p>case null =&gt; 0</p><p>case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions)</p><p>}</p><p>override def equals(other: Any): Boolean = other match {</p><p>case h: HashPartitioner =&gt;</p><p>h.numPartitions == numPartitions</p><p>case _ =&gt;</p><p>false</p><p>}</p><p>override def hashCode: Int = numPartitions</p><p>}</p><ol start="2"><li>Range分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</li></ol><p>class RangePartitioner[K : Ordering : ClassTag, V](</p><p>partitions: Int,</p><p>rdd: RDD[_ &lt;: Product2[K, V]],</p><p>private var ascending: Boolean = true)</p><p>extends Partitioner {</p><p>// We allow partitions = 0, which happens when sorting an empty RDD under the default settings.</p><p>require(partitions &gt;= 0, s”Number of partitions cannot be negative but found $partitions.”)</p><p>private var ordering = implicitly[Ordering[K]]</p><p>// An array of upper bounds for the first (partitions - 1) partitions</p><p>private var rangeBounds: Array[K] = {</p><p>…</p><p>}</p><p>def numPartitions: Int = rangeBounds.length + 1</p><p>private var binarySearch: ((Array[K], K) =&gt; Int) = CollectionsUtils.makeBinarySearch[K]</p><p>def getPartition(key: Any): Int = {</p><p>val k = key.asInstanceOf[K]</p><p>var partition = 0</p><p>if (rangeBounds.length &lt;= 128) {</p><p>// If we have less than 128 partitions naive search</p><p>​ while (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) {</p><p>​ partition += 1</p><p>}</p><p>} else {</p><p>// Determine which binary search method to use only once.</p><p>partition = binarySearch(rangeBounds, k)</p><p>// binarySearch either returns the match location or -[insertion point]-1</p><p>if (partition &lt; 0) {</p><p>​ partition = -partition-1</p><p>}</p><p>if (partition &gt; rangeBounds.length) {</p><p>​ partition = rangeBounds.length</p><p>}</p><p>}</p><p>if (ascending) {</p><p>partition</p><p>} else {</p><p>rangeBounds.length - partition</p><p>}</p><p>}</p><p>override def equals(other: Any): Boolean = other match {</p><p>…</p><p>}</p><p>override def hashCode(): Int = {</p><p>…</p><p>}</p><p>@throws(classOf[IOException])</p><p>private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException {</p><p>…</p><p>}</p><p>@throws(classOf[IOException])</p><p>private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {</p><p>…</p><p>}</p><p>}</p><h4 id="2-1-4-10-RDD文件读取与保存"><a href="#2-1-4-10-RDD文件读取与保存" class="headerlink" title="2.1.4.10 RDD文件读取与保存"></a>2.1.4.10 RDD文件读取与保存</h4><p>Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。</p><p>文件格式分为：text文件、csv文件、sequence文件以及Object文件；</p><p>文件系统分为：本地文件系统、HDFS、HBASE以及数据库。</p><p>Ø text文件</p><p>// 读取输入文件</p><p>val inputRDD: RDD[String] = sc.textFile(“input/1.txt”)</p><p>// 保存数据</p><p>inputRDD.saveAsTextFile(“output”)</p><p>Ø sequence文件</p><p>SequenceFile文件是<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/hadoop">Hadoop</a>用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。在SparkContext中，可以调用sequenceFile<a href="path">keyClass, valueClass</a>。</p><p>// 保存数据为SequenceFile</p><p>dataRDD.saveAsSequenceFile(“output”)</p><p>// 读取SequenceFile文件</p><p>sc.sequenceFile<a href="%22output%22">Int,Int</a>.collect().foreach(println)</p><p>Ø object对象文件</p><p>对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFile<a href="path">T: ClassTag</a>函数接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。因为是序列化所以要指定类型。</p><p>// 保存数据</p><p>dataRDD.saveAsObjectFile(“output”)</p><p>// 读取数据</p><p>sc.objectFile<a href="%22output%22">Int</a>.collect().foreach(println)</p><h2 id="2-2-累加器"><a href="#2-2-累加器" class="headerlink" title="2.2 累加器"></a>2.2 累加器</h2><h3 id="2-2-1-实现原理"><a href="#2-2-1-实现原理" class="headerlink" title="2.2.1 实现原理"></a>2.2.1 实现原理</h3><p>累加器用来把Executor端变量信息聚合到Driver端。在Driver程序中定义的变量，在Executor端的每个Task都会得到这个变量的一份新的副本，每个task更新这些副本的值后，传回Driver端进行merge。</p><h3 id="2-2-2-基础编程"><a href="#2-2-2-基础编程" class="headerlink" title="2.2.2 基础编程"></a>2.2.2 基础编程</h3><h4 id="2-2-2-1-系统累加器"><a href="#2-2-2-1-系统累加器" class="headerlink" title="2.2.2.1 系统累加器"></a>2.2.2.1 系统累加器</h4><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true">// 声明累加器</span>
<span class="token keyword">var</span> sum <span class="token operator">=</span> sc<span class="token punctuation">.</span>longAccumulator<span class="token punctuation">(</span><span class="token string">"sum"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
rdd<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>
  num <span class="token keyword">=></span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token comment" spellcheck="true">// 使用累加器</span>
    sum<span class="token punctuation">.</span>add<span class="token punctuation">(</span>num<span class="token punctuation">)</span>
  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true">// 获取累加器的值</span>
println<span class="token punctuation">(</span><span class="token string">"sum = "</span> <span class="token operator">+</span> sum<span class="token punctuation">.</span>value<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-2-2-2-自定义累加器"><a href="#2-2-2-2-自定义累加器" class="headerlink" title="2.2.2.2 自定义累加器"></a>2.2.2.2 自定义累加器</h4><pre class="line-numbers language-scala"><code class="language-scala"><span class="token comment" spellcheck="true">// 自定义累加器</span>
<span class="token comment" spellcheck="true">// 1. 继承AccumulatorV2，并设定泛型</span>
<span class="token comment" spellcheck="true">// 2. 重写累加器的抽象方法</span>
<span class="token keyword">class</span> WordCountAccumulator <span class="token keyword">extends</span> AccumulatorV2<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Long</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

<span class="token keyword">var</span> map <span class="token operator">:</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Long</span><span class="token punctuation">]</span> <span class="token operator">=</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true">// 累加器是否为初始状态</span>
<span class="token keyword">override</span> <span class="token keyword">def</span> isZero<span class="token operator">:</span> <span class="token builtin">Boolean</span> <span class="token operator">=</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
  map<span class="token punctuation">.</span>isEmpty
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// 复制累加器</span>
<span class="token keyword">override</span> <span class="token keyword">def</span> copy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> AccumulatorV2<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Long</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
  <span class="token keyword">new</span> WordCountAccumulator
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// 重置累加器</span>
<span class="token keyword">override</span> <span class="token keyword">def</span> reset<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
  map<span class="token punctuation">.</span>clear<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// 向累加器中增加数据 (In)</span>
<span class="token keyword">override</span> <span class="token keyword">def</span> add<span class="token punctuation">(</span>word<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token comment" spellcheck="true">// 查询map中是否存在相同的单词</span>
    <span class="token comment" spellcheck="true">// 如果有相同的单词，那么单词的数量加1</span>
    <span class="token comment" spellcheck="true">// 如果没有相同的单词，那么在map中增加这个单词</span>
    map<span class="token punctuation">(</span>word<span class="token punctuation">)</span> <span class="token operator">=</span> map<span class="token punctuation">.</span>getOrElse<span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">0L</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1L</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// 合并累加器</span>
<span class="token keyword">override</span> <span class="token keyword">def</span> merge<span class="token punctuation">(</span>other<span class="token operator">:</span> AccumulatorV2<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Long</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

  <span class="token keyword">val</span> map1 <span class="token operator">=</span> map
  <span class="token keyword">val</span> map2 <span class="token operator">=</span> other<span class="token punctuation">.</span>value

  <span class="token comment" spellcheck="true">// 两个Map的合并</span>
  map <span class="token operator">=</span> map1<span class="token punctuation">.</span>foldLeft<span class="token punctuation">(</span>map2<span class="token punctuation">)</span><span class="token punctuation">(</span>
    <span class="token punctuation">(</span> innerMap<span class="token punctuation">,</span> kv <span class="token punctuation">)</span> <span class="token keyword">=></span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
      innerMap<span class="token punctuation">(</span>kv<span class="token punctuation">.</span>_1<span class="token punctuation">)</span> <span class="token operator">=</span> innerMap<span class="token punctuation">.</span>getOrElse<span class="token punctuation">(</span>kv<span class="token punctuation">.</span>_1<span class="token punctuation">,</span> <span class="token number">0L</span><span class="token punctuation">)</span> <span class="token operator">+</span> kv<span class="token punctuation">.</span>_2
      innerMap
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
  <span class="token punctuation">)</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// 返回累加器的结果 （Out）</span>
<span class="token keyword">override</span> <span class="token keyword">def</span> value<span class="token operator">:</span> mutable<span class="token punctuation">.</span>Map<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Long</span><span class="token punctuation">]</span> <span class="token operator">=</span> map
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-3-广播变量"><a href="#2-3-广播变量" class="headerlink" title="2.3 广播变量"></a>2.3 广播变量</h2><h3 id="2-3-1-实现原理"><a href="#2-3-1-实现原理" class="headerlink" title="2.3.1 实现原理"></a>2.3.1 实现原理</h3><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。</p><h3 id="2-3-2-基础编程"><a href="#2-3-2-基础编程" class="headerlink" title="2.3.2 基础编程"></a>2.3.2 基础编程</h3><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> rdd1 <span class="token operator">=</span> sc<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>List<span class="token punctuation">(</span> <span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"b"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"c"</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"d"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> list <span class="token operator">=</span> List<span class="token punctuation">(</span> <span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"b"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"c"</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"d"</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span> <span class="token punctuation">)</span>
<span class="token comment" spellcheck="true">// 声明广播变量</span>
<span class="token keyword">val</span> broadcast<span class="token operator">:</span> Broadcast<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> sc<span class="token punctuation">.</span>broadcast<span class="token punctuation">(</span>list<span class="token punctuation">)</span>

<span class="token keyword">val</span> resultRDD<span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">Int</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> rdd1<span class="token punctuation">.</span>map <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
  <span class="token keyword">case</span> <span class="token punctuation">(</span>key<span class="token punctuation">,</span> num<span class="token punctuation">)</span> <span class="token keyword">=></span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token keyword">var</span> num2 <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token comment" spellcheck="true">// 使用广播变量</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> v<span class="token punctuation">)</span> <span class="token keyword">&lt;-</span> broadcast<span class="token punctuation">.</span>value<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token keyword">if</span> <span class="token punctuation">(</span>k <span class="token operator">==</span> key<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        num2 <span class="token operator">=</span> v
      <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token punctuation">(</span>key<span class="token punctuation">,</span> <span class="token punctuation">(</span>num<span class="token punctuation">,</span> num2<span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div></div><div class="copyright"><p><span>本文标题:</span><a href="/2020/11/29/bigdata-Spark2-framework/">Spark学习笔记（二） 架构解析和RDD编程</a></p><p><span>文章作者:</span><a href="/" title="回到主页">m01ly</a></p><p><span>发布时间:</span>2020-11-29, 17:29:54</p><p><span>最后更新:</span>2021-11-30, 10:12:28</p><p><span>原始链接:</span><a class="post-url" href="/2020/11/29/bigdata-Spark2-framework/" title="Spark学习笔记（二） 架构解析和RDD编程">https://m01ly.github.io/2020/11/29/bigdata-Spark2-framework/</a></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target="_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。</p></div><nav id="article-nav"><div id="article-nav-newer" class="article-nav-title"><a href="/2020/12/01/bigdata-zookeeper1-setup/">Zookeeper学习笔记（一） 搭建教程</a></div><div id="article-nav-older" class="article-nav-title"><a href="/2020/11/29/bigdata-Spark1-setup/">Spark学习笔记（一） 搭建Spark</a></div></nav></article><div id="toc" class="toc-article"><strong class="toc-title">文章目录</strong><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Spark%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84"><span class="toc-text">1 Spark运行架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84"><span class="toc-text">1.1 运行架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-text">1.2 核心组件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-1-Driver"><span class="toc-text">1.2.1 Driver</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-2-Executor"><span class="toc-text">1.2.2 Executor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-3-Master-amp-Worker"><span class="toc-text">1.2.3 Master &amp; Worker</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-4-ApplicationMaster"><span class="toc-text">1.2.4 ApplicationMaster</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-text">1.3 核心概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-1-Executor%E4%B8%8ECore%EF%BC%88%E6%A0%B8%EF%BC%89"><span class="toc-text">1.3.1 Executor与Core（核）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-2-%E5%B9%B6%E8%A1%8C%E5%BA%A6%EF%BC%88Parallelism%EF%BC%89"><span class="toc-text">1.3.2 并行度（Parallelism）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-3-%E6%9C%89%E5%90%91%E6%97%A0%E7%8E%AF%E5%9B%BE%EF%BC%88DAG%EF%BC%89"><span class="toc-text">1.3.3 有向无环图（DAG）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B"><span class="toc-text">1.4 提交流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-1-Yarn-Client%E6%A8%A1%E5%BC%8F"><span class="toc-text">1.2.1 Yarn Client模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-2-Yarn-Cluster%E6%A8%A1%E5%BC%8F"><span class="toc-text">1.2.2 Yarn Cluster模式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B"><span class="toc-text">2 Spark核心编程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-RDD"><span class="toc-text">2.1 RDD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1-%E4%BB%80%E4%B9%88%E6%98%AFRDD"><span class="toc-text">2.1.1 什么是RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-%E6%A0%B8%E5%BF%83%E5%B1%9E%E6%80%A7"><span class="toc-text">2.1.2 核心属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-3-%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86"><span class="toc-text">2.1.3 执行原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-4-%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B"><span class="toc-text">2.1.4 基础编程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-4-1-RDD%E5%88%9B%E5%BB%BA"><span class="toc-text">2.1.4.1 RDD创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-4-2-RDD%E5%B9%B6%E8%A1%8C%E5%BA%A6%E4%B8%8E%E5%88%86%E5%8C%BA"><span class="toc-text">2.1.4.2 RDD并行度与分区</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-4-3-RDD%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90"><span class="toc-text">2.1.4.3 RDD转换算子</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-map"><span class="toc-text">1) map</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-mapPartitions"><span class="toc-text">2) mapPartitions</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-mapPartitionsWithIndex"><span class="toc-text">3) mapPartitionsWithIndex</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-flatMap"><span class="toc-text">4) flatMap</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-glom"><span class="toc-text">5) glom</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#6-groupBy"><span class="toc-text">6) groupBy</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#7-filter"><span class="toc-text">7) filter</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#8-sample"><span class="toc-text">8) sample</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-distinct"><span class="toc-text">9) distinct</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#10-coalesce"><span class="toc-text">10) coalesce</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#11-repartition"><span class="toc-text">11) repartition</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#12-sortBy"><span class="toc-text">12) sortBy</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#13-intersection"><span class="toc-text">13) intersection</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#14-union"><span class="toc-text">14) union</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#15-subtract"><span class="toc-text">15) subtract</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#16-zip"><span class="toc-text">16) zip</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#17-partitionBy"><span class="toc-text">17) partitionBy</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#18-reduceByKey"><span class="toc-text">18) reduceByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#19-groupByKey"><span class="toc-text">19) groupByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#20-aggregateByKey"><span class="toc-text">20) aggregateByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#21-foldByKey"><span class="toc-text">21) foldByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#22-combineByKey"><span class="toc-text">22) combineByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#23-sortByKey"><span class="toc-text">23) sortByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#24-join"><span class="toc-text">24) join</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#25-leftOuterJoin"><span class="toc-text">25) leftOuterJoin</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#26-cogroup"><span class="toc-text">26) cogroup</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-4-4-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-text">2.1.4.4 案例实操</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-4-5-RDD%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90"><span class="toc-text">2.1.4.5 RDD行动算子</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-reduce"><span class="toc-text">1) reduce</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-collect"><span class="toc-text">2) collect</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-count"><span class="toc-text">3) count</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-first"><span class="toc-text">4) first</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-take"><span class="toc-text">5) take</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#6-takeOrdered"><span class="toc-text">6) takeOrdered</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#7-aggregate"><span class="toc-text">7) aggregate</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#8-fold"><span class="toc-text">8) fold</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-countByKey"><span class="toc-text">9) countByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#10-save%E7%9B%B8%E5%85%B3%E7%AE%97%E5%AD%90"><span class="toc-text">10) save相关算子</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#11-foreach"><span class="toc-text">11) foreach</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-4-6-RDD%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-text">2.1.4.6 RDD序列化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-4-7-RDD%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="toc-text">2.1.4.7 RDD依赖关系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-4-8-RDD%E6%8C%81%E4%B9%85%E5%8C%96"><span class="toc-text">2.1.4.8 RDD持久化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-4-9-RDD%E5%88%86%E5%8C%BA%E5%99%A8"><span class="toc-text">2.1.4.9 RDD分区器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-4-10-RDD%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E4%B8%8E%E4%BF%9D%E5%AD%98"><span class="toc-text">2.1.4.10 RDD文件读取与保存</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="toc-text">2.2 累加器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86"><span class="toc-text">2.2.1 实现原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B"><span class="toc-text">2.2.2 基础编程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-1-%E7%B3%BB%E7%BB%9F%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="toc-text">2.2.2.1 系统累加器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-2-%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="toc-text">2.2.2.2 自定义累加器</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="toc-text">2.3 广播变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-1-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86"><span class="toc-text">2.3.1 实现原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-2-%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B"><span class="toc-text">2.3.2 基础编程</span></a></li></ol></li></ol></li></ol></div><style>.left-col .switch-area,.left-col .switch-btn{display:none}.toc-level-6 i,.toc-level-6 ol{display:none!important}</style><input type="button" id="tocButton" value="隐藏目录" title="点击按钮隐藏或者显示文章目录"><script>yiliaConfig.toc=["隐藏目录","显示目录",!0],$(".left-col").is(":hidden")&&$("#tocButton").attr("value",yiliaConfig.toc[1])</script><div class="share"><link rel="stylesheet" type="text/css" href="/share/iconfont.css"><link rel="stylesheet" href="/share/spongebob.min.css" type="text/css" media="all"><div class="social_share"><ul id="social_list" class="social_icon_list"></ul></div><script>var shareConfig={title:"Spark学习笔记（二） 架构解析和RDD编程",url:window.location.href,author:"m01ly",img:"https:/img/avatar.jpg"}</script><script src="/share/qrcode.min.js"></script><script src="/share/spongebob.min.js"></script></div><section id="comments" style="margin:2em;padding:2em;background:rgba(255,255,255,.5)"><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script><div id="gitalk-container"></div><script type="text/javascript">var gitalk=new Gitalk({clientID:"41a199ade404435645c4",clientSecret:"1de34fbb95212de986a29fea6d6f22bb57b2d473",repo:"m01ly.github.io",owner:"m01ly",admin:["m01ly"],id:window.location.pathname});gitalk.render("gitalk-container")</script></section><div class="scroll" id="post-nav-button"><a href="/2020/12/01/bigdata-zookeeper1-setup/" title="上一篇: Zookeeper学习笔记（一） 搭建教程"><i class="fa fa-angle-left"></i> </a><a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a> <a href="/2020/11/29/bigdata-Spark1-setup/" title="下一篇: Spark学习笔记（一） 搭建Spark"><i class="fa fa-angle-right"></i></a></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/01/04/pt-docker-escape/">docker逃逸常用方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/23/security-tools-kubebench/">kube-bench工具使用--</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/21/security-cve-recurrent-CVE-2019-5736/">Docker逃逸漏洞复现（CVE-2019-5736）</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/20/security-tools-kubehunter/">kube-hunter工具使用--</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/17/security-tools-kubesploit/">kubesploit工具使用--一个针对容器化环境的跨平台后渗透利用工具</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/07/security-tools/">security-tools</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/28/scan-nessus-compliance/">nessus扫描合规性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/22/cert-letsencrypt/">证书管理工具之letsencrypt</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/06/htps-recommend/">安全的TLS协议</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/07/leetcode-slidewindow/">滑动窗口相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/24/burpsuite-develop-detect-nginx/">开发burpsuite插件-识别nginx版本并列出已知CVE</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/21/burpsuite-develop/">从0开发burpsuite插件（Java）</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/26/machine-learning-classify-knn/">机器学习算法之KNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/07/openvas-develop/">openvas插件开发</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/30/pt-antSword/">渗透测试工具之蚁剑</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/11/linux-disk/">centos7把/mnt空间合并到/(根目录)</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/19/install-guide-elk-filebeats/">elk笔记三--利用elk+filebeat搭建SIEM系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/18/linux-jdk8/">linux安装jdk1.8</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/01/21/esc-DefectDojo/">DefectDojo安装与使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper3-API/">Zookeeper学习笔记（三） API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper2-framework/">Zookeeper学习笔记（二） 架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper1-setup/">Zookeeper学习笔记（一） 搭建教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/29/bigdata-Spark2-framework/">Spark学习笔记（二） 架构解析和RDD编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/29/bigdata-Spark1-setup/">Spark学习笔记（一） 搭建Spark</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase2-framework/">HBase学习笔记（二） HBase架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase4-phoenix/">HBase学习笔记（四） HBase整合phoenix和Hive</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase3-API/">HBase学习笔记（三） HBase的API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase1-setup/">HBase学习笔记（一） 安装教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/23/bigdata-datacollect1-userbehavior/">大数据实践（一）数仓采集项目</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/22/bigdata-sqoop/">sqoop安装教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/18/bigdata-kafka4-test/">kafka学习笔记（四） kafka面试集锦</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/17/bigdata-kafka3-API/">kafka学习笔记（三） kafka的API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/16/bigdata-flume3-monitor/">flume学习笔记（三） flum数据流监控及面试题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/16/bigdata-kafka2-framework/">kafka学习笔记（二） kafka框架深入</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-flume2-framework/">flume学习笔记（二） flum事务和部署架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-flume1-setup/">flume学习笔记（一） flume搭建</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive3/">Hive学习笔记（三） Hive的分区表和分桶表</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive5-example/">Hive学习笔记（五） Hive实战</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive4-optimize/">Hive学习笔记（四） Hive的企业级调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive2/">Hive学习笔记（二） Hive对数据基本操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-kafka1-setup/">kafka学习笔记（一） kafka搭建</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/14/bigdata-hive1/">Hive学习笔记（一） Hive安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-mapreduce2-framework/">Hadoop 教程（五）mapreduce架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs1/">Hadoop 教程（二）安装hadoop集群-完全分布式部署及API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs/">Hadoop 教程（一）hadoop介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-yarn-framework/">Hadoop 教程（六）yarn-架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-mapreduce1-setup/">Hadoop 教程（四）mapreduce介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs3-framework/">Hadoop 教程（三）hdfs-架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/10/cipher-certificate-format/">证书的各种格式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/docker-guide/">docker使用大全</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/linux-cmd/">linux命令大全</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-metosploitInAliyun/">在阿里云主机反弹metosploit</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-info-collection/">信息收集</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-tools/">最佳网络安全和黑客软件</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/18/mobilesecurity-experience/">小白如何在三天一步步逆向app，找到私钥</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/elk-login/">elk笔记二--通过X-Pack权限控制设置elk登录</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-centosInvm/">vm 安装centos 7教程详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/writeup-sqli-labs/">writeup-sqli-labs</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-elk-suricata/">elk笔记一---suricata+elk搭建入侵检测系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-suricata/">centos7中安装suricata</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/10/pt-sqlbypass/">sql关键词绕过</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/04/pt-portinfo/">常见端口说明和攻击汇总</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/03/m01ly-wiki/">m01ly-wiki</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/03/htps-attack-heartbleed/">TLS攻击之心脏滴血</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/01/htps-attack-paddingoracle/">TLS 攻击之POODLE</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/31/blockcipher-padding/">分组密码--填充模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/31/blockcipher-operation-mode/">分组密码--工作模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/hexo-guide/">Hexo踩坑</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/scan-awvs-nessus/">AWVS和Nessus镜像安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/scan-zap/">ZAP的安装和使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/pt-tiquan/">提权</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/htps-tools/">TLS安全检测小工具</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/htps-build/">搭建https网站</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/apple/">竟然有人能把https/TLS1.2协议讲的这么详细</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/26/leetcode-binary/">二分查找相关的题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/16/leetcode-stackandqueue/">栈和队列相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/07/leetcode-list/">链表相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/06/leetcode-daily/">leetcode每日一题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/29/leetcode-sort/">排序算法</a></li></ul><script></script></div><footer id="footer"><div class="outer"><div id="footer-info"><div class="footer-left"><i class="fa fa-copyright"></i> 2017-2022 冀-18010769-1</div><div class="visit"><span id="busuanzi_container_site_pv" style="display:none"><span id="site-visit" title="本站到访人数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span> </span></span><span>| </span><span id="busuanzi_container_page_pv" style="display:none"><span id="page-visit" title="本页访问次数"><i class="fa fa-eye" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span></span></span></div><div class="footer-right"><i class="fa fa-heart"></i><a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架"> Hexo</a> Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a></div></div></div></footer></div><script type="application/javascript">var leftWidth,hide=!1;$(".hide-left-col").click(function(){hide=hide?($(".left-col").css("width",leftWidth),$(".left-col .intrude-less").fadeIn(200),$("#tocButton").fadeIn(200),"block"===$("#switch-btn").css("display")&&"block"===$("#switch-area").css("display")||$("#toc").fadeIn(200),$(".hide-left-col").css("left",leftWidth).html('<i class="fa fa-angle-double-left"></i>'),$(".mid-col").css("left",leftWidth),$("#post-nav-button").css("left",leftWidth),$("#post-nav-button > a:nth-child(2)").css("display","block"),!1):(leftWidth=$(".left-col")[0].style.width,$(".left-col").css("width",0),$(".left-col .intrude-less").fadeOut(200),$("#toc").fadeOut(100),$("#tocButton").fadeOut(100),$(".hide-left-col").css("left",0).html('<i class="fa fa-angle-double-right"></i>'),$(".mid-col").css("left",0),$("#post-nav-button").css("left",0),$("#post-nav-button > a:nth-child(2)").css("display","none"),$(".post-list").is(":visible")&&($("#post-nav-button .fa-bars,#post-nav-button .fa-times").toggle(),$(".post-list").toggle()),!0)})</script><script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.3.5/require.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[["$$","$$"],["$","$"],["\\(","\\)"]],processEscapes:!0,skipTags:["script","noscript","style","textarea","pre","code"]}}),MathJax.Hub.Queue(function(){var a,e=MathJax.Hub.getAllJax();for(a=0;a<e.length;a+=1)e[a].SourceElement().parentNode.className+=" has-jax"})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script><div class="scroll" id="scroll"><a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a> <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a> <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a></div><script>var oOpenInNew={post:".copyright a[href]",friends:"#js-friends a",socail:".social a"};for(var x in oOpenInNew)$(oOpenInNew[x]).attr("target","_blank")</script><script>var titleTime,originTitle=document.title;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="(つェ⊂)"+originTitle,clearTimeout(titleTime)):(document.title="(*´∇｀*)~ "+originTitle,titleTime=setTimeout(function(){document.title=originTitle},2e3))})</script><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><link href="//cdn.bootcss.com/aos/2.2.0/aos.css" rel="stylesheet"><script type="text/javascript">AOS.init({easing:"ease-out-back",once:!0})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"live2d_models/live2d-widget-model-izumi"},"display":{"position":"right","width":100,"height":200,"hOffset":-50,"vOffset":-85},"mobile":{"show":false},"react":{"opacityDefault":0.9,"opacityOnHover":0.3},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false});</script></body></html>