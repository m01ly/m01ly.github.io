<!DOCTYPE html><html lang="zh-Hans"><head><!--[if IE]><style>body{display:none;}</style><script>alert('IE浏览器下无法展示效果，请更换浏览器！');var headNode=document.getElementsByTagName('head')[0];var refresh=document.createElement('meta');refresh.setAttribute('http-equiv','Refresh');refresh.setAttribute('Content','0; url=http://outdatedbrowser.com/');headNode.appendChild(refresh);</script><![endif]--><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="m01ly"><meta name="description" content="下面我们通过Hive对数据进行操作，主要包括对数据库，表的基本操作和基本函数的使用。 1   Hive数据类型1.1 基本数据类型   Hive数据类型 Java数据类型 长度 例子    TINYINT byte 1byte有符号整数 20   SMALINT short 2byte有符号整数 20   INT int 4byte有符号整数 20   BIGINT long 8byte有符号整数"><meta property="og:type" content="article"><meta property="og:title" content="Hive学习笔记（二） Hive对数据基本操作"><meta property="og:url" content="https://m01ly.github.io/2020/11/15/bigdata-hive2/index.html"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="下面我们通过Hive对数据进行操作，主要包括对数据库，表的基本操作和基本函数的使用。 1   Hive数据类型1.1 基本数据类型   Hive数据类型 Java数据类型 长度 例子    TINYINT byte 1byte有符号整数 20   SMALINT short 2byte有符号整数 20   INT int 4byte有符号整数 20   BIGINT long 8byte有符号整数"><meta property="og:locale"><meta property="og:image" content="https://m01ly.github.io/2020/11/15/bigdata-hive2/1636962952895.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/15/bigdata-hive2/1637036085261.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/15/bigdata-hive2/1637036128510.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/15/bigdata-hive2/1637036162813.png"><meta property="article:published_time" content="2020-11-15T07:45:51.000Z"><meta property="article:modified_time" content="2021-11-16T08:55:28.424Z"><meta property="article:author" content="m01ly"><meta property="article:tag" content="Hive"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://m01ly.github.io/2020/11/15/bigdata-hive2/1636962952895.png"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="shortcut icon" href="/favicon.ico"><link href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css" rel="stylesheet"><link rel="stylesheet" href="/css/style.css"><title>Hive学习笔记（二） Hive对数据基本操作 | Hexo</title><script src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"><script src="//cdn.bootcss.com/aos/2.2.0/aos.js"></script><script>var yiliaConfig={fancybox:!0,isHome:!1,isPost:!0,isArchive:!1,isTag:!1,isCategory:!1,fancybox_js:"//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js",search:!0}</script><script>yiliaConfig.jquery_ui=[!1]</script><script>yiliaConfig.rootUrl="/"</script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/rss+xml">
<link rel="stylesheet" href="/css/prism-a11y-dark.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="container"><div class="left-col"><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><form id="search-form"><input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false"> <i class="fa fa-times" onclick="resetSearch()"></i></form><div id="local-search-result"></div><p class="no-result">No results found <i class="fa fa-spinner fa-pulse"></i></p><div id="switch-btn" class="switch-btn"><div class="icon"><div class="icon-ctn"><div class="icon-wrap icon-house" data-idx="0"><div class="birdhouse"></div><div class="birdhouse_holes"></div></div><div class="icon-wrap icon-ribbon hide" data-idx="1"><div class="ribbon"></div></div><div class="icon-wrap icon-link hide" data-idx="2"><div class="loopback_l"></div><div class="loopback_r"></div></div><div class="icon-wrap icon-me hide" data-idx="3"><div class="user"></div><div class="shoulder"></div></div></div></div><div class="tips-box hide"><div class="tips-arrow"></div><ul class="tips-inner"><li>菜单</li><li>标签</li><li>友情链接</li><li>目标</li></ul></div></div><div id="switch-area" class="switch-area"><div class="switch-wrap"><section class="switch-part switch-part1"><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" rel="noopener" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" rel="noopener" href="https://music.163.com/" title="网易云音乐"></a></ul><ul class="social"><div class="donateIcon-position"><p style="display:block"><a class="donateIcon" href="javascript:void(0)" onmouseout='var qr=document.getElementById("donate");qr.style.display="none"' onmouseenter='var qr=document.getElementById("donate");qr.style.display="block"'>赏</a></p><div id="donate"><img id="multipay" src="/img/multipay.png" width="250px" alt="m01ly Multipay"><div class="triangle"></div></div></div></ul></nav></section><section class="switch-part switch-part2"><div class="widget tagcloud" id="js-tagcloud"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TLS/" rel="tag">TLS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zookeeper/" rel="tag">Zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/" rel="tag">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/" rel="tag">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BC%81%E4%B8%9A%E5%AE%89%E5%85%A8%E5%BB%BA%E8%AE%BE/" rel="tag">企业安全建设</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E5%85%A8%E6%89%AB%E6%8F%8F/" rel="tag">安全扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" rel="tag">安装教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8%E6%95%99%E7%A8%8B/" rel="tag">容器教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/" rel="tag">密码学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" rel="tag">插件开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E4%BB%93%E9%87%87%E9%9B%86%E9%A1%B9%E7%9B%AE/" rel="tag">数仓采集项目</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/" rel="tag">日志管理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/" rel="tag">流量分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" rel="tag">渗透测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%8F%E6%B4%9E%E6%89%AB%E6%8F%8F/" rel="tag">漏洞扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%BB%E5%8A%A8%E5%AE%89%E5%85%A8/" rel="tag">移动安全</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%B6%E5%9C%BAwriteup/" rel="tag">靶场writeup</a></li></ul></div></section><section class="switch-part switch-part3"><div id="js-friends"><a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/TechCatsLab">TechCatsLab</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://yangchenglong11.github.io">YangChengLong</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://jsharkc.github.io">LiuJiaChang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://blog.yusank.space">YusanKurban</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.lizebang.top">Lizebang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/sunanxiang">SunAnXiang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/DoubleWoodH">LinHao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.littlechao.top">ShiChao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/Txiaozhe">TangXiaoJi</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/LLLeon">JiaChenHui</a></div></section><section class="switch-part switch-part4"><div id="js-aboutme">不悲不喜，不卑不亢，努力成为一个更好的程序猿！</div></section></div></div></header></div></div><div class="hide-left-col" title="隐藏侧栏"><i class="fa fa-angle-double-left"></i></div><div class="mid-col"><nav id="mobile-nav"><div class="overlay"><div class="slider-trigger"></div><h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">m01ly</a></h1></div><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/" title="回到主页">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li><div class="clearfix"></div></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" href="https://music.163.com/" title="网易云音乐"></a></ul></nav></header></div><link class="menu-list" tags="标签" friends="友情链接" about="目标"></nav><div class="body-wrap"><article id="post-bigdata-hive2" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/15/bigdata-hive2/" class="article-date"><time class="published" datetime="2020-11-15T07:45:51.000Z" itemprop="datePublished">2020-11-15 发布</time> <time class="updated" datetime="2021-11-16T08:55:28.424Z" itemprop="dateUpdated">2021-11-16 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 class="article-title" itemprop="name">Hive学习笔记（二） Hive对数据基本操作</h1></header><div class="article-info article-info-post"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li></ul></div><span class="post-count">总字数7.3k</span> <span class="post-count">预计阅读31分钟</span></div><div class="clearfix"></div></div><div class="article-entry" itemprop="articleBody"><p>下面我们通过Hive对数据进行操作，主要包括对数据库，表的基本操作和基本函数的使用。</p><h1 id="1-Hive数据类型"><a href="#1-Hive数据类型" class="headerlink" title="1   Hive数据类型"></a>1 Hive数据类型</h1><h2 id="1-1-基本数据类型"><a href="#1-1-基本数据类型" class="headerlink" title="1.1 基本数据类型"></a>1.1 基本数据类型</h2><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE FALSE</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td></td><td>字节数组</td><td></td></tr></tbody></table><p>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p><h2 id="1-2-集合数据类型"><a href="#1-2-集合数据类型" class="headerlink" title="1.2 集合数据类型"></a>1.2 集合数据类型</h2><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct() 例如struct&lt;street:string, city:string&gt;</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map() 例如map&lt;string, int&gt;</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array() 例如array<string></string></td></tr></tbody></table><p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p><p>1）案例实操</p><p>（1）假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</p><pre class="line-numbers language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>

  <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"songsong"</span><span class="token punctuation">,</span>

  <span class="token property">"friends"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"bingbing"</span> <span class="token punctuation">,</span> <span class="token string">"lili"</span><span class="token punctuation">]</span> <span class="token punctuation">,</span>    //列表Array<span class="token punctuation">,</span> 

  <span class="token property">"children"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>           //键值Map<span class="token punctuation">,</span>

​    <span class="token property">"xiao song"</span><span class="token operator">:</span> <span class="token number">19</span> <span class="token punctuation">,</span>

​    <span class="token property">"xiaoxiao song"</span><span class="token operator">:</span> <span class="token number">18</span>

  &amp;#<span class="token number">125</span><span class="token punctuation">;</span>

  <span class="token property">"address"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>           //结构Struct<span class="token punctuation">,</span>

​    <span class="token property">"street"</span><span class="token operator">:</span> <span class="token string">"hui long guan"</span> <span class="token punctuation">,</span>

​    <span class="token property">"city"</span><span class="token operator">:</span> <span class="token string">"beijing"</span> 

  &amp;#<span class="token number">125</span><span class="token punctuation">;</span>

&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据。</p><p>创建本地测试文件test.txt</p><p>songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</p><p>yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</p><p>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p><p>（3）Hive上创建测试表test</p><pre class="line-numbers language-bash"><code class="language-bash">create table test<span class="token punctuation">(</span>
name string,
friends array<span class="token operator">&lt;</span>string<span class="token operator">></span>,
children map<span class="token operator">&lt;</span>string, int<span class="token operator">></span>,
address struct<span class="token operator">&lt;</span>street:string, city:string<span class="token operator">></span>
<span class="token punctuation">)</span>
row <span class="token function">format</span> delimited fields terminated by <span class="token string">','</span>
collection items terminated by <span class="token string">'_'</span>
map keys terminated by <span class="token string">':'</span>
lines terminated by <span class="token string">'\n'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>字段解释：</p><p>row format delimited fields terminated by ‘,’ – 列分隔符</p><p>collection items terminated by ‘_’ –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p><p>map keys terminated by ‘:’ – MAP中的key与value的分隔符</p><p>lines terminated by ‘\n’; – 行分隔符</p><p>（4）导入文本数据到测试表</p><p>load data local inpath ‘/opt/module/hive/datas/test.txt’ into table test;</p><p>（5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> friends<span class="token punctuation">[</span>1<span class="token punctuation">]</span>,children<span class="token punctuation">[</span><span class="token string">'xiao song'</span><span class="token punctuation">]</span>,address.city from <span class="token function">test</span>
where name<span class="token operator">=</span><span class="token string">"songsong"</span><span class="token punctuation">;</span>
OK
_c0   _c1   city
lili  18   beijing
Time taken: 0.076 seconds, Fetched: 1 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="1-3-类型转化"><a href="#1-3-类型转化" class="headerlink" title="1.3 类型转化"></a>1.3 类型转化</h2><p>Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p><p>1）隐式类型转换规则如下</p><p>（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。</p><p>（2）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</p><p>（3）TINYINT、SMALLINT、INT都可以转换为FLOAT。</p><p>（4）BOOLEAN类型不可以转换为任何其它的类型。</p><p>2）可以使用CAST操作显示进行数据类型转换</p><p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p><pre class="line-numbers language-bash"><code class="language-bash">0: jdbc:hive2://hadoop102:10000<span class="token operator">></span> <span class="token keyword">select</span> <span class="token string">'1'</span>+2, cast<span class="token punctuation">(</span><span class="token string">'1'</span>as int<span class="token punctuation">)</span> + 2<span class="token punctuation">;</span>
+------+------+--+
<span class="token operator">|</span> _c0 <span class="token operator">|</span> _c1 <span class="token operator">|</span>
+------+------+--+
<span class="token operator">|</span> 3.0 <span class="token operator">|</span> 3  <span class="token operator">|</span>
+------+------+--+<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2 DDL数据定义</p><h1 id="2-DDL数据定义"><a href="#2-DDL数据定义" class="headerlink" title="2 DDL数据定义"></a>2 DDL数据定义</h1><p>数据库模式定义语言DDL(Data Definition Language)，是用于描述数据库中要存储的现实世界实体的语言。具体其实就是对数据库和表的CRUD操作。</p><h2 id="2-1-创建数据库"><a href="#2-1-创建数据库" class="headerlink" title="2.1 创建数据库"></a>2.1 创建数据库</h2><p>创建数据库语法如下所示：</p><pre class="line-numbers language-bash"><code class="language-bash">CREATE DATABASE <span class="token punctuation">[</span>IF NOT EXISTS<span class="token punctuation">]</span> database_name  --数据库名称
<span class="token punctuation">[</span>COMMENT database_comment<span class="token punctuation">]</span>  --数据库备注
<span class="token punctuation">[</span>LOCATION hdfs_path<span class="token punctuation">]</span>  ---数据库存储位置
<span class="token punctuation">[</span>WITH DBPROPERTIES <span class="token punctuation">(</span>property_name<span class="token operator">=</span>property_value, <span class="token punctuation">..</span>.<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">;</span> --DB的一些属性<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>1）创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create datavase mydb<span class="token punctuation">;</span>
create table <span class="token keyword">if</span> not exists test1<span class="token punctuation">(</span>
 <span class="token function">id</span> int comment <span class="token string">"this is id"</span>,
 name string comment <span class="token string">"this is name"</span>
<span class="token punctuation">)</span>
comment <span class="token string">"this is table"</span>
row <span class="token function">format</span> delimited fields terminated by <span class="token string">','</span>
STORED as textfile
TBLPROPERTIES<span class="token punctuation">(</span><span class="token string">"createtime="</span> 2020-4-11"<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以在hdfs上看到创建的数据库目录。</p><p><img src="/2020/11/15/bigdata-hive2/1636962952895.png" alt="1636962952898"></p><p>2）避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法）</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create database db_hive<span class="token punctuation">;</span>
FAILED: Execution Error, <span class="token keyword">return</span> code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create database <span class="token keyword">if</span> not exists db_hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>3）创建一个数据库，指定数据库在HDFS上存放的位置</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create database db_hive2 location <span class="token string">'/db_hive2.db'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-2-查询数据库"><a href="#2-2-查询数据库" class="headerlink" title="2.2 查询数据库"></a>2.2 查询数据库</h2><p>1）显示数据库</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> show databases<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）显示数据库信息</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> desc database db_hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）显示数据库详细信息，extended</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> desc database extended db_hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）切换当前数据库</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> use db_hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-3-修改数据库"><a href="#2-3-修改数据库" class="headerlink" title="2.3 修改数据库"></a>2.3 修改数据库</h2><p>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> alter database db_hive <span class="token keyword">set</span> dbproperties<span class="token punctuation">(</span><span class="token string">'createtime'</span><span class="token operator">=</span><span class="token string">'20170830'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在hive中查看修改结果</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> desc database extended db_hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-4-删除数据库"><a href="#2-4-删除数据库" class="headerlink" title="2.4 删除数据库"></a>2.4 删除数据库</h2><p>1）删除空数据库</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span>drop database db_hive2<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）如果删除的数据库不存在，最好采用 if exists判断数据库是否存在</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> drop database db_hive<span class="token punctuation">;</span>
FAILED: SemanticException <span class="token punctuation">[</span>Error 10072<span class="token punctuation">]</span>: Database does not exist: db_hive
hive<span class="token operator">></span> drop database <span class="token keyword">if</span> exists db_hive2<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>3）如果数据库不为空，可以采用cascade命令，强制删除</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> drop database db_hive<span class="token punctuation">;</span>
FAILED: Execution Error, <span class="token keyword">return</span> code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException<span class="token punctuation">(</span>message:Database db_hive is not empty. One or <span class="token function">more</span> tables exist.<span class="token punctuation">)</span>
hive<span class="token operator">></span> drop database db_hive cascade<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="2-5-创建表"><a href="#2-5-创建表" class="headerlink" title="2.5 创建表"></a>2.5 创建表</h2><p><strong>1）建表语法</strong></p><pre class="line-numbers language-bash"><code class="language-bash">CREATE <span class="token punctuation">[</span>EXTERNAL<span class="token punctuation">]</span> TABLE <span class="token punctuation">[</span>IF NOT EXISTS<span class="token punctuation">]</span> table_name -
<span class="token punctuation">[</span><span class="token punctuation">(</span>col_name data_type <span class="token punctuation">[</span>COMMENT col_comment<span class="token punctuation">]</span>, <span class="token punctuation">..</span>.<span class="token punctuation">)</span><span class="token punctuation">]</span> 
<span class="token punctuation">[</span>COMMENT table_comment<span class="token punctuation">]</span> 
<span class="token punctuation">[</span>PARTITIONED BY <span class="token punctuation">(</span>col_name data_type <span class="token punctuation">[</span>COMMENT col_comment<span class="token punctuation">]</span>, <span class="token punctuation">..</span>.<span class="token punctuation">)</span><span class="token punctuation">]</span> 
<span class="token punctuation">[</span>CLUSTERED BY <span class="token punctuation">(</span>col_name, col_name, <span class="token punctuation">..</span>.<span class="token punctuation">)</span> 
<span class="token punctuation">[</span>SORTED BY <span class="token punctuation">(</span>col_name <span class="token punctuation">[</span>ASC<span class="token operator">|</span>DESC<span class="token punctuation">]</span>, <span class="token punctuation">..</span>.<span class="token punctuation">)</span><span class="token punctuation">]</span> INTO num_buckets BUCKETS<span class="token punctuation">]</span> 
<span class="token punctuation">[</span>ROW FORMAT row_format<span class="token punctuation">]</span> 
<span class="token punctuation">[</span>STORED AS file_format<span class="token punctuation">]</span> 
<span class="token punctuation">[</span>LOCATION hdfs_path<span class="token punctuation">]</span>
<span class="token punctuation">[</span>TBLPROPERTIES <span class="token punctuation">(</span>property_name<span class="token operator">=</span>property_value, <span class="token punctuation">..</span>.<span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span>AS select_statement<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>2）字段解释说明</strong></p><p>（1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p><p>（2）<strong>EXTERNAL 关键字可以让用户创建一个外部表</strong>，在建表的同时可以指定一个指向实际数据的路径（LOCATION），<strong>在删除表的时候，内部表的元数据和hdfs数据会被一起删除，而外部表只删除元数据，不删除存在hdfs上的数据。</strong></p><p>（3）COMMENT：为表和列添加注释。</p><p>（4）PARTITIONED BY创建分区表</p><p>（5）CLUSTERED BY创建分桶表</p><p>（6）SORTED BY不常用，对桶中的一个或多个列另外排序</p><p>（7）ROW FORMAT</p><p>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]</p><p>​ [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</p><p>| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]</p><p>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。</p><p>SerDe是Serialize/Deserilize的简称， hive使用Serde进行行对象的序列与反序列化。</p><p>（8）STORED AS指定存储文件类型</p><p>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</p><p>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</p><p>（9）LOCATION ：指定表在HDFS上的存储位置。</p><p>（10）AS：后跟查询语句，根据查询结果创建表。</p><p>（11）LIKE允许用户复制现有的表结构，但是不复制数据。</p><h3 id="2-3-1-内部表-管理表"><a href="#2-3-1-内部表-管理表" class="headerlink" title="2.3.1 内部表(管理表)"></a>2.3.1 内部表(管理表)</h3><p>1）理论</p><p>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。 当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</p><p>（1）普通创建表</p><pre class="line-numbers language-bash"><code class="language-bash">create table <span class="token keyword">if</span> not exists student<span class="token punctuation">(</span>
<span class="token function">id</span> int, name string
<span class="token punctuation">)</span>
row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span>
stored as textfile
location <span class="token string">'/user/hive/warehouse/student'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）根据查询结果创建表（查询的结果会添加到新创建的表中）</p><pre class="line-numbers language-bash"><code class="language-bash">create table <span class="token keyword">if</span> not exists student2 as <span class="token keyword">select</span> id, name from student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）根据已经存在的表结构创建表</p><pre class="line-numbers language-bash"><code class="language-bash">create table <span class="token keyword">if</span> not exists student3 like student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）查询表的类型</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> desc formatted student2<span class="token punctuation">;</span>
Table Type:       MANAGED_TABLE <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="2-3-2-外部表"><a href="#2-3-2-外部表" class="headerlink" title="2.3.2 外部表"></a>2.3.2 外部表</h3><p>1）理论</p><p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</p><p>2）管理表和外部表的使用场景</p><p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p><p>3）案例实操</p><p>分别创建部门和员工外部表，并向表中导入数据。</p><p>（1）上传数据到HDFS</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -mkdir /student<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -put /opt/module/datas/student.txt /student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（2）建表语句，创建外部表</p><p>创建部门表</p><pre class="line-numbers language-bash"><code class="language-bash">create external table <span class="token keyword">if</span> not exists dept<span class="token punctuation">(</span>
deptno int,
dname string,
loc int<span class="token punctuation">)</span>
row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）查看创建的表</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>show tables<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）查看表格式化数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> desc formatted dept<span class="token punctuation">;</span>
Table Type:       EXTERNAL_TABLE<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（5）删除外部表</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> drop table dept<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>外部表删除后，hdfs中的数据还在，但是metadata中dept的元数据已被删除</p><h3 id="2-5-3-管理表与外部表的互相转换"><a href="#2-5-3-管理表与外部表的互相转换" class="headerlink" title="2.5.3 管理表与外部表的互相转换"></a>2.5.3 管理表与外部表的互相转换</h3><p>（1）查询表的类型</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> desc formatted student2<span class="token punctuation">;</span>
Table Type:       MANAGED_TABLE<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（2）修改内部表student2为外部表</p><pre class="line-numbers language-bash"><code class="language-bash">alter table student2 <span class="token keyword">set</span> tblproperties<span class="token punctuation">(</span><span class="token string">'EXTERNAL'</span><span class="token operator">=</span><span class="token string">'TRUE'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）修改外部表student2为内部表</p><pre class="line-numbers language-bash"><code class="language-bash">alter table student2 <span class="token keyword">set</span> tblproperties<span class="token punctuation">(</span><span class="token string">'EXTERNAL'</span><span class="token operator">=</span><span class="token string">'FALSE'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</p><h2 id="2-6-修改表"><a href="#2-6-修改表" class="headerlink" title="2.6 修改表"></a>2.6 修改表</h2><h3 id="2-6-1-重命名表"><a href="#2-6-1-重命名表" class="headerlink" title="2.6.1 重命名表"></a>2.6.1 重命名表</h3><p>1）语法</p><pre class="line-numbers language-bash"><code class="language-bash">ALTER TABLE table_name RENAME TO new_table_name<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）实操案例</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> alter table dept_partition2 <span class="token function">rename</span> to dept_partition3<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-6-2-增加-修改-替换列信息"><a href="#2-6-2-增加-修改-替换列信息" class="headerlink" title="2.6.2 增加/修改/替换列信息"></a>2.6.2 增加/修改/替换列信息</h3><p>1）语法</p><p>（1）更新列</p><pre class="line-numbers language-bash"><code class="language-bash">ALTER TABLE table_name CHANGE <span class="token punctuation">[</span>COLUMN<span class="token punctuation">]</span> col_old_name col_new_name column_type <span class="token punctuation">[</span>COMMENT col_comment<span class="token punctuation">]</span> <span class="token punctuation">[</span>FIRST<span class="token operator">|</span>AFTER column_name<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）增加和替换列</p><pre class="line-numbers language-bash"><code class="language-bash">ALTER TABLE table_name ADD<span class="token operator">|</span>REPLACE COLUMNS <span class="token punctuation">(</span>col_name data_type <span class="token punctuation">[</span>COMMENT col_comment<span class="token punctuation">]</span>, <span class="token punctuation">..</span>.<span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。</p><p>2）实操案例</p><p>（1）查询表结构</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token operator">></span> desc dept<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）添加列</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> alter table dept add columns<span class="token punctuation">(</span>deptdesc string<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）更新列</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> alter table dept change column deptdesc desc string<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（5）替换列</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> alter table dept replace columns<span class="token punctuation">(</span>deptno string, dname
 string, loc string<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="2-7-删除表"><a href="#2-7-删除表" class="headerlink" title="2.7 删除表"></a>2.7 删除表</h2><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> drop table dept<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="3-DML数据操作"><a href="#3-DML数据操作" class="headerlink" title="3  DML数据操作"></a>3 DML数据操作</h1><h2 id="3-1-数据导入"><a href="#3-1-数据导入" class="headerlink" title="3.1 数据导入"></a>3.1 数据导入</h2><p>让hdfs的数据和hive表产生关联。上传表数据：hive创建的表其实就是hdfs中的一个目录，所以表数据就映射为hdfs目录下的数据txt。</p><h3 id="3-1-1-向表中装载数据（Load）"><a href="#3-1-1-向表中装载数据（Load）" class="headerlink" title="3.1.1 向表中装载数据（Load）"></a>3.1.1 向表中装载数据（Load）</h3><p><strong>1）语法</strong></p><p>hive&gt; load data [local] inpath ‘数据的path’ [overwrite] into table student [partition (partcol1=val1,…)];</p><p>（1）load data:表示加载数据</p><p>（2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p><p>（3）inpath:表示加载数据的路径</p><p>（4）overwrite:表示覆盖表中已有数据，否则表示追加</p><p>（5）into table:表示加载到哪张表</p><p>（6）student:表示具体的表</p><p>（7）partition:表示上传到指定分区</p><p><strong>2）实操案例</strong></p><p>（0）创建一张表</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create table student<span class="token punctuation">(</span>id string, name string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（1）加载本地文件到hive</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> load data local inpath <span class="token string">'/opt/module/hive/datas/student.txt'</span> into table default.student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）加载HDFS文件到hive中</p><p>上传文件到HDFS</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -put /opt/module/hive/datas/student.txt /user/molly/hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>加载HDFS上数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> load data inpath <span class="token string">'/user/molly/hive/student.txt'</span> into table default.student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）加载数据覆盖表中已有的数据</p><p>上传文件到HDFS</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -put /opt/module/datas/student.txt /user/molly/hive<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>加载数据覆盖表中已有的数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> load data inpath <span class="token string">'/user/molly/hive/student.txt'</span> overwrite into table default.student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-1-2-通过查询语句向表中插入数据（Insert）"><a href="#3-1-2-通过查询语句向表中插入数据（Insert）" class="headerlink" title="3.1.2 通过查询语句向表中插入数据（Insert）"></a>3.1.2 通过查询语句向表中插入数据（Insert）</h3><p>1）创建一张表</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create table student_par<span class="token punctuation">(</span>id int, name string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）基本插入数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert into table student_par values<span class="token punctuation">(</span>1,<span class="token string">'wangwu'</span><span class="token punctuation">)</span>,<span class="token punctuation">(</span>2,<span class="token string">'zhaoliu'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）基本模式插入（根据单张表查询结果）</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite table student_par <span class="token keyword">select</span> id, name from student <span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>insert into：以追加数据的方式插入到表或分区，原有数据不会删除</p><p>insert overwrite：会覆盖表中已存在的数据</p><p>注意：insert不支持插入部分字段</p><p>4）多表（多分区）插入模式（根据多张表查询结果）</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> from student
​       insert overwrite table student partition<span class="token punctuation">(</span>month<span class="token operator">=</span><span class="token string">'201707'</span><span class="token punctuation">)</span>
​       <span class="token keyword">select</span> id, name where month<span class="token operator">=</span><span class="token string">'201709'</span>
​       insert overwrite table student partition<span class="token punctuation">(</span>month<span class="token operator">=</span><span class="token string">'201706'</span><span class="token punctuation">)</span>
​       <span class="token keyword">select</span> id, name where month<span class="token operator">=</span><span class="token string">'201709'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-1-3-查询语句中创建表并加载数据（As-Select）"><a href="#3-1-3-查询语句中创建表并加载数据（As-Select）" class="headerlink" title="3.1.3 查询语句中创建表并加载数据（As Select）"></a>3.1.3 查询语句中创建表并加载数据（As Select）</h3><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>create table <span class="token keyword">if</span> not exists student3 as <span class="token keyword">select</span> id, name from student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-1-4-创建表时通过Location指定加载数据路径—用的多"><a href="#3-1-4-创建表时通过Location指定加载数据路径—用的多" class="headerlink" title="3.1.4 创建表时通过Location指定加载数据路径—用的多"></a>3.1.4 创建表时通过Location指定加载数据路径—用的多</h3><p>1）上传数据到hdfs上</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -mkdir /student<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -put /opt/module/datas/student.txt /student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）创建表，并指定在hdfs上的位置</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create external table <span class="token keyword">if</span> not exists student5<span class="token punctuation">(</span>
​       <span class="token function">id</span> int, name string<span class="token punctuation">)</span>
​       row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span>
​       location '/student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>先建表，后指定hdfss数据文件到该表。</p><pre class="line-numbers language-bash"><code class="language-bash">hadoop fs -put test.txt /test2.table<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）查询数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from student5<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-1-5-Import数据到指定Hive表中"><a href="#3-1-5-Import数据到指定Hive表中" class="headerlink" title="3.1.5 Import数据到指定Hive表中"></a>3.1.5 Import数据到指定Hive表中</h3><p>注意：先用export导出后，再将数据导入。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token function">import</span> table student2  from <span class="token string">'/user/hive/warehouse/export/student'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="3-2-数据导出"><a href="#3-2-数据导出" class="headerlink" title="3.2 数据导出"></a>3.2 数据导出</h2><h3 id="3-2-1-Insert导出"><a href="#3-2-1-Insert导出" class="headerlink" title="3.2.1 Insert导出"></a>3.2.1 Insert导出</h3><p>1）将查询的结果导出到本地(有local)</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite local directory <span class="token string">'/opt/module/hive/datas/export/student'</span>
​      <span class="token keyword">select</span> * from student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）将查询的结果<strong>格式化</strong>导出到本地</p><pre class="line-numbers language-bash"><code class="language-bash">hive<span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>insert overwrite local directory <span class="token string">'/opt/module/hive/datas/export/student1'</span>
​      ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="token string">'\t'</span>       <span class="token keyword">select</span> * from student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>3）将查询的结果导出到HDFS上(没有local)</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite directory <span class="token string">'/user/molly/student2'</span>
​       ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="token string">'\t'</span> 
​       <span class="token keyword">select</span> * from student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="3-2-2-Hadoop命令导出到本地"><a href="#3-2-2-Hadoop命令导出到本地" class="headerlink" title="3.2.2 Hadoop命令导出到本地"></a>3.2.2 Hadoop命令导出到本地</h3><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> dfs -get /user/hive/warehouse/student/student.txt
/opt/module/datas/export/student3.txt<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="3-2-3-Hive-Shell-命令导出"><a href="#3-2-3-Hive-Shell-命令导出" class="headerlink" title="3.2.3 Hive Shell 命令导出"></a>3.2.3 Hive Shell 命令导出</h3><p>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hive<span class="token punctuation">]</span>$ bin/hive -e <span class="token string">'select * from default.student;'</span> <span class="token operator">></span> /opt/module/hive/datas/export/student4.txt<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-2-4-Export导出到HDFS上"><a href="#3-2-4-Export导出到HDFS上" class="headerlink" title="3.2.4 Export导出到HDFS上"></a>3.2.4 Export导出到HDFS上</h3><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>export table default.student to <span class="token string">'/user/hive/warehouse/export/student'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p><h3 id="3-2-5-清除表中数据（Truncate）"><a href="#3-2-5-清除表中数据（Truncate）" class="headerlink" title="3.2.5 清除表中数据（Truncate）"></a>3.2.5 清除表中数据（Truncate）</h3><p>注意：Truncate只能删除管理表，不能删除外部表中数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> truncate table student<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="4-数据查询—用的很多"><a href="#4-数据查询—用的很多" class="headerlink" title="4  数据查询—用的很多"></a>4 数据查询—用的很多</h1><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">查询语句语法</a>：</p><pre class="line-numbers language-bash"><code class="language-bash">SELECT <span class="token punctuation">[</span>ALL <span class="token operator">|</span> DISTINCT<span class="token punctuation">]</span> select_expr, select_expr, <span class="token punctuation">..</span>.
 FROM table_reference
 <span class="token punctuation">[</span>WHERE where_condition<span class="token punctuation">]</span> -筛选条件
 <span class="token punctuation">[</span>GROUP BY col_list<span class="token punctuation">]</span>---分组
 <span class="token punctuation">[</span>HAVING having_condition<span class="token punctuation">]</span>---分组后的过滤条件
 <span class="token punctuation">[</span>ORDER BY col_list<span class="token punctuation">]</span>—--全局排序
 <span class="token punctuation">[</span>CLUSTER BY col_list—---分区排序
<span class="token operator">|</span> 
<span class="token punctuation">[</span>DISTRIBUTE BY col_list<span class="token punctuation">]</span> –---分区排序  类似MR中的分区
<span class="token punctuation">[</span>SORT BY col_list<span class="token punctuation">]</span> –区内排序
 <span class="token punctuation">]</span>
 <span class="token punctuation">[</span>LIMIT number<span class="token punctuation">]</span>--限制返回条数<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>HQL查询语法和sql大致相同，这里就不一一列举，这里需要提出说的是排序的用法。</p><p>需要知道的是order by是全局排序，针对所有数据进行排序；SORT BY是分区内的排序</p><h3 id="4-1-全局排序（Order-By）"><a href="#4-1-全局排序（Order-By）" class="headerlink" title="4.1 全局排序（Order By）"></a>4.1 全局排序（Order By）</h3><p>Order By：<strong>全局排序</strong>，只有一个Reducer</p><p>例如：查询员工信息按工资升序排列</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp order by sal<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-2-每个Reduce内部排序（Sort-By）"><a href="#4-2-每个Reduce内部排序（Sort-By）" class="headerlink" title="4.2 每个Reduce内部排序（Sort By）"></a>4.2 每个Reduce内部排序（Sort By）</h3><p>Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。</p><p><strong>Sort by为每个reducer产生一个排序文件（即为分区内的排序）。每个Reducer内部进行排序，对全局结果集来说不是排序</strong>。</p><p>1）设置reduce个数</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce.job.reduces<span class="token operator">=</span>3<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）查看设置reduce个数</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce.job.reduces<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）根据部门编号降序查看员工信息</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp <span class="token function">sort</span> by deptno desc<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）将查询结果导入到文件中（按照部门编号降序排序）</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite local directory <span class="token string">'/opt/module/hive/datas/sortby-result'</span>
 <span class="token keyword">select</span> * from emp <span class="token function">sort</span> by deptno desc<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="4-3-分区（Distribute-By）"><a href="#4-3-分区（Distribute-By）" class="headerlink" title="4.3 分区（Distribute By）"></a>4.3 分区（Distribute By）</h3><p>Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。<strong>distribute by</strong> 子句可以做这件事。<strong>distribute by</strong>类似MR中partition（自定义分区），进行分区，结合sort by使用。</p><p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>1）案例实操：</p><p>（1）先按照部门编号分区，再按照员工编号降序排序。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce.job.reduces<span class="token operator">=</span>3<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite local directory <span class="token string">'/opt/module/hive/datas/distribute-result'</span> <span class="token keyword">select</span> * from emp distribute by deptno <span class="token function">sort</span> by empno desc<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意：</p><p>Ø distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。</p><p>Ø Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</p><h3 id="4-4-Cluster-By"><a href="#4-4-Cluster-By" class="headerlink" title="4.4 Cluster By"></a>4.4 Cluster By</h3><p>当distribute by和sort by字段相同时，可以使用cluster by方式。</p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p><p>（1）以下两种写法等价</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp cluster by deptno<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp distribute by deptno <span class="token function">sort</span> by deptno<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p><h1 id="5-函数使用"><a href="#5-函数使用" class="headerlink" title="5 函数使用"></a>5 函数使用</h1><h2 id="5-1-系统内置函数"><a href="#5-1-系统内置函数" class="headerlink" title="5.1 系统内置函数"></a>5.1 系统内置函数</h2><p>1）查看系统自带的函数</p><pre><code>hive&gt; show functions;</code></pre><p>2）显示自带的函数的用法</p><pre><code>hive&gt; desc function upper;</code></pre><p>3）详细显示自带的函数的用法</p><pre><code>hive&gt; desc function extended upper;</code></pre><h2 id="5-2-常用内置函数"><a href="#5-2-常用内置函数" class="headerlink" title="5.2 常用内置函数"></a>5.2 常用内置函数</h2><h3 id="5-2-1-空字段赋值"><a href="#5-2-1-空字段赋值" class="headerlink" title="5.2.1 空字段赋值"></a>5.2.1 空字段赋值</h3><p>1）函数说明</p><p>NVL：给值为NULL的数据赋值，它的格式是NVL( value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值，如果两个参数都为NULL ，则返回NULL。</p><p>2）数据准备：采用员工表</p><p>3）查询：如果员工的comm为NULL，则用-1代替</p><pre><code>hive (default)&gt; select comm,nvl(comm, -1) from emp;</code></pre><h3 id="5-2-2-CASE-WHEN-THEN-ELSE-END"><a href="#5-2-2-CASE-WHEN-THEN-ELSE-END" class="headerlink" title="5.2.2 CASE WHEN THEN ELSE END"></a>5.2.2 CASE WHEN THEN ELSE END</h3><p>1）数据准备</p><table><thead><tr><th>name</th><th>dept_id</th><th>sex</th></tr></thead><tbody><tr><td>悟空</td><td>A</td><td>男</td></tr><tr><td>大海</td><td>A</td><td>男</td></tr><tr><td>宋宋</td><td>B</td><td>男</td></tr><tr><td>凤姐</td><td>A</td><td>女</td></tr><tr><td>婷姐</td><td>B</td><td>女</td></tr><tr><td>婷婷</td><td>B</td><td>女</td></tr></tbody></table><p>2）需求</p><p>求出不同部门男女各多少人。结果如下：</p><p>dept_Id 男 女</p><p>A 2 1</p><p>B 1 2</p><p>3）按需求查询数据</p><p>select</p><pre><code> dept_id,
 sum(case sex when &#39;男&#39; then 1 else 0 end) male_count,
 sum(case sex when &#39;女&#39; then 1 else 0 end) female_count
from 
 emp_sex
group by
 dept_id;</code></pre><h3 id="5-2-3-行转列"><a href="#5-2-3-行转列" class="headerlink" title="5.2.3 行转列"></a>5.2.3 行转列</h3><p>1）相关函数说明</p><p><strong>CONCAT</strong>(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p><p><strong>CONCAT_WS</strong>(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p><p>注意: CONCAT_WS must be “string or array<string></string></p><p><strong>COLLECT_SET</strong>(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p><p><strong>COLLECT_list</strong>(col): 相比COLLECT_SET就是不去重的操作。</p><p>2）数据准备</p><table><thead><tr><th>name</th><th>constellation</th><th>blood_type</th></tr></thead><tbody><tr><td>孙悟空</td><td>白羊座</td><td>A</td></tr><tr><td>大海</td><td>射手座</td><td>A</td></tr><tr><td>宋宋</td><td>白羊座</td><td>B</td></tr><tr><td>猪八戒</td><td>白羊座</td><td>A</td></tr><tr><td>凤姐</td><td>射手座</td><td>A</td></tr><tr><td>苍老师</td><td>白羊座</td><td>B</td></tr></tbody></table><p>3）需求</p><p>把星座和血型一样的人归类到一起。结果如下：</p><pre><code>射手座,A      大海|凤姐
白羊座,A      孙悟空|猪八戒
白羊座,B       宋宋|苍老师</code></pre><p>4）创建本地constellation.txt，导入数据</p><pre><code>[molly@hadoop102 datas]$ vim person_info.txt
孙悟空  白羊座  A
大海 射手座  A
宋宋 白羊座  B
猪八戒  白羊座  A
凤姐 射手座  A
苍老师  白羊座  B</code></pre><p>5）创建hive表并导入数据</p><pre><code>create table person_info(
name string, 
constellation string, 
blood_type string) 
row format delimited fields terminated by &quot;\t&quot;;
load data local inpath &quot;/opt/module/hive/datas/person_info.txt&quot; into table person_info;</code></pre><p>6）按需求查询数据</p><pre><code>SELECT t1.c_b , CONCAT_WS(&quot;|&quot;,collect_set(t1.name))
FROM (
SELECT NAME ,CONCAT_WS(&#39;,&#39;,constellation,blood_type) c_b
FROM person_info
)t1 
GROUP BY t1.c_b</code></pre><h3 id="5-2-4-列转行"><a href="#5-2-4-列转行" class="headerlink" title="5.2.4 列转行"></a>5.2.4 列转行</h3><p>1）函数说明</p><p>**EXPLODE(col)**：将hive一列中复杂的array或者map结构拆分成多行。</p><p><strong>LATERAL VIEW</strong>（侧写表）</p><p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p><p>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p><p>2）数据准备</p><p>表6-7 数据准备</p><table><thead><tr><th>movie</th><th>category</th></tr></thead><tbody><tr><td>《疑犯追踪》</td><td>悬疑,动作,科幻,剧情</td></tr><tr><td>《Lie to me》</td><td>悬疑,警匪,动作,心理,剧情</td></tr><tr><td>《战狼2》</td><td>战争,动作,灾难</td></tr></tbody></table><p>3）需求</p><p>将电影分类中的数组数据展开。结果如下：</p><pre><code>《疑犯追踪》   悬疑
《疑犯追踪》   动作
《疑犯追踪》   科幻
《疑犯追踪》   剧情
《Lie to me》  悬疑
《Lie to me》  警匪
《Lie to me》  动作
《Lie to me》  心理
《Lie to me》  剧情
《战狼2》     战争
《战狼2》     动作
《战狼2》     灾难</code></pre><p>4）创建本地movie.txt，导入数据</p><pre><code>[molly@hadoop102 datas]$ vi movie_info.txt
《疑犯追踪》 悬疑,动作,科幻,剧情
《Lie to me》 悬疑,警匪,动作,心理,剧情
《战狼2》 战争,动作,灾难</code></pre><p>5）创建hive表并导入数据</p><pre><code>create table movie_info(
  movie string, 
  category string) 
row format delimited fields terminated by &quot;\t&quot;;
load data local inpath &quot;/opt/module/hive/datas/movie_info.txt&quot; into table movie_info;</code></pre><p>6）按需求查询数据</p><pre><code>SELECT movie,category_name 
FROM movie_info 
lateral VIEW
explode(split(category,&quot;,&quot;)) movie_info_tmp AS category_name ;</code></pre><h3 id="5-2-5-窗口函数（开窗函数）"><a href="#5-2-5-窗口函数（开窗函数）" class="headerlink" title="5.2.5 窗口函数（开窗函数）"></a>5.2.5 窗口函数（开窗函数）</h3><p>1）相关函数说明</p><p>**OVER()**：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的改变而变化。涉及关键字如下</p><table><thead><tr><th>CURRENT ROW</th><th>当前行</th></tr></thead><tbody><tr><td>n PRECEDING</td><td>往前n行数据</td></tr><tr><td>n FOLLOWING</td><td>往后n行数据</td></tr><tr><td>UNBOUNDED</td><td>起点</td></tr><tr><td>UNBOUNDED PRECEDING</td><td>表示从前面的起点</td></tr><tr><td>UNBOUNDED FOLLOWING</td><td>表示到后面的终点</td></tr><tr><td>LAG(col,n,default_val)</td><td>往前第n行数据</td></tr><tr><td>LEAD(col,n, default_val)</td><td>往后第n行数据</td></tr><tr><td>NTILE(n)</td><td>把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型</td></tr></tbody></table><p><strong>总结如下：</strong></p><p><strong>OVER()</strong> 默认为每条数据都开一个窗口，窗口大小是当前数据集的大小。</p><p><strong>OVER(partition by…. )</strong> 会按照指定字段进行分区，将分区字段的值仙童的数据划分到相同的区；每个区中的每条数据都会开启一个窗口，每条数据的窗口大小默认为当前分区数据的大小。</p><p><strong>OVER(order by …)</strong> 会在窗口中按照指定的字段对数据进行排序，会为每条数据都开启一个窗口，默认窗口大小为从数据集开始到当前行。</p><p>**OVER(partition by …order by..)**：会按照指定的字段进行分区，将分区字段的值仙童的数据划分到相同的区，在每个区会按照指定字段进行排序；每个区中的每条数据都会开启一个窗口，每条数据的窗口大小默认为当前分区中从数据集开始到当前行。相当于：partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row</p><p><strong>关键字总结</strong></p><table><thead><tr><th>Order by</th><th>全局排序；窗口函数中排序</th></tr></thead><tbody><tr><td>Distribute by</td><td>分区</td></tr><tr><td>Sort by</td><td>区内排序</td></tr><tr><td>Cluster by</td><td>分区排序</td></tr><tr><td>Partition by</td><td>窗口函数中分区</td></tr><tr><td>Partitioned by</td><td>建表 指定分区字段</td></tr><tr><td>Clustered by</td><td>建表 指定分桶字段</td></tr></tbody></table><p><strong>注意partition by …order by组合；Distribute by和Sort by 组合使用</strong></p><p>2）数据准备：name，orderdate，cost</p><pre class="line-numbers language-sh"><code class="language-sh">jack,2017-01-01,10
tony,2017-01-02,15
jack,2017-02-03,23
tony,2017-01-04,29
jack,2017-01-05,46
jack,2017-04-06,42
tony,2017-01-07,50
jack,2017-01-08,55
mart,2017-04-08,62
mart,2017-04-09,68
neil,2017-05-10,12
mart,2017-04-11,75
neil,2017-06-12,80
mart,2017-04-13,94<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）需求</p><p>（1）查询在2017年4月份购买过的顾客及总人数</p><p>（2）查询顾客的购买明细及月购买总额</p><p>（3）上述的场景, 将每个顾客的cost按照日期进行累加</p><p>（4）查询每个顾客上次的购买时间</p><p>（5）查询前20%时间的订单信息</p><p>4）创建本地business.txt，导入数据</p><pre><code>[molly@hadoop102 datas]$ vi business.txt</code></pre><p>5）创建hive表并导入数据</p><pre><code>create table business(
name string, 
orderdate string,
cost int
) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;;
load data local inpath &quot;/opt/module/hive/datas/business.txt&quot; into table business;</code></pre><p>6）按需求查询数据</p><p>（1） 查询在2017年4月份购买过的顾客及总人数</p><pre><code>select name,count(*) over () 
from business 
where substring(orderdate,1,7) = &#39;2017-04&#39; 
group by name;</code></pre><p>（2） 查询顾客的购买明细及所有顾客的月购买总额</p><p>使用分区：按照每个月做分区</p><pre><code>sum(cost) over(partition by month(orderdate))：表示对某个月的所有顾客的购买总额求sum；当前over的窗口大小是分区大小。
select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from
business;</code></pre><p>（3） 将每个顾客的cost按照日期进行累加</p><pre><code>select name,orderdate,cost, 
sum(cost) over() as sample1,--所有行相加 
sum(cost) over(partition by name) as sample2,--按name分组，组内数据相加 
sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加 </code></pre><p>rows必须跟在Order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量</p><p>sample3的执行结果如下：</p><p>​ <img src="/2020/11/15/bigdata-hive2/1637036085261.png" alt="1637036085261"></p><p><strong>拓展：数据窗口大小变化</strong></p><pre><code>sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到当前行的聚合 
sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合 
sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行 
sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行 
from business;</code></pre><p>（4） 查看顾客上次的购买时间和下一次购买时间：使用lag和lead函数</p><pre><code>select name,orderdate,cost, 
lag(orderdate,1,&#39;1900-01-01&#39;) over(partition by name order by orderdate ) as p_ orderdate, lead(orderdate,1,&#39;9999-01-01&#39;) over (partition by name order by orderdate) as p_ orderdate
from business;</code></pre><p>​ <img src="/2020/11/15/bigdata-hive2/1637036128510.png" alt="1637036128510"></p><p>（5） 查询前20%时间的订单信息，使用NTILE函数进行分组。按照时间排序分成5组，取第一个组，即前20%</p><pre><code>select * from (
  select name,orderdate,cost, ntile(5) over(order by orderdate) gid
   from business
) t
where t. gid = 1;</code></pre><p>​ <img src="/2020/11/15/bigdata-hive2/1637036162813.png" alt="1637036162813"></p><h3 id="5-2-6-Rank"><a href="#5-2-6-Rank" class="headerlink" title="5.2.6 Rank"></a>5.2.6 Rank</h3><p>1）函数说明</p><p><strong>RANK() 排序相同时会重复，总数不会变</strong></p><p><strong>DENSE_RANK() 排序相同时会重复，总数会减少</strong></p><p><strong>ROW_NUMBER() 会根据顺序计算</strong></p><p>2）数据准备</p><table><thead><tr><th>name</th><th>subject</th><th>score</th></tr></thead><tbody><tr><td>孙悟空</td><td>语文</td><td>87</td></tr><tr><td>孙悟空</td><td>数学</td><td>95</td></tr><tr><td>孙悟空</td><td>英语</td><td>68</td></tr><tr><td>大海</td><td>语文</td><td>94</td></tr><tr><td>大海</td><td>数学</td><td>56</td></tr><tr><td>大海</td><td>英语</td><td>84</td></tr><tr><td>宋宋</td><td>语文</td><td>64</td></tr><tr><td>宋宋</td><td>数学</td><td>86</td></tr><tr><td>宋宋</td><td>英语</td><td>84</td></tr><tr><td>婷婷</td><td>语文</td><td>65</td></tr><tr><td>婷婷</td><td>数学</td><td>85</td></tr><tr><td>婷婷</td><td>英语</td><td>78</td></tr></tbody></table><p>3）需求</p><p>计算每门学科成绩排名。</p><p>4）创建本地score.txt，导入数据</p><pre><code>[molly@hadoop102 datas]$ vi score.txt</code></pre><p>5）创建hive表并导入数据</p><pre><code>create table score(
name string,
subject string, 
score int) 
row format delimited fields terminated by &quot;\t&quot;;
load data local inpath &#39;/opt/module/hive/datas/score.txt&#39; into table score;</code></pre><p>6）按需求查询数据</p><pre><code>select name,subject,score,rank() over(partition by subject order by score desc) rp,
dense_rank() over(partition by subject order by score desc) drp,
row_number() over(partition by subject order by score desc) rmp
from score;</code></pre><p>查询结果如下面所示：</p><pre><code>name  subject score  rp   drp   rmp
孙悟空 数学  95   1    1    1
宋宋  数学  86   2    2    2
婷婷  数学  85   3    3    3
大海  数学  56   4    4    4
宋宋  英语  84   1    1    1
大海  英语  84   1    1    2
婷婷  英语  78   3    2    3
孙悟空 英语  68   4    3    4
大海  语文  94   1    1    1
孙悟空 语文  87   2    2    2
婷婷  语文  65   3    3    3
宋宋  语文  64   4    4    4</code></pre><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div></div><div class="copyright"><p><span>本文标题:</span><a href="/2020/11/15/bigdata-hive2/">Hive学习笔记（二） Hive对数据基本操作</a></p><p><span>文章作者:</span><a href="/" title="回到主页">m01ly</a></p><p><span>发布时间:</span>2020-11-15, 15:45:51</p><p><span>最后更新:</span>2021-11-16, 16:55:28</p><p><span>原始链接:</span><a class="post-url" href="/2020/11/15/bigdata-hive2/" title="Hive学习笔记（二） Hive对数据基本操作">https://m01ly.github.io/2020/11/15/bigdata-hive2/</a></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target="_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。</p></div><nav id="article-nav"><div id="article-nav-newer" class="article-nav-title"><a href="/2020/11/15/bigdata-hive4-optimize/">Hive学习笔记（四） Hive的企业级调优</a></div><div id="article-nav-older" class="article-nav-title"><a href="/2020/11/15/bigdata-kafka1-setup/">kafka学习笔记（一） kafka搭建</a></div></nav></article><div id="toc" class="toc-article"><strong class="toc-title">文章目录</strong><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Hive%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-text">1 Hive数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-text">1.1 基本数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E9%9B%86%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-text">1.2 集合数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E7%B1%BB%E5%9E%8B%E8%BD%AC%E5%8C%96"><span class="toc-text">1.3 类型转化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-DDL%E6%95%B0%E6%8D%AE%E5%AE%9A%E4%B9%89"><span class="toc-text">2 DDL数据定义</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">2.1 创建数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">2.2 查询数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E4%BF%AE%E6%94%B9%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">2.3 修改数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">2.4 删除数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-%E5%88%9B%E5%BB%BA%E8%A1%A8"><span class="toc-text">2.5 创建表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-1-%E5%86%85%E9%83%A8%E8%A1%A8-%E7%AE%A1%E7%90%86%E8%A1%A8"><span class="toc-text">2.3.1 内部表(管理表)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-2-%E5%A4%96%E9%83%A8%E8%A1%A8"><span class="toc-text">2.3.2 外部表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-3-%E7%AE%A1%E7%90%86%E8%A1%A8%E4%B8%8E%E5%A4%96%E9%83%A8%E8%A1%A8%E7%9A%84%E4%BA%92%E7%9B%B8%E8%BD%AC%E6%8D%A2"><span class="toc-text">2.5.3 管理表与外部表的互相转换</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-%E4%BF%AE%E6%94%B9%E8%A1%A8"><span class="toc-text">2.6 修改表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-1-%E9%87%8D%E5%91%BD%E5%90%8D%E8%A1%A8"><span class="toc-text">2.6.1 重命名表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-2-%E5%A2%9E%E5%8A%A0-%E4%BF%AE%E6%94%B9-%E6%9B%BF%E6%8D%A2%E5%88%97%E4%BF%A1%E6%81%AF"><span class="toc-text">2.6.2 增加&#x2F;修改&#x2F;替换列信息</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-7-%E5%88%A0%E9%99%A4%E8%A1%A8"><span class="toc-text">2.7 删除表</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-DML%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-text">3 DML数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5"><span class="toc-text">3.1 数据导入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-%E5%90%91%E8%A1%A8%E4%B8%AD%E8%A3%85%E8%BD%BD%E6%95%B0%E6%8D%AE%EF%BC%88Load%EF%BC%89"><span class="toc-text">3.1.1 向表中装载数据（Load）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E9%80%9A%E8%BF%87%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E5%90%91%E8%A1%A8%E4%B8%AD%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE%EF%BC%88Insert%EF%BC%89"><span class="toc-text">3.1.2 通过查询语句向表中插入数据（Insert）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-3-%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E4%B8%AD%E5%88%9B%E5%BB%BA%E8%A1%A8%E5%B9%B6%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%EF%BC%88As-Select%EF%BC%89"><span class="toc-text">3.1.3 查询语句中创建表并加载数据（As Select）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-4-%E5%88%9B%E5%BB%BA%E8%A1%A8%E6%97%B6%E9%80%9A%E8%BF%87Location%E6%8C%87%E5%AE%9A%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E8%B7%AF%E5%BE%84%E2%80%94%E7%94%A8%E7%9A%84%E5%A4%9A"><span class="toc-text">3.1.4 创建表时通过Location指定加载数据路径—用的多</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-5-Import%E6%95%B0%E6%8D%AE%E5%88%B0%E6%8C%87%E5%AE%9AHive%E8%A1%A8%E4%B8%AD"><span class="toc-text">3.1.5 Import数据到指定Hive表中</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA"><span class="toc-text">3.2 数据导出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-Insert%E5%AF%BC%E5%87%BA"><span class="toc-text">3.2.1 Insert导出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-Hadoop%E5%91%BD%E4%BB%A4%E5%AF%BC%E5%87%BA%E5%88%B0%E6%9C%AC%E5%9C%B0"><span class="toc-text">3.2.2 Hadoop命令导出到本地</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-Hive-Shell-%E5%91%BD%E4%BB%A4%E5%AF%BC%E5%87%BA"><span class="toc-text">3.2.3 Hive Shell 命令导出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-4-Export%E5%AF%BC%E5%87%BA%E5%88%B0HDFS%E4%B8%8A"><span class="toc-text">3.2.4 Export导出到HDFS上</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-5-%E6%B8%85%E9%99%A4%E8%A1%A8%E4%B8%AD%E6%95%B0%E6%8D%AE%EF%BC%88Truncate%EF%BC%89"><span class="toc-text">3.2.5 清除表中数据（Truncate）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E2%80%94%E7%94%A8%E7%9A%84%E5%BE%88%E5%A4%9A"><span class="toc-text">4 数据查询—用的很多</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%85%A8%E5%B1%80%E6%8E%92%E5%BA%8F%EF%BC%88Order-By%EF%BC%89"><span class="toc-text">4.1 全局排序（Order By）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%AF%8F%E4%B8%AAReduce%E5%86%85%E9%83%A8%E6%8E%92%E5%BA%8F%EF%BC%88Sort-By%EF%BC%89"><span class="toc-text">4.2 每个Reduce内部排序（Sort By）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%88%86%E5%8C%BA%EF%BC%88Distribute-By%EF%BC%89"><span class="toc-text">4.3 分区（Distribute By）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Cluster-By"><span class="toc-text">4.4 Cluster By</span></a></li></ol></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8"><span class="toc-text">5 函数使用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E7%B3%BB%E7%BB%9F%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="toc-text">5.1 系统内置函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E5%B8%B8%E7%94%A8%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="toc-text">5.2 常用内置函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-1-%E7%A9%BA%E5%AD%97%E6%AE%B5%E8%B5%8B%E5%80%BC"><span class="toc-text">5.2.1 空字段赋值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-2-CASE-WHEN-THEN-ELSE-END"><span class="toc-text">5.2.2 CASE WHEN THEN ELSE END</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-3-%E8%A1%8C%E8%BD%AC%E5%88%97"><span class="toc-text">5.2.3 行转列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-4-%E5%88%97%E8%BD%AC%E8%A1%8C"><span class="toc-text">5.2.4 列转行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-5-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%EF%BC%88%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0%EF%BC%89"><span class="toc-text">5.2.5 窗口函数（开窗函数）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-6-Rank"><span class="toc-text">5.2.6 Rank</span></a></li></ol></li></ol></li></div><style>.left-col .switch-area,.left-col .switch-btn{display:none}.toc-level-6 i,.toc-level-6 ol{display:none!important}</style><input type="button" id="tocButton" value="隐藏目录" title="点击按钮隐藏或者显示文章目录"><script>yiliaConfig.toc=["隐藏目录","显示目录",!0],$(".left-col").is(":hidden")&&$("#tocButton").attr("value",yiliaConfig.toc[1])</script><div class="share"><link rel="stylesheet" type="text/css" href="/share/iconfont.css"><link rel="stylesheet" href="/share/spongebob.min.css" type="text/css" media="all"><div class="social_share"><ul id="social_list" class="social_icon_list"></ul></div><script>var shareConfig={title:"Hive学习笔记（二） Hive对数据基本操作",url:window.location.href,author:"m01ly",img:"https:/img/avatar.jpg"}</script><script src="/share/qrcode.min.js"></script><script src="/share/spongebob.min.js"></script></div><section id="comments" style="margin:2em;padding:2em;background:rgba(255,255,255,.5)"><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script><div id="gitalk-container"></div><script type="text/javascript">var gitalk=new Gitalk({clientID:"41a199ade404435645c4",clientSecret:"1de34fbb95212de986a29fea6d6f22bb57b2d473",repo:"m01ly.github.io",owner:"m01ly",admin:["m01ly"],id:window.location.pathname});gitalk.render("gitalk-container")</script></section><div class="scroll" id="post-nav-button"><a href="/2020/11/15/bigdata-hive4-optimize/" title="上一篇: Hive学习笔记（四） Hive的企业级调优"><i class="fa fa-angle-left"></i> </a><a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a> <a href="/2020/11/15/bigdata-kafka1-setup/" title="下一篇: kafka学习笔记（一） kafka搭建"><i class="fa fa-angle-right"></i></a></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/12/01/bigdata-mapreduce2-framework/">Hadoop 教程（五）mapreduce架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/29/leetcode-binarytree/">leetcode-binarytree</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/28/scan-nessus-compliance/">nessus扫描合规性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/22/cert-letsencrypt/">cert-letsencrypt</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/16/leetcode-stackandqueue/">栈和队列相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/06/htps-recommend/">安全的TLS协议</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/07/leetcode-slidewindow/">滑动窗口相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/07/leetcode-list/">链表相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/26/leetcode-binary/">二分查找相关的题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/24/burpsuite-develop-detect-nginx/">开发burpsuite插件-识别nginx版本并列出已知CVE</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/21/burpsuite-develop/">从0开发burpsuite插件（Java）</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/26/machine-learning-classify-knn/">机器学习算法之KNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/07/openvas-develop/">openvas插件开发</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/06/leetcode-daily/">leetcode每日一题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/30/pt-antSword/">渗透测试工具之蚁剑</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/29/leetcode-sort/">排序算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/11/linux-disk/">centos7把/mnt空间合并到/(根目录)</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/19/install-guide-elk-filebeats/">elk笔记三--利用elk+filebeat搭建SIEM系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/18/linux-jdk8/">linux安装jdk1.8</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/01/21/esc-DefectDojo/">DefectDojo安装与使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper3-API/">Zookeeper学习笔记（三） API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper2-framework/">Zookeeper学习笔记（二） 架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper1-setup/">Zookeeper学习笔记（一） 搭建教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/29/bigdata-Spark2-framework/">Spark学习笔记（二） 架构解析和RDD编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/29/bigdata-Spark1-setup/">Spark学习笔记（一） 搭建Spark</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase2-framework/">HBase学习笔记（二） HBase架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase4-phoenix/">HBase学习笔记（四） HBase整合phoenix和Hive</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase3-API/">HBase学习笔记（三） HBase的API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase1-setup/">HBase学习笔记（一） 安装教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/23/bigdata-datacollect1-userbehavior/">大数据实践（一）数仓采集项目</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/22/bigdata-sqoop/">sqoop安装教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/18/bigdata-kafka4-test/">kafka学习笔记（四） kafka面试集锦</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/17/bigdata-kafka3-API/">kafka学习笔记（三） kafka的API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/16/bigdata-flume3-monitor/">flume学习笔记（三） flum数据流监控及面试题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/16/bigdata-kafka2-framework/">kafka学习笔记（二） kafka框架深入</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-flume2-framework/">flume学习笔记（二） flum事务和部署架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-flume1-setup/">flume学习笔记（一） flume搭建</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive3/">Hive学习笔记（三） Hive的分区表和分桶表</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive5-example/">Hive学习笔记（五） Hive实战</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive4-optimize/">Hive学习笔记（四） Hive的企业级调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive2/">Hive学习笔记（二） Hive对数据基本操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-kafka1-setup/">kafka学习笔记（一） kafka搭建</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/14/bigdata-hive1/">Hive学习笔记（一） Hive安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs1/">Hadoop 教程（二）安装hadoop集群-完全分布式部署及API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs/">Hadoop 教程（一）hadoop介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-yarn-framework/">Hadoop 教程（六）yarn-架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-mapreduce1-setup/">Hadoop 教程（四）mapreduce介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs3-framework/">Hadoop 教程（三）hdfs-架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/10/cipher-certificate-format/">证书的各种格式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/docker-guide/">docker使用大全</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/linux-cmd/">linux命令大全</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-metosploitInAliyun/">在阿里云主机反弹metosploit</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-info-collection/">信息收集</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-tools/">最佳网络安全和黑客软件</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/18/mobilesecurity-experience/">小白如何在三天一步步逆向app，找到私钥</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/elk-login/">elk笔记二--通过X-Pack权限控制设置elk登录</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-centosInvm/">vm 安装centos 7教程详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/writeup-sqli-labs/">writeup-sqli-labs</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-elk-suricata/">elk笔记一---suricata+elk搭建入侵检测系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-suricata/">centos7中安装suricata</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/10/pt-sqlbypass/">sql关键词绕过</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/04/pt-portinfo/">常见端口说明和攻击汇总</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/03/m01ly-wiki/">m01ly-wiki</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/03/htps-attack-heartbleed/">TLS攻击之心脏滴血</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/01/htps-attack-paddingoracle/">TLS 攻击之POODLE</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/31/blockcipher-padding/">分组密码--填充模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/31/blockcipher-operation-mode/">分组密码--工作模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/hexo-guide/">Hexo踩坑</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/scan-awvs-nessus/">AWVS和Nessus镜像安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/scan-zap/">ZAP的安装和使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/pt-tiquan/">提权</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/htps-tools/">TLS安全检测小工具</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/htps-build/">搭建https网站</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/apple/">竟然有人能把https/TLS1.2协议讲的这么详细</a></li></ul><script></script></div><footer id="footer"><div class="outer"><div id="footer-info"><div class="footer-left"><i class="fa fa-copyright"></i> 2017-2021 冀-18010769-1</div><div class="visit"><span id="busuanzi_container_site_pv" style="display:none"><span id="site-visit" title="本站到访人数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span> </span></span><span>| </span><span id="busuanzi_container_page_pv" style="display:none"><span id="page-visit" title="本页访问次数"><i class="fa fa-eye" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span></span></span></div><div class="footer-right"><i class="fa fa-heart"></i><a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架"> Hexo</a> Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a></div></div></div></footer></div><script type="application/javascript">var leftWidth,hide=!1;$(".hide-left-col").click(function(){hide=hide?($(".left-col").css("width",leftWidth),$(".left-col .intrude-less").fadeIn(200),$("#tocButton").fadeIn(200),"block"===$("#switch-btn").css("display")&&"block"===$("#switch-area").css("display")||$("#toc").fadeIn(200),$(".hide-left-col").css("left",leftWidth).html('<i class="fa fa-angle-double-left"></i>'),$(".mid-col").css("left",leftWidth),$("#post-nav-button").css("left",leftWidth),$("#post-nav-button > a:nth-child(2)").css("display","block"),!1):(leftWidth=$(".left-col")[0].style.width,$(".left-col").css("width",0),$(".left-col .intrude-less").fadeOut(200),$("#toc").fadeOut(100),$("#tocButton").fadeOut(100),$(".hide-left-col").css("left",0).html('<i class="fa fa-angle-double-right"></i>'),$(".mid-col").css("left",0),$("#post-nav-button").css("left",0),$("#post-nav-button > a:nth-child(2)").css("display","none"),$(".post-list").is(":visible")&&($("#post-nav-button .fa-bars,#post-nav-button .fa-times").toggle(),$(".post-list").toggle()),!0)})</script><script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.3.5/require.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[["$$","$$"],["$","$"],["\\(","\\)"]],processEscapes:!0,skipTags:["script","noscript","style","textarea","pre","code"]}}),MathJax.Hub.Queue(function(){var a,e=MathJax.Hub.getAllJax();for(a=0;a<e.length;a+=1)e[a].SourceElement().parentNode.className+=" has-jax"})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script><div class="scroll" id="scroll"><a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a> <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a> <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a></div><script>var oOpenInNew={post:".copyright a[href]",friends:"#js-friends a",socail:".social a"};for(var x in oOpenInNew)$(oOpenInNew[x]).attr("target","_blank")</script><script>var titleTime,originTitle=document.title;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="(つェ⊂)"+originTitle,clearTimeout(titleTime)):(document.title="(*´∇｀*)~ "+originTitle,titleTime=setTimeout(function(){document.title=originTitle},2e3))})</script><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><link href="//cdn.bootcss.com/aos/2.2.0/aos.css" rel="stylesheet"><script type="text/javascript">AOS.init({easing:"ease-out-back",once:!0})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"live2d_models/live2d-widget-model-izumi"},"display":{"position":"right","width":100,"height":200,"hOffset":-50,"vOffset":-85},"mobile":{"show":false},"react":{"opacityDefault":0.9,"opacityOnHover":0.3},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false});</script></body></html>