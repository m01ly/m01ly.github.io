<!DOCTYPE html><html lang="zh-Hans"><head><!--[if IE]><style>body{display:none;}</style><script>alert('IE浏览器下无法展示效果，请更换浏览器！');var headNode=document.getElementsByTagName('head')[0];var refresh=document.createElement('meta');refresh.setAttribute('http-equiv','Refresh');refresh.setAttribute('Content','0; url=http://outdatedbrowser.com/');headNode.appendChild(refresh);</script><![endif]--><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="m01ly"><meta name="description" content="对于Hive的操作是面对大数据层面，因此对于查询效率是有要求的，这篇主要从以下几个方面进行调优。 总结:优化：1 设置抓取为more:set hive.fetch.task.conversion&#x3D;more;2 开启本地模式:配置小数据量放到本地跑set hive.exec.mode.local.auto&#x3D;true;  &#x2F;&#x2F;开启本地mr&#x2F;&#x2F;设置local mr的最大输入数据量，当输入数据量小于这个值"><meta property="og:type" content="article"><meta property="og:title" content="Hive学习笔记（四） Hive的企业级调优"><meta property="og:url" content="https://m01ly.github.io/2020/11/15/bigdata-hive4-optimize/index.html"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="对于Hive的操作是面对大数据层面，因此对于查询效率是有要求的，这篇主要从以下几个方面进行调优。 总结:优化：1 设置抓取为more:set hive.fetch.task.conversion&#x3D;more;2 开启本地模式:配置小数据量放到本地跑set hive.exec.mode.local.auto&#x3D;true;  &#x2F;&#x2F;开启本地mr&#x2F;&#x2F;设置local mr的最大输入数据量，当输入数据量小于这个值"><meta property="og:locale"><meta property="og:image" content="https://m01ly.github.io/2020/11/15/bigdata-hive4-optimize/1637047033227.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/15/bigdata-hive4-optimize/1637047154377.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/15/bigdata-hive4-optimize/1637047177458.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/15/bigdata-hive4-optimize/1637047313467.png"><meta property="article:published_time" content="2020-11-15T07:45:51.000Z"><meta property="article:modified_time" content="2021-11-16T07:31:40.892Z"><meta property="article:author" content="m01ly"><meta property="article:tag" content="Hive"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://m01ly.github.io/2020/11/15/bigdata-hive4-optimize/1637047033227.png"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="shortcut icon" href="/favicon.ico"><link href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css" rel="stylesheet"><link rel="stylesheet" href="/css/style.css"><title>Hive学习笔记（四） Hive的企业级调优 | Hexo</title><script src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"><script src="//cdn.bootcss.com/aos/2.2.0/aos.js"></script><script>var yiliaConfig={fancybox:!0,isHome:!1,isPost:!0,isArchive:!1,isTag:!1,isCategory:!1,fancybox_js:"//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js",search:!0}</script><script>yiliaConfig.jquery_ui=[!1]</script><script>yiliaConfig.rootUrl="/"</script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/rss+xml">
<link rel="stylesheet" href="/css/prism-a11y-dark.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="container"><div class="left-col"><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><form id="search-form"><input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false"> <i class="fa fa-times" onclick="resetSearch()"></i></form><div id="local-search-result"></div><p class="no-result">No results found <i class="fa fa-spinner fa-pulse"></i></p><div id="switch-btn" class="switch-btn"><div class="icon"><div class="icon-ctn"><div class="icon-wrap icon-house" data-idx="0"><div class="birdhouse"></div><div class="birdhouse_holes"></div></div><div class="icon-wrap icon-ribbon hide" data-idx="1"><div class="ribbon"></div></div><div class="icon-wrap icon-link hide" data-idx="2"><div class="loopback_l"></div><div class="loopback_r"></div></div><div class="icon-wrap icon-me hide" data-idx="3"><div class="user"></div><div class="shoulder"></div></div></div></div><div class="tips-box hide"><div class="tips-arrow"></div><ul class="tips-inner"><li>菜单</li><li>标签</li><li>友情链接</li><li>目标</li></ul></div></div><div id="switch-area" class="switch-area"><div class="switch-wrap"><section class="switch-part switch-part1"><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" rel="noopener" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" rel="noopener" href="https://music.163.com/" title="网易云音乐"></a></ul><ul class="social"><div class="donateIcon-position"><p style="display:block"><a class="donateIcon" href="javascript:void(0)" onmouseout='var qr=document.getElementById("donate");qr.style.display="none"' onmouseenter='var qr=document.getElementById("donate");qr.style.display="block"'>赏</a></p><div id="donate"><img id="multipay" src="/img/multipay.png" width="250px" alt="m01ly Multipay"><div class="triangle"></div></div></div></ul></nav></section><section class="switch-part switch-part2"><div class="widget tagcloud" id="js-tagcloud"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TLS/" rel="tag">TLS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zookeeper/" rel="tag">Zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/" rel="tag">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/" rel="tag">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BC%81%E4%B8%9A%E5%AE%89%E5%85%A8%E5%BB%BA%E8%AE%BE/" rel="tag">企业安全建设</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E5%85%A8%E6%89%AB%E6%8F%8F/" rel="tag">安全扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" rel="tag">安装教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8%E6%95%99%E7%A8%8B/" rel="tag">容器教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/" rel="tag">密码学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" rel="tag">插件开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E4%BB%93%E9%87%87%E9%9B%86%E9%A1%B9%E7%9B%AE/" rel="tag">数仓采集项目</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/" rel="tag">日志管理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/" rel="tag">流量分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" rel="tag">渗透测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%8F%E6%B4%9E%E6%89%AB%E6%8F%8F/" rel="tag">漏洞扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%BB%E5%8A%A8%E5%AE%89%E5%85%A8/" rel="tag">移动安全</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%B6%E5%9C%BAwriteup/" rel="tag">靶场writeup</a></li></ul></div></section><section class="switch-part switch-part3"><div id="js-friends"><a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/TechCatsLab">TechCatsLab</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://yangchenglong11.github.io">YangChengLong</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://jsharkc.github.io">LiuJiaChang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://blog.yusank.space">YusanKurban</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.lizebang.top">Lizebang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/sunanxiang">SunAnXiang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/DoubleWoodH">LinHao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.littlechao.top">ShiChao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/Txiaozhe">TangXiaoJi</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/LLLeon">JiaChenHui</a></div></section><section class="switch-part switch-part4"><div id="js-aboutme">不悲不喜，不卑不亢，努力成为一个更好的程序猿！</div></section></div></div></header></div></div><div class="hide-left-col" title="隐藏侧栏"><i class="fa fa-angle-double-left"></i></div><div class="mid-col"><nav id="mobile-nav"><div class="overlay"><div class="slider-trigger"></div><h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">m01ly</a></h1></div><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/" title="回到主页">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li><div class="clearfix"></div></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" href="https://music.163.com/" title="网易云音乐"></a></ul></nav></header></div><link class="menu-list" tags="标签" friends="友情链接" about="目标"></nav><div class="body-wrap"><article id="post-bigdata-hive4-optimize" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/15/bigdata-hive4-optimize/" class="article-date"><time class="published" datetime="2020-11-15T07:45:51.000Z" itemprop="datePublished">2020-11-15 发布</time> <time class="updated" datetime="2021-11-16T07:31:40.892Z" itemprop="dateUpdated">2021-11-16 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 class="article-title" itemprop="name">Hive学习笔记（四） Hive的企业级调优</h1></header><div class="article-info article-info-post"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li></ul></div><span class="post-count">总字数4.9k</span> <span class="post-count">预计阅读21分钟</span></div><div class="clearfix"></div></div><div class="article-entry" itemprop="articleBody"><p>对于Hive的操作是面对大数据层面，因此对于查询效率是有要求的，这篇主要从以下几个方面进行调优。</p><p>总结:优化：<br>1 设置抓取为more:set hive.fetch.task.conversion=more;<br>2 开启本地模式:配置小数据量放到本地跑<br>set hive.exec.mode.local.auto=true; //开启本地mr<br>//设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128M<br>set hive.exec.mode.local.auto.inputbytes.max=50000000;<br>//设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4<br>set hive.exec.mode.local.auto.input.files.max=10;</p><h1 id="1-执行计划（Explain）"><a href="#1-执行计划（Explain）" class="headerlink" title="1 执行计划（Explain）"></a>1 执行计划（Explain）</h1><p>explain很详细的看到语句执行过程中发生的事情。</p><h2 id="1-1基本语法"><a href="#1-1基本语法" class="headerlink" title="1.1基本语法"></a>1.1基本语法</h2><p>EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query</p><p>Explain主要是分析一下sql的执行过程。</p><h2 id="1-2-案例实操"><a href="#1-2-案例实操" class="headerlink" title="1.2 案例实操"></a>1.2 案例实操</h2><p>（1）查看下面这条语句的执行计划</p><p>没有生成MR任务的</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> explain <span class="token keyword">select</span> * from emp<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>有生成MR任务的</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> explain <span class="token keyword">select</span> deptno, avg<span class="token punctuation">(</span>sal<span class="token punctuation">)</span> avg_sal from emp group by deptno<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）查看详细执行计划</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> explain extended <span class="token keyword">select</span> * from emp<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> explain extended <span class="token keyword">select</span> deptno, avg<span class="token punctuation">(</span>sal<span class="token punctuation">)</span> avg_sal from emp group by deptno<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h1 id="2-Fetch抓取"><a href="#2-Fetch抓取" class="headerlink" title="2 Fetch抓取"></a>2 Fetch抓取</h1><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。</p><p>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>property<span class="token operator">></span>
  <span class="token operator">&lt;</span>name<span class="token operator">></span>hive.fetch.task.conversion<span class="token operator">&lt;</span>/name<span class="token operator">></span>
  <span class="token operator">&lt;</span>value<span class="token operator">></span>more<span class="token operator">&lt;</span>/value<span class="token operator">></span>
  <span class="token operator">&lt;</span>description<span class="token operator">></span>
   Expects one of <span class="token punctuation">[</span>none, minimal, more<span class="token punctuation">]</span>.
   Some <span class="token keyword">select</span> queries can be converted to single FETCH task minimizing latency.
   Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts <span class="token punctuation">(</span>which incurs RS<span class="token punctuation">)</span>, lateral views and joins.
   \0. none <span class="token keyword">:</span> disable hive.fetch.task.conversion
   \1. minimal <span class="token keyword">:</span> SELECT STAR, FILTER on partition columns, LIMIT only
   \2. <span class="token function">more</span> <span class="token keyword">:</span> SELECT, FILTER, LIMIT only <span class="token punctuation">(</span>support TABLESAMPLE and virtual columns<span class="token punctuation">)</span>
  <span class="token operator">&lt;</span>/description<span class="token operator">></span>
<span class="token operator">&lt;</span>/property<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-1-案例实操："><a href="#2-1-案例实操：" class="headerlink" title="2.1 案例实操："></a>2.1 案例实操：</h2><p>（1）把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> hive.fetch.task.conversion<span class="token operator">=</span>none<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> ename from emp<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> ename from emp limit 3<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>（2）把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> hive.fetch.task.conversion<span class="token operator">=</span>more<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> ename from emp<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> ename from emp limit 3<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-本地模式"><a href="#3-本地模式" class="headerlink" title="3 本地模式"></a>3 本地模式</h1><p>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</p><p>用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。</p><p>set hive.exec.mode.local.auto=true; //开启本地mr</p><p>//设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128M</p><p>set hive.exec.mode.local.auto.inputbytes.max=50000000;</p><p>//设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4</p><p>set hive.exec.mode.local.auto.input.files.max=10;</p><h2 id="3-1-案例实操："><a href="#3-1-案例实操：" class="headerlink" title="3.1 案例实操："></a>3.1 案例实操：</h2><p>（1）开启本地模式，并执行查询语句</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> hive.exec.mode.local.auto<span class="token operator">=</span>true<span class="token punctuation">;</span> 
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp cluster by deptno<span class="token punctuation">;</span>
Time taken: 1.328 seconds, Fetched: 14 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（2）关闭本地模式，并执行查询语句</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> hive.exec.mode.local.auto<span class="token operator">=</span>false<span class="token punctuation">;</span> 
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> * from emp cluster by deptno<span class="token punctuation">;</span>
Time taken: 20.09 seconds, Fetched: 14 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h1 id="4-表的优化"><a href="#4-表的优化" class="headerlink" title="4 表的优化"></a>4 表的优化</h1><h2 id="4-1-小表大表Join-MapJoin"><a href="#4-1-小表大表Join-MapJoin" class="headerlink" title="4.1 小表大表Join(MapJoin)"></a>4.1 小表大表Join(MapJoin)</h2><p>将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成join。</p><p>实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</p><p>案例实操</p><p>1）需求</p><p>测试大表JOIN小表和小表JOIN大表的效率</p><p>2）开启MapJoin参数设置</p><p>（1）设置自动选择Mapjoin</p><p>set hive.auto.convert.join = true; 默认为true</p><p>（2）大表小表的阈值设置（默认25M以下认为是小表）：</p><p>set hive.mapjoin.smalltable.filesize = 25000000;</p><p>3）MapJoin工作机制</p><p>​ <img src="/2020/11/15/bigdata-hive4-optimize/1637047033227.png" alt="1637047033227"></p><p>4）建大表、小表和JOIN后表的语句</p><pre class="line-numbers language-bash"><code class="language-bash">// 创建大表
create table bigtable<span class="token punctuation">(</span>id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
 // 创建小表
create table smalltable<span class="token punctuation">(</span>id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
// 创建join后表的语句
create table jointable<span class="token punctuation">(</span>id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5）分别向大表和小表中导入数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> load data local inpath <span class="token string">'/opt/module/hive/datas/bigtable'</span> into table bigtable<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span>load data local inpath <span class="token string">'/opt/module/hive/datas/smalltable'</span> into table smalltable<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>6）小表JOIN大表语句</p><pre class="line-numbers language-bash"><code class="language-bash">insert overwrite table jointable
<span class="token keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
from smalltable s
<span class="token function">join</span> bigtable b
on b.id <span class="token operator">=</span> s.id<span class="token punctuation">;</span>
Time taken: 35.921 seconds
No rows affected <span class="token punctuation">(</span>44.456 seconds<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>7）执行大表JOIN小表语句</p><pre class="line-numbers language-bash"><code class="language-bash">insert overwrite table jointable
<span class="token keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
from bigtable b
<span class="token function">join</span> smalltable s
on s.id <span class="token operator">=</span> b.id<span class="token punctuation">;</span>
Time taken: 34.196 seconds
No rows affected <span class="token punctuation">(</span>26.287 seconds<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-2-大表Join大表"><a href="#4-2-大表Join大表" class="headerlink" title="4.2 大表Join大表"></a>4.2 大表Join大表</h2><h3 id="4-2-1-空KEY过滤"><a href="#4-2-1-空KEY过滤" class="headerlink" title="4.2.1 空KEY过滤"></a>4.2.1 空KEY过滤</h3><p>有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：</p><p>案例实操</p><p>（1）配置历史服务器</p><p>配置mapred-site.xml</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>property<span class="token operator">></span>
<span class="token operator">&lt;</span>name<span class="token operator">></span>mapreduce.jobhistory.address<span class="token operator">&lt;</span>/name<span class="token operator">></span>
<span class="token operator">&lt;</span>value<span class="token operator">></span>hadoop102:10020<span class="token operator">&lt;</span>/value<span class="token operator">></span>
<span class="token operator">&lt;</span>/property<span class="token operator">></span>
<span class="token operator">&lt;</span>property<span class="token operator">></span>
  <span class="token operator">&lt;</span>name<span class="token operator">></span>mapreduce.jobhistory.webapp.address<span class="token operator">&lt;</span>/name<span class="token operator">></span>
  <span class="token operator">&lt;</span>value<span class="token operator">></span>hadoop102:19888<span class="token operator">&lt;</span>/value<span class="token operator">></span>
<span class="token operator">&lt;</span>/property<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>启动历史服务器</p><pre class="line-numbers language-bash"><code class="language-bash">sbin/mr-jobhistory-daemon.sh start historyserver<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查看jobhistory</p><p><a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p><p>（2）创建原始数据表、空id表、合并后数据表</p><p>// 创建空id表</p><pre class="line-numbers language-bash"><code class="language-bash">create table nullidtable<span class="token punctuation">(</span>id bigint, t bigint, uid string, keyword string, url_rank int, click_num int, click_url string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）分别加载原始数据和空id数据到对应表中</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> load data local inpath <span class="token string">'/opt/module/hive/datas/nullid'</span> into table nullidtable<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）测试不过滤空id</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite table jointable <span class="token keyword">select</span> n.* from nullidtable n
left <span class="token function">join</span> bigtable o on n.id <span class="token operator">=</span> o.id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（5）测试过滤空id</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> insert overwrite table jointable <span class="token keyword">select</span> n.* from <span class="token punctuation">(</span>select * from nullidtable where <span class="token function">id</span> is not null <span class="token punctuation">)</span> n left <span class="token function">join</span> bigtable o on n.id <span class="token operator">=</span> o.id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-2-2-空key转换"><a href="#4-2-2-空key转换" class="headerlink" title="4.2.2 空key转换"></a>4.2.2 空key转换</h3><p>有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如：</p><p>案例实操：</p><p>不随机分布空null值：</p><p>（1）设置5个reduce个数</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token keyword">set</span> mapreduce.job.reduces <span class="token operator">=</span> 5<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）JOIN两张表</p><pre class="line-numbers language-bash"><code class="language-bash">insert overwrite table jointable

<span class="token keyword">select</span> n.* from nullidtable n left <span class="token function">join</span> bigtable b on n.id <span class="token operator">=</span> b.id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>结果：如下图所示，可以看出来，出现了数据倾斜，某些reducer的资源消耗远大于其他reducer。</p><p>​ <img src="/2020/11/15/bigdata-hive4-optimize/1637047154377.png" alt="1637047154377"></p><p>随机分布空null值</p><p>（1）设置5个reduce个数</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token keyword">set</span> mapreduce.job.reduces <span class="token operator">=</span> 5<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）JOIN两张表</p><pre class="line-numbers language-bash"><code class="language-bash">insert overwrite table jointable
<span class="token keyword">select</span> n.* from nullidtable n full <span class="token function">join</span> bigtable o on 
nvl<span class="token punctuation">(</span>n.id,rand<span class="token punctuation">(</span><span class="token punctuation">))</span> <span class="token operator">=</span> o.id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>结果：如下图所示，可以看出来，消除了数据倾斜，负载均衡reducer的资源消耗</p><p>​ <img src="/2020/11/15/bigdata-hive4-optimize/1637047177458.png" alt="1637047177458"></p><h3 id="4-2-3-SMB-Sort-Merge-Bucket-join"><a href="#4-2-3-SMB-Sort-Merge-Bucket-join" class="headerlink" title="4.2.3 SMB(Sort Merge Bucket join)"></a>4.2.3 SMB(Sort Merge Bucket join)</h3><p>（1）创建第二张大表</p><pre class="line-numbers language-bash"><code class="language-bash">create table bigtable2<span class="token punctuation">(</span>
  <span class="token function">id</span> bigint,
  t bigint,
  uid string,
  keyword string,
  url_rank int,
  click_num int,
  click_url string<span class="token punctuation">)</span>
row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
load data local inpath <span class="token string">'/opt/module/data/bigtable'</span> into table bigtable2<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>测试大表直接JOIN</p><pre class="line-numbers language-bash"><code class="language-bash">insert overwrite table jointable
<span class="token keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
from bigtable s
<span class="token function">join</span> bigtable2 b
on b.id <span class="token operator">=</span> s.id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）创建分桶表1,桶的个数不要超过可用CPU的核数</p><pre class="line-numbers language-bash"><code class="language-bash">create table bigtable_buck1<span class="token punctuation">(</span>
  <span class="token function">id</span> bigint,
  t bigint,
  uid string,
  keyword string,
  url_rank int,
  click_num int,
  click_url string<span class="token punctuation">)</span>
clustered by<span class="token punctuation">(</span>id<span class="token punctuation">)</span> 
sorted by<span class="token punctuation">(</span>id<span class="token punctuation">)</span>
into 6 buckets
row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
insert into bigtable_buck1 <span class="token keyword">select</span> * from bigtable<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）创建分通表2,桶的个数不要超过可用CPU的核数</p><pre class="line-numbers language-bash"><code class="language-bash">create table bigtable_buck2<span class="token punctuation">(</span>
  <span class="token function">id</span> bigint,
  t bigint,
  uid string,
  keyword string,
  url_rank int,
  click_num int,
  click_url string<span class="token punctuation">)</span>
clustered by<span class="token punctuation">(</span>id<span class="token punctuation">)</span>
sorted by<span class="token punctuation">(</span>id<span class="token punctuation">)</span> 
into 6 buckets
row <span class="token function">format</span> delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
insert into bigtable_buck2 <span class="token keyword">select</span> * from bigtable<span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（4）设置参数</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token keyword">set</span> hive.optimize.bucketmapjoin <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive.input.format<span class="token operator">=</span>org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（5）测试</p><pre class="line-numbers language-bash"><code class="language-bash">insert overwrite table jointable
<span class="token keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
from bigtable_buck1 s
<span class="token function">join</span> bigtable_buck2 b
on b.id <span class="token operator">=</span> s.id<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-3-Group-By"><a href="#4-3-Group-By" class="headerlink" title="4.3 Group By"></a>4.3 Group By</h2><p>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。</p><p>​ <img src="/2020/11/15/bigdata-hive4-optimize/1637047313467.png" alt="1637047313467"></p><p>并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p><h3 id="4-3-1-开启Map端聚合参数设置"><a href="#4-3-1-开启Map端聚合参数设置" class="headerlink" title="4.3.1 开启Map端聚合参数设置"></a>4.3.1 开启Map端聚合参数设置</h3><p>（1）是否在Map端进行聚合，默认为True</p><p>set hive.map.aggr = true</p><p>（2）在Map端进行聚合操作的条目数目</p><p>set hive.groupby.mapaggr.checkinterval = 100000</p><p>（3）有数据倾斜的时候进行负载均衡（默认是false）</p><p>set hive.groupby.skewindata = true</p><p>当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> deptno from emp group by deptno<span class="token punctuation">;</span>
Stage-Stage-1: Map: 1 Reduce: 5  Cumulative CPU: 23.68 sec  HDFS Read: 19987 HDFS Write: 9 SUCCESS
Total MapReduce CPU Time Spent: 23 seconds 680 msec
OK
deptno
10
20
30<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>优化以后</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> hive.groupby.skewindata <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> deptno from emp group by deptno<span class="token punctuation">;</span>
Stage-Stage-1: Map: 1 Reduce: 5  Cumulative CPU: 28.53 sec  HDFS Read: 18209 HDFS Write: 534 SUCCESS
Stage-Stage-2: Map: 1 Reduce: 5  Cumulative CPU: 38.32 sec  HDFS Read: 15014 HDFS Write: 9 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 6 seconds 850 msec
OK
deptno
10
20
30<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-4-Count-Distinct-去重统计"><a href="#4-4-Count-Distinct-去重统计" class="headerlink" title="4.4 Count(Distinct) 去重统计"></a>4.4 Count(Distinct) 去重统计</h2><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换,但是需要注意group by造成的数据倾斜问题.</p><p>1） 案例实操</p><p>（1）创建一张大表</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> create table bigtable<span class="token punctuation">(</span>id bigint, <span class="token function">time</span> bigint, uid string, keyword
string, url_rank int, click_num int, click_url string<span class="token punctuation">)</span> row <span class="token function">format</span> delimited
fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（2）加载数据</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> load data local inpath <span class="token string">'/opt/module/datas/bigtable'</span> into table bigtable<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）设置5个reduce个数</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token keyword">set</span> mapreduce.job.reduces <span class="token operator">=</span> 5<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）执行去重id查询</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> count<span class="token punctuation">(</span>distinct id<span class="token punctuation">)</span> from bigtable<span class="token punctuation">;</span>
Stage-Stage-1: Map: 1 Reduce: 1  Cumulative CPU: 7.12 sec  HDFS Read: 120741990 HDFS Write: 7 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 120 msec
OK
c0
100001
Time taken: 23.607 seconds, Fetched: 1 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（5）采用GROUP by去重id</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> count<span class="token punctuation">(</span>id<span class="token punctuation">)</span> from <span class="token punctuation">(</span>select <span class="token function">id</span> from bigtable group by id<span class="token punctuation">)</span> a<span class="token punctuation">;</span>
Stage-Stage-1: Map: 1 Reduce: 5  Cumulative CPU: 17.53 sec  HDFS Read: 120752703 HDFS Write: 580 SUCCESS
Stage-Stage-2: Map: 1 Reduce: 1  Cumulative CPU: 4.29 sec2  HDFS Read: 9409 HDFS Write: 7 SUCCESS
Total MapReduce CPU Time Spent: 21 seconds 820 msec
OK
_c0
100001
Time taken: 50.795 seconds, Fetched: 1 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。</p><h2 id="4-5-笛卡尔积"><a href="#4-5-笛卡尔积" class="headerlink" title="4.5 笛卡尔积"></a>4.5 笛卡尔积</h2><p>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p><h2 id="4-6-行列过滤"><a href="#4-6-行列过滤" class="headerlink" title="4.6 行列过滤"></a>4.6 行列过滤</h2><p>列处理：在SELECT中，只拿需要的列，如果有分区，尽量使用分区过滤，少用SELECT *。</p><p>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤，比如：</p><p>案例实操：</p><p>1）测试先关联两张表，再用where条件过滤</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> o.id from bigtable b
<span class="token function">join</span> bigtable  o.id <span class="token operator">=</span> b.id
where o.id <span class="token operator">&lt;=</span> 10<span class="token punctuation">;</span>
Time taken: 34.406 seconds, Fetched: 100 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>2）通过子查询后，再关联表</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> b.id from bigtable b
<span class="token function">join</span> <span class="token punctuation">(</span>select <span class="token function">id</span> from bigtable where <span class="token function">id</span> <span class="token operator">&lt;=</span> 10 <span class="token punctuation">)</span> o on b.id <span class="token operator">=</span> o.id<span class="token punctuation">;</span>
Time taken: 30.058 seconds, Fetched: 100 row<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="4-7-分区分桶"><a href="#4-7-分区分桶" class="headerlink" title="4.7 分区分桶"></a>4.7 分区分桶</h2><p>在涉及存储结构时候，设置分区分桶，查找时候效率就会更高。</p><h1 id="5-合理设置Map及Reduce数"><a href="#5-合理设置Map及Reduce数" class="headerlink" title="5 合理设置Map及Reduce数"></a>5 合理设置Map及Reduce数</h1><p>1）通常情况下，作业会通过input的目录产生一个或者多个map任务。</p><p>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</p><p>2）是不是map数越多越好？</p><p>答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</p><p>3）是不是保证每个map处理接近128m的文件块，就高枕无忧了？</p><p>答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</p><p>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；</p><h2 id="5-1-复杂文件增加Map数"><a href="#5-1-复杂文件增加Map数" class="headerlink" title="5.1 复杂文件增加Map数"></a>5.1 复杂文件增加Map数</h2><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p><p>增加map的方法为：根据</p><p>computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</p><p>案例实操：</p><p>1）执行查询</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> count<span class="token punctuation">(</span>*<span class="token punctuation">)</span> from emp<span class="token punctuation">;</span>
Hadoop job information <span class="token keyword">for</span> Stage-1: number of mappers: 1<span class="token punctuation">;</span> number of reducers: 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2）设置最大切片值为100个字节</p><pre class="line-numbers language-bash"><code class="language-bash">hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">set</span> mapreduce.input.fileinputformat.split.maxsize<span class="token operator">=</span>100<span class="token punctuation">;</span>
hive <span class="token punctuation">(</span>default<span class="token punctuation">)</span><span class="token operator">></span> <span class="token keyword">select</span> count<span class="token punctuation">(</span>*<span class="token punctuation">)</span> from emp<span class="token punctuation">;</span>
Hadoop job information <span class="token keyword">for</span> Stage-1: number of mappers: 6<span class="token punctuation">;</span> number of reducers: 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="5-2-小文件进行合并"><a href="#5-2-小文件进行合并" class="headerlink" title="5.2 小文件进行合并"></a>5.2 小文件进行合并</h2><p>1）在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。</p><p>set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</p><p>2）在Map-Reduce的任务结束时合并小文件的设置：</p><p>在map-only任务结束时合并小文件，默认true</p><p>SET hive.merge.mapfiles = true;</p><p>在map-reduce任务结束时合并小文件，默认false</p><p>SET hive.merge.mapredfiles = true;</p><p>合并文件的大小，默认256M</p><p>SET hive.merge.size.per.task = 268435456;</p><p>当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge</p><p>SET hive.merge.smallfiles.avgsize = 16777216;</p><h2 id="5-3-合理设置Reduce数"><a href="#5-3-合理设置Reduce数" class="headerlink" title="5.3 合理设置Reduce数"></a>5.3 合理设置Reduce数</h2><p>1）调整reduce个数方法一</p><p>（1）每个Reduce处理的数据量默认是256MB</p><p>hive.exec.reducers.bytes.per.reducer=256000000</p><p>（2）每个任务最大的reduce数，默认为1009</p><p>hive.exec.reducers.max=1009</p><p>（3）计算reducer数的公式</p><p>N=min(参数2，总输入数据量/参数1)</p><p>2）调整reduce个数方法二</p><p>在hadoop的mapred-default.xml文件中修改</p><p>设置每个job的Reduce个数</p><p>set mapreduce.job.reduces = 15;</p><p>3）reduce个数并不是越多越好</p><p>（1）过多的启动和初始化reduce也会消耗时间和资源；</p><p>（2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</p><p>在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；</p><h1 id="6-并行执行"><a href="#6-并行执行" class="headerlink" title="6 并行执行"></a>6 并行执行</h1><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p><p>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</p><p>set hive.exec.parallel=true; //打开任务并行执行</p><p>set hive.exec.parallel.thread.number=16; //同一个sql允许最大并行度，默认为8。</p><p>当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。</p><h1 id="7-严格模式"><a href="#7-严格模式" class="headerlink" title="7 严格模式"></a>7 严格模式</h1><p>Hive可以通过设置防止一些危险操作：</p><p>1）分区表不使用分区过滤</p><p>将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。<br>2）使用order by没有limit过滤</p><p>将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</p><p>3）笛卡尔积</p><p>将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在 执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p><h1 id="8-压缩"><a href="#8-压缩" class="headerlink" title="8 压缩"></a>8 压缩</h1><p>在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。</p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div></div><div class="copyright"><p><span>本文标题:</span><a href="/2020/11/15/bigdata-hive4-optimize/">Hive学习笔记（四） Hive的企业级调优</a></p><p><span>文章作者:</span><a href="/" title="回到主页">m01ly</a></p><p><span>发布时间:</span>2020-11-15, 15:45:51</p><p><span>最后更新:</span>2021-11-16, 15:31:40</p><p><span>原始链接:</span><a class="post-url" href="/2020/11/15/bigdata-hive4-optimize/" title="Hive学习笔记（四） Hive的企业级调优">https://m01ly.github.io/2020/11/15/bigdata-hive4-optimize/</a></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target="_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。</p></div><nav id="article-nav"><div id="article-nav-newer" class="article-nav-title"><a href="/2020/11/15/bigdata-hive5-example/">Hive学习笔记（五） Hive实战</a></div><div id="article-nav-older" class="article-nav-title"><a href="/2020/11/15/bigdata-hive2/">Hive学习笔记（二） Hive对数据基本操作</a></div></nav></article><div id="toc" class="toc-article"><strong class="toc-title">文章目录</strong><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%EF%BC%88Explain%EF%BC%89"><span class="toc-text">1 执行计划（Explain）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95"><span class="toc-text">1.1基本语法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-text">1.2 案例实操</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Fetch%E6%8A%93%E5%8F%96"><span class="toc-text">2 Fetch抓取</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%EF%BC%9A"><span class="toc-text">2.1 案例实操：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F"><span class="toc-text">3 本地模式</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%EF%BC%9A"><span class="toc-text">3.1 案例实操：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E8%A1%A8%E7%9A%84%E4%BC%98%E5%8C%96"><span class="toc-text">4 表的优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%B0%8F%E8%A1%A8%E5%A4%A7%E8%A1%A8Join-MapJoin"><span class="toc-text">4.1 小表大表Join(MapJoin)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E5%A4%A7%E8%A1%A8Join%E5%A4%A7%E8%A1%A8"><span class="toc-text">4.2 大表Join大表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-%E7%A9%BAKEY%E8%BF%87%E6%BB%A4"><span class="toc-text">4.2.1 空KEY过滤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2-%E7%A9%BAkey%E8%BD%AC%E6%8D%A2"><span class="toc-text">4.2.2 空key转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3-SMB-Sort-Merge-Bucket-join"><span class="toc-text">4.2.3 SMB(Sort Merge Bucket join)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-Group-By"><span class="toc-text">4.3 Group By</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-%E5%BC%80%E5%90%AFMap%E7%AB%AF%E8%81%9A%E5%90%88%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-text">4.3.1 开启Map端聚合参数设置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-Count-Distinct-%E5%8E%BB%E9%87%8D%E7%BB%9F%E8%AE%A1"><span class="toc-text">4.4 Count(Distinct) 去重统计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF"><span class="toc-text">4.5 笛卡尔积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6-%E8%A1%8C%E5%88%97%E8%BF%87%E6%BB%A4"><span class="toc-text">4.6 行列过滤</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-7-%E5%88%86%E5%8C%BA%E5%88%86%E6%A1%B6"><span class="toc-text">4.7 分区分桶</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AEMap%E5%8F%8AReduce%E6%95%B0"><span class="toc-text">5 合理设置Map及Reduce数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E5%A4%8D%E6%9D%82%E6%96%87%E4%BB%B6%E5%A2%9E%E5%8A%A0Map%E6%95%B0"><span class="toc-text">5.1 复杂文件增加Map数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E5%B0%8F%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E5%90%88%E5%B9%B6"><span class="toc-text">5.2 小文件进行合并</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AEReduce%E6%95%B0"><span class="toc-text">5.3 合理设置Reduce数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C"><span class="toc-text">6 并行执行</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E4%B8%A5%E6%A0%BC%E6%A8%A1%E5%BC%8F"><span class="toc-text">7 严格模式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-%E5%8E%8B%E7%BC%A9"><span class="toc-text">8 压缩</span></a></li></ol></div><style>.left-col .switch-area,.left-col .switch-btn{display:none}.toc-level-6 i,.toc-level-6 ol{display:none!important}</style><input type="button" id="tocButton" value="隐藏目录" title="点击按钮隐藏或者显示文章目录"><script>yiliaConfig.toc=["隐藏目录","显示目录",!0],$(".left-col").is(":hidden")&&$("#tocButton").attr("value",yiliaConfig.toc[1])</script><div class="share"><link rel="stylesheet" type="text/css" href="/share/iconfont.css"><link rel="stylesheet" href="/share/spongebob.min.css" type="text/css" media="all"><div class="social_share"><ul id="social_list" class="social_icon_list"></ul></div><script>var shareConfig={title:"Hive学习笔记（四） Hive的企业级调优",url:window.location.href,author:"m01ly",img:"https:/img/avatar.jpg"}</script><script src="/share/qrcode.min.js"></script><script src="/share/spongebob.min.js"></script></div><section id="comments" style="margin:2em;padding:2em;background:rgba(255,255,255,.5)"><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script><div id="gitalk-container"></div><script type="text/javascript">var gitalk=new Gitalk({clientID:"41a199ade404435645c4",clientSecret:"1de34fbb95212de986a29fea6d6f22bb57b2d473",repo:"m01ly.github.io",owner:"m01ly",admin:["m01ly"],id:window.location.pathname});gitalk.render("gitalk-container")</script></section><div class="scroll" id="post-nav-button"><a href="/2020/11/15/bigdata-hive5-example/" title="上一篇: Hive学习笔记（五） Hive实战"><i class="fa fa-angle-left"></i> </a><a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a> <a href="/2020/11/15/bigdata-hive2/" title="下一篇: Hive学习笔记（二） Hive对数据基本操作"><i class="fa fa-angle-right"></i></a></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/12/01/bigdata-mapreduce2-framework/">Hadoop 教程（五）mapreduce架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/29/leetcode-binarytree/">leetcode-binarytree</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/28/scan-nessus-compliance/">nessus扫描合规性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/22/cert-letsencrypt/">cert-letsencrypt</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/16/leetcode-stackandqueue/">栈和队列相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/06/htps-recommend/">安全的TLS协议</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/07/leetcode-slidewindow/">滑动窗口相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/07/leetcode-list/">链表相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/26/leetcode-binary/">二分查找相关的题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/24/burpsuite-develop-detect-nginx/">开发burpsuite插件-识别nginx版本并列出已知CVE</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/21/burpsuite-develop/">从0开发burpsuite插件（Java）</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/26/machine-learning-classify-knn/">机器学习算法之KNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/07/openvas-develop/">openvas插件开发</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/06/leetcode-daily/">leetcode每日一题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/30/pt-antSword/">渗透测试工具之蚁剑</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/29/leetcode-sort/">排序算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/11/linux-disk/">centos7把/mnt空间合并到/(根目录)</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/19/install-guide-elk-filebeats/">elk笔记三--利用elk+filebeat搭建SIEM系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/18/linux-jdk8/">linux安装jdk1.8</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/01/21/esc-DefectDojo/">DefectDojo安装与使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper3-API/">Zookeeper学习笔记（三） API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper2-framework/">Zookeeper学习笔记（二） 架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper1-setup/">Zookeeper学习笔记（一） 搭建教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/29/bigdata-Spark2-framework/">Spark学习笔记（二） 架构解析和RDD编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/29/bigdata-Spark1-setup/">Spark学习笔记（一） 搭建Spark</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase2-framework/">HBase学习笔记（二） HBase架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase4-phoenix/">HBase学习笔记（四） HBase整合phoenix和Hive</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase3-API/">HBase学习笔记（三） HBase的API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase1-setup/">HBase学习笔记（一） 安装教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/23/bigdata-datacollect1-userbehavior/">大数据实践（一）数仓采集项目</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/22/bigdata-sqoop/">sqoop安装教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/18/bigdata-kafka4-test/">kafka学习笔记（四） kafka面试集锦</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/17/bigdata-kafka3-API/">kafka学习笔记（三） kafka的API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/16/bigdata-flume3-monitor/">flume学习笔记（三） flum数据流监控及面试题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/16/bigdata-kafka2-framework/">kafka学习笔记（二） kafka框架深入</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-flume2-framework/">flume学习笔记（二） flum事务和部署架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-flume1-setup/">flume学习笔记（一） flume搭建</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive3/">Hive学习笔记（三） Hive的分区表和分桶表</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive5-example/">Hive学习笔记（五） Hive实战</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive4-optimize/">Hive学习笔记（四） Hive的企业级调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive2/">Hive学习笔记（二） Hive对数据基本操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-kafka1-setup/">kafka学习笔记（一） kafka搭建</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/14/bigdata-hive1/">Hive学习笔记（一） Hive安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs1/">Hadoop 教程（二）安装hadoop集群-完全分布式部署及API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs/">Hadoop 教程（一）hadoop介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-yarn-framework/">Hadoop 教程（六）yarn-架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-mapreduce1-setup/">Hadoop 教程（四）mapreduce介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs3-framework/">Hadoop 教程（三）hdfs-架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/10/cipher-certificate-format/">证书的各种格式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/docker-guide/">docker使用大全</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/linux-cmd/">linux命令大全</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-metosploitInAliyun/">在阿里云主机反弹metosploit</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-info-collection/">信息收集</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-tools/">最佳网络安全和黑客软件</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/18/mobilesecurity-experience/">小白如何在三天一步步逆向app，找到私钥</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/elk-login/">elk笔记二--通过X-Pack权限控制设置elk登录</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-centosInvm/">vm 安装centos 7教程详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/writeup-sqli-labs/">writeup-sqli-labs</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-elk-suricata/">elk笔记一---suricata+elk搭建入侵检测系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-suricata/">centos7中安装suricata</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/10/pt-sqlbypass/">sql关键词绕过</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/04/pt-portinfo/">常见端口说明和攻击汇总</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/03/m01ly-wiki/">m01ly-wiki</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/03/htps-attack-heartbleed/">TLS攻击之心脏滴血</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/01/htps-attack-paddingoracle/">TLS 攻击之POODLE</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/31/blockcipher-padding/">分组密码--填充模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/31/blockcipher-operation-mode/">分组密码--工作模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/hexo-guide/">Hexo踩坑</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/scan-awvs-nessus/">AWVS和Nessus镜像安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/scan-zap/">ZAP的安装和使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/pt-tiquan/">提权</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/htps-tools/">TLS安全检测小工具</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/htps-build/">搭建https网站</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/apple/">竟然有人能把https/TLS1.2协议讲的这么详细</a></li></ul><script></script></div><footer id="footer"><div class="outer"><div id="footer-info"><div class="footer-left"><i class="fa fa-copyright"></i> 2017-2021 冀-18010769-1</div><div class="visit"><span id="busuanzi_container_site_pv" style="display:none"><span id="site-visit" title="本站到访人数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span> </span></span><span>| </span><span id="busuanzi_container_page_pv" style="display:none"><span id="page-visit" title="本页访问次数"><i class="fa fa-eye" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span></span></span></div><div class="footer-right"><i class="fa fa-heart"></i><a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架"> Hexo</a> Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a></div></div></div></footer></div><script type="application/javascript">var leftWidth,hide=!1;$(".hide-left-col").click(function(){hide=hide?($(".left-col").css("width",leftWidth),$(".left-col .intrude-less").fadeIn(200),$("#tocButton").fadeIn(200),"block"===$("#switch-btn").css("display")&&"block"===$("#switch-area").css("display")||$("#toc").fadeIn(200),$(".hide-left-col").css("left",leftWidth).html('<i class="fa fa-angle-double-left"></i>'),$(".mid-col").css("left",leftWidth),$("#post-nav-button").css("left",leftWidth),$("#post-nav-button > a:nth-child(2)").css("display","block"),!1):(leftWidth=$(".left-col")[0].style.width,$(".left-col").css("width",0),$(".left-col .intrude-less").fadeOut(200),$("#toc").fadeOut(100),$("#tocButton").fadeOut(100),$(".hide-left-col").css("left",0).html('<i class="fa fa-angle-double-right"></i>'),$(".mid-col").css("left",0),$("#post-nav-button").css("left",0),$("#post-nav-button > a:nth-child(2)").css("display","none"),$(".post-list").is(":visible")&&($("#post-nav-button .fa-bars,#post-nav-button .fa-times").toggle(),$(".post-list").toggle()),!0)})</script><script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.3.5/require.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[["$$","$$"],["$","$"],["\\(","\\)"]],processEscapes:!0,skipTags:["script","noscript","style","textarea","pre","code"]}}),MathJax.Hub.Queue(function(){var a,e=MathJax.Hub.getAllJax();for(a=0;a<e.length;a+=1)e[a].SourceElement().parentNode.className+=" has-jax"})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script><div class="scroll" id="scroll"><a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a> <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a> <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a></div><script>var oOpenInNew={post:".copyright a[href]",friends:"#js-friends a",socail:".social a"};for(var x in oOpenInNew)$(oOpenInNew[x]).attr("target","_blank")</script><script>var titleTime,originTitle=document.title;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="(つェ⊂)"+originTitle,clearTimeout(titleTime)):(document.title="(*´∇｀*)~ "+originTitle,titleTime=setTimeout(function(){document.title=originTitle},2e3))})</script><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><link href="//cdn.bootcss.com/aos/2.2.0/aos.css" rel="stylesheet"><script type="text/javascript">AOS.init({easing:"ease-out-back",once:!0})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"live2d_models/live2d-widget-model-izumi"},"display":{"position":"right","width":100,"height":200,"hOffset":-50,"vOffset":-85},"mobile":{"show":false},"react":{"opacityDefault":0.9,"opacityOnHover":0.3},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false});</script></body></html>