<!DOCTYPE html><html lang="zh-Hans"><head><!--[if IE]><style>body{display:none;}</style><script>alert('IE浏览器下无法展示效果，请更换浏览器！');var headNode=document.getElementsByTagName('head')[0];var refresh=document.createElement('meta');refresh.setAttribute('http-equiv','Refresh');refresh.setAttribute('Content','0; url=http://outdatedbrowser.com/');headNode.appendChild(refresh);</script><![endif]--><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="m01ly"><meta name="description" content="1 数仓概念 2 项目需求及架构设计2.1 项目需求分析1）用户行为数据采集平台搭建（存在日志服务器-用flume采集到数据仓库（hdfs）中）2）业务数据采集平台搭建（存在mysql,oracle）：通过sqoop采集到数据仓库（hdfs）中3）数据仓库维度建模：建库建表4）分析，设备 会员 商品 地区 活动等电商核心主题，统计的报表指标近100个。5）采用即席查询工具，随时进行指标分析6）对集"><meta property="og:type" content="article"><meta property="og:title" content="大数据实践（一）数仓采集项目"><meta property="og:url" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/index.html"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="1 数仓概念 2 项目需求及架构设计2.1 项目需求分析1）用户行为数据采集平台搭建（存在日志服务器-用flume采集到数据仓库（hdfs）中）2）业务数据采集平台搭建（存在mysql,oracle）：通过sqoop采集到数据仓库（hdfs）中3）数据仓库维度建模：建库建表4）分析，设备 会员 商品 地区 活动等电商核心主题，统计的报表指标近100个。5）采用即席查询工具，随时进行指标分析6）对集"><meta property="og:locale"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637651564709.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637652372079.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637657318225.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637661734977.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637662122810.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637662603236.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637662802344.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637662962636.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637662988690.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637663231181.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637663793390.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637664623705.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637724868076.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637722930078.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637725114820.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637738855416.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637739135017.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637740975327.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637745317255.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637745332754.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637746045465.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637746273106.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637746298966.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637746311874.png"><meta property="og:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637746489890.png"><meta property="article:published_time" content="2020-11-23T07:10:21.000Z"><meta property="article:modified_time" content="2021-11-25T02:30:35.149Z"><meta property="article:author" content="m01ly"><meta property="article:tag" content="数仓采集项目"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/1637651564709.png"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="shortcut icon" href="/favicon.ico"><link href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css" rel="stylesheet"><link rel="stylesheet" href="/css/style.css"><title>大数据实践（一）数仓采集项目 | Hexo</title><script src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"><script src="//cdn.bootcss.com/aos/2.2.0/aos.js"></script><script>var yiliaConfig={fancybox:!0,isHome:!1,isPost:!0,isArchive:!1,isTag:!1,isCategory:!1,fancybox_js:"//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js",search:!0}</script><script>yiliaConfig.jquery_ui=[!1]</script><script>yiliaConfig.rootUrl="/"</script><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/rss+xml">
<link rel="stylesheet" href="/css/prism-a11y-dark.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="container"><div class="left-col"><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><form id="search-form"><input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false"> <i class="fa fa-times" onclick="resetSearch()"></i></form><div id="local-search-result"></div><p class="no-result">No results found <i class="fa fa-spinner fa-pulse"></i></p><div id="switch-btn" class="switch-btn"><div class="icon"><div class="icon-ctn"><div class="icon-wrap icon-house" data-idx="0"><div class="birdhouse"></div><div class="birdhouse_holes"></div></div><div class="icon-wrap icon-ribbon hide" data-idx="1"><div class="ribbon"></div></div><div class="icon-wrap icon-link hide" data-idx="2"><div class="loopback_l"></div><div class="loopback_r"></div></div><div class="icon-wrap icon-me hide" data-idx="3"><div class="user"></div><div class="shoulder"></div></div></div></div><div class="tips-box hide"><div class="tips-arrow"></div><ul class="tips-inner"><li>菜单</li><li>标签</li><li>友情链接</li><li>目标</li></ul></div></div><div id="switch-area" class="switch-area"><div class="switch-wrap"><section class="switch-part switch-part1"><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" rel="noopener" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" rel="noopener" href="https://music.163.com/" title="网易云音乐"></a></ul><ul class="social"><div class="donateIcon-position"><p style="display:block"><a class="donateIcon" href="javascript:void(0)" onmouseout='var qr=document.getElementById("donate");qr.style.display="none"' onmouseenter='var qr=document.getElementById("donate");qr.style.display="block"'>赏</a></p><div id="donate"><img id="multipay" src="/img/multipay.png" width="250px" alt="m01ly Multipay"><div class="triangle"></div></div></div></ul></nav></section><section class="switch-part switch-part2"><div class="widget tagcloud" id="js-tagcloud"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TLS/" rel="tag">TLS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zookeeper/" rel="tag">Zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/" rel="tag">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/" rel="tag">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BC%81%E4%B8%9A%E5%AE%89%E5%85%A8%E5%BB%BA%E8%AE%BE/" rel="tag">企业安全建设</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E5%85%A8%E5%B7%A5%E5%85%B7/" rel="tag">安全工具</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" rel="tag">安装教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8%E6%95%99%E7%A8%8B/" rel="tag">容器教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/" rel="tag">密码学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" rel="tag">插件开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E4%BB%93%E9%87%87%E9%9B%86%E9%A1%B9%E7%9B%AE/" rel="tag">数仓采集项目</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/" rel="tag">日志管理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/" rel="tag">流量分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" rel="tag">渗透测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%8F%E6%B4%9E%E5%A4%8D%E7%8E%B0/" rel="tag">漏洞复现</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%8F%E6%B4%9E%E6%89%AB%E6%8F%8F/" rel="tag">漏洞扫描</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%BB%E5%8A%A8%E5%AE%89%E5%85%A8/" rel="tag">移动安全</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%B6%E5%9C%BAwriteup/" rel="tag">靶场writeup</a></li></ul></div></section><section class="switch-part switch-part3"><div id="js-friends"><a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/TechCatsLab">TechCatsLab</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://yangchenglong11.github.io">YangChengLong</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://jsharkc.github.io">LiuJiaChang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://blog.yusank.space">YusanKurban</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.lizebang.top">Lizebang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/sunanxiang">SunAnXiang</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/DoubleWoodH">LinHao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://blog.littlechao.top">ShiChao</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/Txiaozhe">TangXiaoJi</a> <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://github.com/LLLeon">JiaChenHui</a></div></section><section class="switch-part switch-part4"><div id="js-aboutme">不悲不喜，不卑不亢，努力成为一个更好的程序猿！</div></section></div></div></header></div></div><div class="hide-left-col" title="隐藏侧栏"><i class="fa fa-angle-double-left"></i></div><div class="mid-col"><nav id="mobile-nav"><div class="overlay"><div class="slider-trigger"></div><h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">m01ly</a></h1></div><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/avatar.jpg"></a><hgroup><h1 class="header-author"><a href="/" title="回到主页">m01ly</a></h1></hgroup><p class="header-subtitle">人生在世，全靠命</p><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">简历</a></li><div class="clearfix"></div></ul></nav><nav class="header-nav"><ul class="social"><a class="fa GitHub" target="_blank" href="https://github.com/m01ly" title="GitHub"></a> <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a> <a class="fa 网易云音乐" target="_blank" href="https://music.163.com/" title="网易云音乐"></a></ul></nav></header></div><link class="menu-list" tags="标签" friends="友情链接" about="目标"></nav><div class="body-wrap"><article id="post-bigdata-datacollect1-userbehavior" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2020/11/23/bigdata-datacollect1-userbehavior/" class="article-date"><time class="published" datetime="2020-11-23T07:10:21.000Z" itemprop="datePublished">2020-11-23 发布</time> <time class="updated" datetime="2021-11-25T02:30:35.149Z" itemprop="dateUpdated">2021-11-25 更新</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 class="article-title" itemprop="name">大数据实践（一）数仓采集项目</h1></header><div class="article-info article-info-post"><div class="article-count"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a></div><div class="article-tag tagcloud"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%95%B0%E4%BB%93%E9%87%87%E9%9B%86%E9%A1%B9%E7%9B%AE/" rel="tag">数仓采集项目</a></li></ul></div><span class="post-count">总字数12.8k</span> <span class="post-count">预计阅读57分钟</span></div><div class="clearfix"></div></div><div class="article-entry" itemprop="articleBody"><h1 id="1-数仓概念"><a href="#1-数仓概念" class="headerlink" title="1 数仓概念"></a>1 数仓概念</h1><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637651564709.png" alt="1637651564709"></p><h1 id="2-项目需求及架构设计"><a href="#2-项目需求及架构设计" class="headerlink" title="2 项目需求及架构设计"></a>2 项目需求及架构设计</h1><h2 id="2-1-项目需求分析"><a href="#2-1-项目需求分析" class="headerlink" title="2.1 项目需求分析"></a>2.1 项目需求分析</h2><p>1）用户行为数据采集平台搭建（存在日志服务器-用flume采集到数据仓库（hdfs）中）<br>2）业务数据采集平台搭建（存在mysql,oracle）：通过sqoop采集到数据仓库（hdfs）中<br>3）数据仓库维度建模：建库建表<br>4）分析，设备 会员 商品 地区 活动等电商核心主题，统计的报表指标近100个。<br>5）采用即席查询工具，随时进行指标分析<br>6）对集群性能进行监控，发送异常需要报警<br>7）元数据管理：管理Hive的元数据(数仓的核心就是hive)<br>8）质量监控：数仓分析的质量 有没有丢数据等</p><h2 id="2-2-项目框架"><a href="#2-2-项目框架" class="headerlink" title="2.2 项目框架"></a>2.2 项目框架</h2><h3 id="2-2-1-技术选型"><a href="#2-2-1-技术选型" class="headerlink" title="2.2.1 技术选型"></a>2.2.1 技术选型</h3><p>技术选型主要考虑因素：数据量大小，业务需求，行业内经验，技术成熟度、开发维护成本、总成本预算。下面加粗的是本次选择的。</p><ul><li>数据采集传输：<strong>Flume</strong>,Logstash(elk系列),<strong>Kafka</strong>,<strong>Sqoop</strong>（开源的）,DataX（阿里技术，比Sqoop强大）</li><li>数据存储：<strong>Mysq</strong>,<strong>HDFS</strong>,HBase,Redis,MongoDB</li><li>数据计算：<strong>Hive</strong>,Tez,<strong>Spark</strong>,Flink,Storm(Hive底层用Tex或者Spark)，Storm较早 不用</li><li>数据查询：<strong>Presto</strong>,<strong>Kylin</strong>（Apache）,Impala,Druid</li><li>数据可视化：Echarts（百度的，现在已经是Apache的了）,<strong>Superset</strong>（开源）,QuickBI,DataV（后两个都是阿里）</li><li>任务调度：<strong>Azkaban</strong>(Apache的，可视化界面),Oozie（CDH生态，HUE，功能比azkaban强大）</li><li>集群监控：<strong>Zabbix</strong></li><li>元数据管理：<strong>Atlas</strong></li></ul><h3 id="2-2-2-系统数据流程设计"><a href="#2-2-2-系统数据流程设计" class="headerlink" title="2.2.2 系统数据流程设计"></a>2.2.2 系统数据流程设计</h3><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637652372079.png" alt="1637652372079"></p><p>多个Flume往HDFS中写数据，会有压力，因此中间部署Kafka，来缓冲。</p><p>Hive通过写sql对数据进行处理，主要有5层处理，分别是ods,dwd,dws,dwt,ads。</p><p>对整个仓库的任务：利用什么时候导入数据等用Azkaban来调度。</p><h3 id="2-2-3-框架版本选型"><a href="#2-2-3-框架版本选型" class="headerlink" title="2.2.3 框架版本选型"></a>2.2.3 框架版本选型</h3><p>1）如何选择Apache/CDH/HDP版本？</p><p>Apache：运维麻烦；组件间兼容性需要自己调研（一般大厂使用，技术实力雄厚，有专业的运维人员）</p><p>CDH：国内使用最多的版本，但是CM不开源。2021年开始收费，一个节点1万美金</p><p>HDP：开源，可以进行二次开发，但是没有CDH稳定，国内使用较少</p><p>目前CDH和HDP已经合并了。</p><p>2）框架版本参考</p><table><thead><tr><th>产品</th><th>版本</th></tr></thead><tbody><tr><td>Hadoop</td><td>3.1.3</td></tr><tr><td>Flume</td><td>1.9.0</td></tr><tr><td>Kafka</td><td>2.4.1</td></tr><tr><td>Hive</td><td>3.1.2</td></tr><tr><td>Sqoop</td><td>1.4.6</td></tr><tr><td>Java</td><td>1.8</td></tr><tr><td>Zookeeper</td><td>3.5.7</td></tr><tr><td>Presto</td><td>0.189</td></tr></tbody></table><h3 id="2-2-4-服务器选型"><a href="#2-2-4-服务器选型" class="headerlink" title="2.2.4   服务器选型"></a>2.2.4 服务器选型</h3><p>服务器选择物理机还是云主机？</p><p>1）物理机：</p><p>以128G内存，20核物理CPU，40线程，戴尔品牌单台报价4W出头，一般物理机寿命5年左右。</p><p>需要有专业的运维任意，平均一个月1万。电费也是不少的开销。</p><p>2）云主机：</p><p>以阿里云为例，差不多配置，每年5W。运维工作都有阿里云完成，运维相对较轻松。</p><p>3）企业选择</p><p>金融有限公司和阿里没有直接冲突的公司选择阿里云。</p><p>中小公司，为了融资上市，选择阿里云，拉到融资后买物理机。</p><p>有长期打算，资金比较足，选择物理机。</p><h3 id="2-2-5-集群资源规划设计"><a href="#2-2-5-集群资源规划设计" class="headerlink" title="2.2.5 集群资源规划设计"></a>2.2.5 集群资源规划设计</h3><p>1）如何确认集群规模？（假设服务器8T磁盘，128G内存）</p><p>（1）每天日活跃用户100万，每人一天平均100条：100万*100条=1亿条</p><p>（2）每条日志1k左右，每天1亿条：100000000/1024/1024=约100G</p><p>（3）半年内不扩容服务器来算：100G*180天=约18T</p><p>（4）保留3副本：18T*3=54T</p><p>（5）保留20%-30%buf=54T/0.7=77T</p><p>（6）算到这里：约8T*10台服务器</p><p>2）测试集群服务器规划</p><table><thead><tr><th>服务名称</th><th>子服务</th><th>服务器 hadoop102</th><th>服务器 hadoop103</th><th>服务器 hadoop104</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td>√</td><td></td><td></td></tr><tr><td></td><td>DataNode</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>SecondaryNameNode</td><td></td><td></td><td>√</td></tr><tr><td>Yarn</td><td>NodeManager</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>Resourcemanager</td><td></td><td>√</td><td></td></tr><tr><td>Zookeeper</td><td>Zookeeper Server</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Flume(采集日志)</td><td>Flume</td><td>√</td><td>√</td><td></td></tr><tr><td>Kafka</td><td>Kafka</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Flume（消费Kafka）</td><td>Flume</td><td></td><td></td><td>√</td></tr><tr><td>Hive</td><td>Hive</td><td>√</td><td></td><td></td></tr><tr><td>MySQL</td><td>MySQL</td><td>√</td><td></td><td></td></tr><tr><td>Sqoop</td><td>Sqoop</td><td>√</td><td></td><td></td></tr><tr><td>Presto</td><td>Coordinator</td><td>√</td><td></td><td></td></tr><tr><td></td><td>Worker</td><td></td><td>√</td><td>√</td></tr><tr><td>Azkaban</td><td>AzkabanWebServer</td><td>√</td><td></td><td></td></tr><tr><td></td><td>AzkabanExecutorServer</td><td>√</td><td></td><td></td></tr><tr><td>Druid</td><td>Druid</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Kylin</td><td></td><td>√</td><td></td><td></td></tr><tr><td>Hbase</td><td>HMaster</td><td>√</td><td></td><td></td></tr><tr><td></td><td>HRegionServer</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Superset</td><td></td><td>√</td><td></td><td></td></tr><tr><td>Atlas</td><td></td><td>√</td><td></td><td></td></tr><tr><td>Solr</td><td>Jar</td><td>√</td><td></td><td></td></tr><tr><td>服务数总计</td><td></td><td>18</td><td>9</td><td>9</td></tr></tbody></table><h1 id="3-数据生成模块"><a href="#3-数据生成模块" class="headerlink" title="3 数据生成模块"></a>3 数据生成模块</h1><h2 id="3-1-目标数据"><a href="#3-1-目标数据" class="headerlink" title="3.1 目标数据"></a>3.1 目标数据</h2><p>我们要收集和分析的数据主要包括页面数据、事件数据、曝光数据、启动数据和错误数据。</p><h3 id="3-1-1-页面"><a href="#3-1-1-页面" class="headerlink" title="3.1.1 页面"></a>3.1.1 页面</h3><p>页面数据主要记录一个页面的用户访问情况，包括访问时间、停留时间、页面路径等信息。</p><p>​</p><p>1）所有页面id如下</p><pre class="line-numbers language-bash"><code class="language-bash">home<span class="token punctuation">(</span><span class="token string">"首页"</span><span class="token punctuation">)</span>,
category<span class="token punctuation">(</span><span class="token string">"分类页"</span><span class="token punctuation">)</span>,
discovery<span class="token punctuation">(</span><span class="token string">"发现页"</span><span class="token punctuation">)</span>,
top_n<span class="token punctuation">(</span><span class="token string">"热门排行"</span><span class="token punctuation">)</span>,
favor<span class="token punctuation">(</span><span class="token string">"收藏页"</span><span class="token punctuation">)</span>,
search<span class="token punctuation">(</span><span class="token string">"搜索页"</span><span class="token punctuation">)</span>,
good_list<span class="token punctuation">(</span><span class="token string">"商品列表页"</span><span class="token punctuation">)</span>,
good_detail<span class="token punctuation">(</span><span class="token string">"商品详情"</span><span class="token punctuation">)</span>,
good_spec<span class="token punctuation">(</span><span class="token string">"商品规格"</span><span class="token punctuation">)</span>,
comment<span class="token punctuation">(</span><span class="token string">"评价"</span><span class="token punctuation">)</span>,
comment_done<span class="token punctuation">(</span><span class="token string">"评价完成"</span><span class="token punctuation">)</span>,
comment_list<span class="token punctuation">(</span><span class="token string">"评价列表"</span><span class="token punctuation">)</span>,
cart<span class="token punctuation">(</span><span class="token string">"购物车"</span><span class="token punctuation">)</span>,
trade<span class="token punctuation">(</span><span class="token string">"下单结算"</span><span class="token punctuation">)</span>,
payment<span class="token punctuation">(</span><span class="token string">"支付页面"</span><span class="token punctuation">)</span>,
payment_done<span class="token punctuation">(</span><span class="token string">"支付完成"</span><span class="token punctuation">)</span>,
orders_all<span class="token punctuation">(</span><span class="token string">"全部订单"</span><span class="token punctuation">)</span>,
orders_unpaid<span class="token punctuation">(</span><span class="token string">"订单待支付"</span><span class="token punctuation">)</span>,
orders_undelivered<span class="token punctuation">(</span><span class="token string">"订单待发货"</span><span class="token punctuation">)</span>,
orders_unreceipted<span class="token punctuation">(</span><span class="token string">"订单待收货"</span><span class="token punctuation">)</span>,
orders_wait_comment<span class="token punctuation">(</span><span class="token string">"订单待评价"</span><span class="token punctuation">)</span>,
mine<span class="token punctuation">(</span><span class="token string">"我的"</span><span class="token punctuation">)</span>,
activity<span class="token punctuation">(</span><span class="token string">"活动"</span><span class="token punctuation">)</span>,
login<span class="token punctuation">(</span><span class="token string">"登录"</span><span class="token punctuation">)</span>,
register<span class="token punctuation">(</span><span class="token string">"注册"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）所有页面对象类型如下：</p><pre class="line-numbers language-bash"><code class="language-bash">sku_id<span class="token punctuation">(</span><span class="token string">"商品skuId"</span><span class="token punctuation">)</span>,
keyword<span class="token punctuation">(</span><span class="token string">"搜索关键词"</span><span class="token punctuation">)</span>,
sku_ids<span class="token punctuation">(</span><span class="token string">"多个商品skuId"</span><span class="token punctuation">)</span>,
activity_id<span class="token punctuation">(</span><span class="token string">"活动id"</span><span class="token punctuation">)</span>,
coupon_id<span class="token punctuation">(</span><span class="token string">"购物券id"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）所有来源类型如下：</p><pre class="line-numbers language-bash"><code class="language-bash">promotion<span class="token punctuation">(</span><span class="token string">"商品推广"</span><span class="token punctuation">)</span>,
recommend<span class="token punctuation">(</span><span class="token string">"算法推荐商品"</span><span class="token punctuation">)</span>,
query<span class="token punctuation">(</span><span class="token string">"查询结果商品"</span><span class="token punctuation">)</span>,
activity<span class="token punctuation">(</span><span class="token string">"促销活动"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-1-2-事件"><a href="#3-1-2-事件" class="headerlink" title="3.1.2 事件"></a>3.1.2 事件</h3><p>事件数据主要记录应用内一个具体操作行为，包括操作类型、操作对象、操作对象描述等信息。</p><p>1）所有动作类型如下：</p><pre class="line-numbers language-bash"><code class="language-bash">favor_add<span class="token punctuation">(</span><span class="token string">"添加收藏"</span><span class="token punctuation">)</span>,
favor_canel<span class="token punctuation">(</span><span class="token string">"取消收藏"</span><span class="token punctuation">)</span>,
cart_add<span class="token punctuation">(</span><span class="token string">"添加购物车"</span><span class="token punctuation">)</span>,
cart_remove<span class="token punctuation">(</span><span class="token string">"删除购物车"</span><span class="token punctuation">)</span>,
cart_add_num<span class="token punctuation">(</span><span class="token string">"增加购物车商品数量"</span><span class="token punctuation">)</span>,
cart_minus_num<span class="token punctuation">(</span><span class="token string">"减少购物车商品数量"</span><span class="token punctuation">)</span>,
trade_add_address<span class="token punctuation">(</span><span class="token string">"增加收货地址"</span><span class="token punctuation">)</span>,
get_coupon<span class="token punctuation">(</span><span class="token string">"领取优惠券"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注：对于下单、支付等业务数据，可从业务数据库获取。</p><p>2）所有动作目标类型如下：</p><p>sku_id(“商品”),<br>coupon_id(“购物券”);</p><h3 id="3-1-3-曝光"><a href="#3-1-3-曝光" class="headerlink" title="3.1.3 曝光"></a>3.1.3 曝光</h3><p>曝光数据主要记录页面所曝光的内容，包括曝光对象，曝光类型等信息。</p><p>1）所有曝光类型如下：</p><pre class="line-numbers language-bash"><code class="language-bash">promotion<span class="token punctuation">(</span><span class="token string">"商品推广"</span><span class="token punctuation">)</span>,
recommend<span class="token punctuation">(</span><span class="token string">"算法推荐商品"</span><span class="token punctuation">)</span>,
query<span class="token punctuation">(</span><span class="token string">"查询结果商品"</span><span class="token punctuation">)</span>,
activity<span class="token punctuation">(</span><span class="token string">"促销活动"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>2）所有曝光对象类型如下：</p><pre class="line-numbers language-bash"><code class="language-bash">sku_id<span class="token punctuation">(</span><span class="token string">"商品skuId"</span><span class="token punctuation">)</span>,
activity_id<span class="token punctuation">(</span><span class="token string">"活动id"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="3-1-4-启动"><a href="#3-1-4-启动" class="headerlink" title="3.1.4 启动"></a>3.1.4 启动</h3><p>启动数据记录应用的启动信息。</p><p>1）所有启动入口类型如下：</p><pre class="line-numbers language-bash"><code class="language-bash">icon<span class="token punctuation">(</span><span class="token string">"图标"</span><span class="token punctuation">)</span>,
notification<span class="token punctuation">(</span><span class="token string">"通知"</span><span class="token punctuation">)</span>,
install<span class="token punctuation">(</span><span class="token string">"安装后启动"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="3-1-5-错误"><a href="#3-1-5-错误" class="headerlink" title="3.1.5 错误"></a>3.1.5 错误</h3><p>错误数据记录应用使用过程中的错误信息，包括错误编号及错误信息。</p><h2 id="3-2-数据埋点"><a href="#3-2-数据埋点" class="headerlink" title="3.2 数据埋点"></a>3.2 数据埋点</h2><h3 id="3-2-1-主流埋点方式（了解）"><a href="#3-2-1-主流埋点方式（了解）" class="headerlink" title="3.2.1 主流埋点方式（了解）"></a>3.2.1 主流埋点方式（了解）</h3><p>目前主流的埋点方式，有代码埋点（前端/后端）、可视化埋点、全埋点三种。</p><p>代码埋点是通过调用埋点SDK函数，在需要埋点的业务逻辑功能位置调用接口，上报埋点数据。例如，我们对页面中的某个按钮埋点后，当这个按钮被点击时，可以在这个按钮对应的 OnClick 函数里面调用SDK提供的数据发送接口，来发送数据。</p><p>可视化埋点只需要研发人员集成采集 SDK，不需要写埋点代码，业务人员就可以通过访问分析平台的“圈选”功能，来“圈”出需要对用户行为进行捕捉的控件，并对该事件进行命名。圈选完毕后，这些配置会同步到各个用户的终端上，由采集 SDK 按照圈选的配置自动进行用户行为数据的采集和发送。（三方埋点技术：神策大数据，GrowingIO）</p><p>全埋点是通过在产品中嵌入SDK，前端自动采集页面上的全部用户行为事件，上报埋点数据，相当于做了一个统一的埋点。然后再通过界面配置哪些数据需要在系统里面进行分析。</p><h3 id="3-2-2-埋点数据日志结构"><a href="#3-2-2-埋点数据日志结构" class="headerlink" title="3.2.2 埋点数据日志结构"></a>3.2.2 埋点数据日志结构</h3><p>我们的日志结构大致可分为两类，一是普通页面埋点日志，二是启动日志。</p><p>普通页面日志结构如下，每条日志包含了，当前页面的页面信息，所有事件（动作）、所有曝光信息以及错误信息。除此之外，还包含了一系列<strong>公共信息</strong>，包括设备信息，地理位置，应用信息等，即下边的<strong>common</strong>字段。</p><p>1）普通页面埋点日志格式</p><pre class="line-numbers language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>
  <span class="token property">"common"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>                  -- 公共信息
    <span class="token property">"ar"</span><span class="token operator">:</span> <span class="token string">"230000"</span><span class="token punctuation">,</span>              -- 地区编码
    <span class="token property">"ba"</span><span class="token operator">:</span> <span class="token string">"iPhone"</span><span class="token punctuation">,</span>              -- 手机品牌
    <span class="token property">"ch"</span><span class="token operator">:</span> <span class="token string">"Appstore"</span><span class="token punctuation">,</span>            -- 渠道
    <span class="token property">"is_new"</span><span class="token operator">:</span> <span class="token string">"1"</span><span class="token punctuation">,</span>--是否首日使用，首次使用的当日，该字段值为<span class="token number">1</span>，过了<span class="token number">24</span><span class="token operator">:</span><span class="token number">00</span>，该字段置为<span class="token number">0</span>。
    <span class="token property">"md"</span><span class="token operator">:</span> <span class="token string">"iPhone 8"</span><span class="token punctuation">,</span>            -- 手机型号
    <span class="token property">"mid"</span><span class="token operator">:</span> <span class="token string">"YXfhjAYH6As2z9Iq"</span><span class="token punctuation">,</span> -- 设备id
    <span class="token property">"os"</span><span class="token operator">:</span> <span class="token string">"iOS 13.2.9"</span><span class="token punctuation">,</span>          -- 操作系统
    <span class="token property">"uid"</span><span class="token operator">:</span> <span class="token string">"485"</span><span class="token punctuation">,</span>                 -- 会员id
    <span class="token property">"vc"</span><span class="token operator">:</span> <span class="token string">"v2.1.134"</span>             -- app版本号
  &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
<span class="token property">"actions"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                     --动作(事件<span class="token punctuation">)</span>  
    &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token property">"action_id"</span><span class="token operator">:</span> <span class="token string">"favor_add"</span><span class="token punctuation">,</span>   --动作id
      <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"3"</span><span class="token punctuation">,</span>                   --目标id
      <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>       --目标类型
      <span class="token property">"ts"</span><span class="token operator">:</span> <span class="token number">1585744376605</span>           --动作时间戳
    &amp;#<span class="token number">125</span><span class="token punctuation">;</span>
  <span class="token punctuation">]</span><span class="token punctuation">,</span>
  <span class="token property">"displays"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
    &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token property">"displayType"</span><span class="token operator">:</span> <span class="token string">"query"</span><span class="token punctuation">,</span>        -- 曝光类型
      <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"3"</span><span class="token punctuation">,</span>                     -- 曝光对象id
      <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>         -- 曝光对象类型
      <span class="token property">"order"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>                      --出现顺序
      <span class="token property">"pos_id"</span><span class="token operator">:</span> <span class="token number">2</span>                      --曝光位置
    &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
    &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token property">"displayType"</span><span class="token operator">:</span> <span class="token string">"promotion"</span><span class="token punctuation">,</span>
      <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"6"</span><span class="token punctuation">,</span>
      <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>
      <span class="token property">"order"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span> 
      <span class="token property">"pos_id"</span><span class="token operator">:</span> <span class="token number">1</span>
    &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
    &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token property">"displayType"</span><span class="token operator">:</span> <span class="token string">"promotion"</span><span class="token punctuation">,</span>
      <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"9"</span><span class="token punctuation">,</span>
      <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>
      <span class="token property">"order"</span><span class="token operator">:</span> <span class="token number">3</span><span class="token punctuation">,</span> 
      <span class="token property">"pos_id"</span><span class="token operator">:</span> <span class="token number">3</span>
    &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
    &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token property">"displayType"</span><span class="token operator">:</span> <span class="token string">"recommend"</span><span class="token punctuation">,</span>
      <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"6"</span><span class="token punctuation">,</span>
      <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>
      <span class="token property">"order"</span><span class="token operator">:</span> <span class="token number">4</span><span class="token punctuation">,</span> 
      <span class="token property">"pos_id"</span><span class="token operator">:</span> <span class="token number">2</span>
    &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
    &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
      <span class="token property">"displayType"</span><span class="token operator">:</span> <span class="token string">"query "</span><span class="token punctuation">,</span>
      <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"6"</span><span class="token punctuation">,</span>
      <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>
      <span class="token property">"order"</span><span class="token operator">:</span> <span class="token number">5</span><span class="token punctuation">,</span> 
      <span class="token property">"pos_id"</span><span class="token operator">:</span> <span class="token number">1</span>
    &amp;#<span class="token number">125</span><span class="token punctuation">;</span>
  <span class="token punctuation">]</span><span class="token punctuation">,</span>
  <span class="token property">"page"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>                       --页面信息
    <span class="token property">"during_time"</span><span class="token operator">:</span> <span class="token number">7648</span><span class="token punctuation">,</span>        -- 持续时间毫秒
    <span class="token property">"item"</span><span class="token operator">:</span> <span class="token string">"3"</span><span class="token punctuation">,</span>                  -- 目标id
    <span class="token property">"item_type"</span><span class="token operator">:</span> <span class="token string">"sku_id"</span><span class="token punctuation">,</span>      -- 目标类型
    <span class="token property">"last_page_id"</span><span class="token operator">:</span> <span class="token string">"login"</span><span class="token punctuation">,</span>    -- 上页类型
    <span class="token property">"page_id"</span><span class="token operator">:</span> <span class="token string">"good_detail"</span><span class="token punctuation">,</span>   -- 页面ID
    <span class="token property">"sourceType"</span><span class="token operator">:</span> <span class="token string">"promotion"</span>   -- 来源类型
  &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
<span class="token property">"err"</span><span class="token operator">:</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span>                     --错误
<span class="token property">"error_code"</span><span class="token operator">:</span> <span class="token string">"1234"</span><span class="token punctuation">,</span>      --错误码
    <span class="token property">"msg"</span><span class="token operator">:</span> <span class="token string">"***********"</span>       --错误信息
&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  <span class="token property">"ts"</span><span class="token operator">:</span> <span class="token number">1585744374423</span>  --跳入时间戳
&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）启动日志格式</p><p>启动日志结构相对简单，主要包含公共信息，启动信息和错误信息。</p><pre class="line-numbers language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>
  <span class="token property">"common"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token property">"ar"</span><span class="token operator">:</span> <span class="token string">"370000"</span><span class="token punctuation">,</span>
    <span class="token property">"ba"</span><span class="token operator">:</span> <span class="token string">"Honor"</span><span class="token punctuation">,</span>
    <span class="token property">"ch"</span><span class="token operator">:</span> <span class="token string">"wandoujia"</span><span class="token punctuation">,</span>
    <span class="token property">"is_new"</span><span class="token operator">:</span> <span class="token string">"1"</span><span class="token punctuation">,</span>
    <span class="token property">"md"</span><span class="token operator">:</span> <span class="token string">"Honor 20s"</span><span class="token punctuation">,</span>
    <span class="token property">"mid"</span><span class="token operator">:</span> <span class="token string">"eQF5boERMJFOujcp"</span><span class="token punctuation">,</span>
    <span class="token property">"os"</span><span class="token operator">:</span> <span class="token string">"Android 11.0"</span><span class="token punctuation">,</span>
    <span class="token property">"uid"</span><span class="token operator">:</span> <span class="token string">"76"</span><span class="token punctuation">,</span>
    <span class="token property">"vc"</span><span class="token operator">:</span> <span class="token string">"v2.1.134"</span>
  &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  <span class="token property">"start"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>   
    <span class="token property">"entry"</span><span class="token operator">:</span> <span class="token string">"icon"</span><span class="token punctuation">,</span>         --icon手机图标  notice 通知   install 安装后启动
    <span class="token property">"loading_time"</span><span class="token operator">:</span> <span class="token number">18803</span><span class="token punctuation">,</span>  --启动加载时间
    <span class="token property">"open_ad_id"</span><span class="token operator">:</span> <span class="token number">7</span><span class="token punctuation">,</span>        --广告页ID
    <span class="token property">"open_ad_ms"</span><span class="token operator">:</span> <span class="token number">3449</span><span class="token punctuation">,</span>    -- 广告总共播放时间
    <span class="token property">"open_ad_skip_ms"</span><span class="token operator">:</span> <span class="token number">1989</span>   --  用户跳过广告时点
  &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
<span class="token property">"err"</span><span class="token operator">:</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span>                     --错误
<span class="token property">"error_code"</span><span class="token operator">:</span> <span class="token string">"1234"</span><span class="token punctuation">,</span>      --错误码
    <span class="token property">"msg"</span><span class="token operator">:</span> <span class="token string">"***********"</span>       --错误信息
&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  <span class="token property">"ts"</span><span class="token operator">:</span> <span class="token number">1585744304000</span>
&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-2-3-埋点数据上报时机"><a href="#3-2-3-埋点数据上报时机" class="headerlink" title="3.2.3 埋点数据上报时机"></a>3.2.3 埋点数据上报时机</h3><p>埋点数据上报时机包括两种方式。</p><p>方式一，在离开该页面时，上传在这个页面产生的所有数据（页面、事件、曝光、错误等）。优点，批处理，减少了服务器接收数据压力。缺点，不是特别及时。</p><p>方式二，每个事件、动作、错误等，产生后，立即发送。优点，响应及时。缺点，对服务器接收数据压力比较大。</p><h2 id="3-3-服务器和JDK准备"><a href="#3-3-服务器和JDK准备" class="headerlink" title="3.3 服务器和JDK准备"></a>3.3 服务器和JDK准备</h2><h3 id="3-3-1-服务器准备"><a href="#3-3-1-服务器准备" class="headerlink" title="3.3.1 服务器准备"></a>3.3.1 服务器准备</h3><p>安装hadoop集群，分别安装hadoop102、hadoop103、hadoop104三台主机。</p><h3 id="3-3-2-阿里云服务器准备（可选）"><a href="#3-3-2-阿里云服务器准备（可选）" class="headerlink" title="3.3.2 阿里云服务器准备（可选）"></a>3.3.2 阿里云服务器准备（可选）</h3><h3 id="3-3-3-JDK准备"><a href="#3-3-3-JDK准备" class="headerlink" title="3.3.3 JDK准备"></a>3.3.3 JDK准备</h3><p>1）卸载现有JDK（3台节点）</p><pre><code>[molly@hadoop102 opt]# sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps
[molly@hadoop103 opt]# sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps
[molly@hadoop104 opt]# sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps</code></pre><p>2）用SecureCRT工具将JDK导入到hadoop102的/opt/software文件夹下面</p><p>3） “alt+p”进入sftp模式</p><p>4）选择jdk1.8拖入工具</p><p>5）在Linux系统下的opt目录中查看软件包是否导入成功</p><pre><code>[molly@hadoop102 software]# ls /opt/software/</code></pre><p>看到如下结果：</p><pre><code>jdk-8u212-linux-x64.tar.gz</code></pre><p>6）解压JDK到/opt/module目录下</p><pre><code>[molly@hadoop102 software]# tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/</code></pre><p>7）配置JDK环境变量</p><p>​ （1）新建/etc/profile.d/my_env.sh文件</p><pre><code>[molly@hadoop102 module]# sudo vim /etc/profile.d/my_env.sh</code></pre><p>添加如下内容，然后保存（:wq）退出</p><pre class="line-numbers language-sh"><code class="language-sh">#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>​ （2）让环境变量生效</p><pre><code>[molly@hadoop102 software]$ source /etc/profile.d/my_env.sh</code></pre><p>8）测试JDK是否安装成功</p><pre><code>[molly@hadoop102 module]# java -version
#如果能看到以下结果、则Java正常安装
java version &quot;1.8.0_212&quot;</code></pre><p>9）分发JDK</p><pre><code>[molly@hadoop102 module]$ xsync /opt/module/jdk1.8.0_212/</code></pre><p>10）分发环境变量配置文件</p><pre><code>[molly@hadoop102 module]$ sudo /home/molly/bin/xsync /etc/profile.d/my_env.sh</code></pre><p>11）分别在hadoop103、hadoop104上执行source</p><pre><code>[molly@hadoop103 module]$ source /etc/profile.d/my_env.sh
[molly@hadoop104 module]$ source /etc/profile.d/my_env.sh</code></pre><h3 id="3-3-4-环境变量配置说明"><a href="#3-3-4-环境变量配置说明" class="headerlink" title="3.3.4 环境变量配置说明"></a>3.3.4 环境变量配置说明</h3><p>Linux的环境变量可在多个文件中配置，如/etc/profile，/etc/profile.d/*.sh，<del>/.bashrc，</del>/.bash_profile等，下面说明上述几个文件之间的关系和区别。</p><p>bash的运行模式可分为login shell和non-login shell。</p><p>例如，我们通过终端，输入用户名、密码，登录系统之后，得到就是一个login shell，而当我们执行以下命令ssh hadoop103 command，在hadoop103执行command的就是一个non-login shell。</p><p>这两种shell的主要区别在于，它们启动时会加载不同的配置文件，login shell启动时会加载/etc/profile，<del>/.bash_profile，</del>/.bashrc，non-login shell启动时会加载~/.bashrc。</p><p>而在加载<del>/.bashrc（实际是</del>/.bashrc中加载的/etc/bashrc）或/etc/profile时，都会执行如下代码片段，</p><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637657318225.png" alt="1637657318225"></p><p>因此不管是login shell还是non-login shell，启动时都会加载/etc/profile.d/*.sh中的环境变量。</p><h2 id="3-4-模拟数据"><a href="#3-4-模拟数据" class="headerlink" title="3.4 模拟数据"></a>3.4 模拟数据</h2><h3 id="3-4-1-使用说明"><a href="#3-4-1-使用说明" class="headerlink" title="3.4.1 使用说明"></a>3.4.1 使用说明</h3><p>1）将application.yml、gmall2020-mock-log-2021-01-22.jar、path.json、logback.xml上传到hadoop102的/opt/module/applog目录下</p><p>（1）创建applog路径</p><pre><code>[molly@hadoop102 module]$ mkdir /opt/module/applog</code></pre><p>（2）上传文件</p><p>2）配置文件</p><p>（1）application.yml文件</p><p>可以根据需求生成对应日期的用户行为日志。</p><pre><code>[molly@hadoop102 applog]$ vim application.yml</code></pre><p>修改如下内容</p><pre class="line-numbers language-yml"><code class="language-yml"># 外部配置打开
# 外部配置打开
logging.config: "./logback.xml"
#业务日期
mock.date: "2020-06-14"

#模拟数据发送模式
#mock.type: "http"
#mock.type: "kafka"
mock.type: "log"

#http模式下，发送的地址
mock.url: "http://hdp1/applog"

#kafka模式下，发送的地址
mock:
  kafka-server: "hdp1:9092,hdp2:9092,hdp3:9092"
  kafka-topic: "ODS_BASE_LOG"

#启动次数
mock.startup.count: 200
#设备最大值
mock.max.mid: 500000
#会员最大值
mock.max.uid: 100
#商品最大值
mock.max.sku-id: 35
#页面平均访问时间
mock.page.during-time-ms: 20000
#错误概率 百分比
mock.error.rate: 3
#每条日志发送延迟 ms
mock.log.sleep: 10
#商品详情来源  用户查询，商品推广，智能推荐, 促销活动
mock.detail.source-type-rate: "40:25:15:20"
#领取购物券概率
mock.if_get_coupon_rate: 75
#购物券最大id
mock.max.coupon-id: 3
#搜索关键词  
mock.search.keyword: "图书,小米,iphone11,电视,口红,ps5,苹果手机,小米盒子"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）path.json，该文件用来配置访问路径</p><p>根据需求，可以灵活配置用户点击路径。</p><p>来到主页-搜索-上篇-下单-。。</p><pre class="line-numbers language-json"><code class="language-json"><span class="token punctuation">[</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span><span class="token punctuation">,</span><span class="token string">"good_list"</span><span class="token punctuation">,</span><span class="token string">"good_detail"</span><span class="token punctuation">,</span><span class="token string">"cart"</span><span class="token punctuation">,</span><span class="token string">"trade"</span><span class="token punctuation">,</span><span class="token string">"payment"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">20</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span><span class="token punctuation">,</span><span class="token string">"search"</span><span class="token punctuation">,</span><span class="token string">"good_list"</span><span class="token punctuation">,</span><span class="token string">"good_detail"</span><span class="token punctuation">,</span><span class="token string">"login"</span><span class="token punctuation">,</span><span class="token string">"good_detail"</span><span class="token punctuation">,</span><span class="token string">"cart"</span><span class="token punctuation">,</span><span class="token string">"trade"</span><span class="token punctuation">,</span><span class="token string">"payment"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">40</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span><span class="token punctuation">,</span><span class="token string">"mine"</span><span class="token punctuation">,</span><span class="token string">"orders_unpaid"</span><span class="token punctuation">,</span><span class="token string">"trade"</span><span class="token punctuation">,</span><span class="token string">"payment"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">10</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span><span class="token punctuation">,</span><span class="token string">"mine"</span><span class="token punctuation">,</span><span class="token string">"orders_unpaid"</span><span class="token punctuation">,</span><span class="token string">"good_detail"</span><span class="token punctuation">,</span><span class="token string">"good_spec"</span><span class="token punctuation">,</span><span class="token string">"comment"</span><span class="token punctuation">,</span><span class="token string">"trade"</span><span class="token punctuation">,</span><span class="token string">"payment"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">5</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span><span class="token punctuation">,</span><span class="token string">"mine"</span><span class="token punctuation">,</span><span class="token string">"orders_unpaid"</span><span class="token punctuation">,</span><span class="token string">"good_detail"</span><span class="token punctuation">,</span><span class="token string">"good_spec"</span><span class="token punctuation">,</span><span class="token string">"comment"</span><span class="token punctuation">,</span><span class="token string">"home"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">5</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span><span class="token punctuation">,</span><span class="token string">"good_detail"</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">10</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>
  &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"path"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"home"</span>  <span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token property">"rate"</span><span class="token operator">:</span><span class="token number">10</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）logback配置文件</p><p>可配置日志生成路径，修改内容如下</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>LOG_HOME<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>/opt/module/applog/log<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>console<span class="token punctuation">"</span></span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ch.qos.logback.core.ConsoleAppender<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>encoder</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pattern</span><span class="token punctuation">></span></span>%msg%n<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pattern</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>encoder</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>appender</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>rollingFile<span class="token punctuation">"</span></span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ch.qos.logback.core.rolling.RollingFileAppender<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>rollingPolicy</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ch.qos.logback.core.rolling.TimeBasedRollingPolicy<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>fileNamePattern</span><span class="token punctuation">></span></span>$<span class="token entity" title="&#123;">&amp;#123;</span>LOG_HOME<span class="token entity" title="&#125;">&amp;#125;</span>/app.%d<span class="token entity" title="&#123;">&amp;#123;</span>yyyy-MM-dd<span class="token entity" title="&#125;">&amp;#125;</span>.log<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>fileNamePattern</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>rollingPolicy</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>encoder</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pattern</span><span class="token punctuation">></span></span>%msg%n<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pattern</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>encoder</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>appender</span><span class="token punctuation">></span></span>

    <span class="token comment" spellcheck="true">&lt;!-- 将某一个包下日志单独打印日志 --></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>logger</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>com.atgugu.gmall2020.mock.log.util.LogUtil<span class="token punctuation">"</span></span>
            <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>INFO<span class="token punctuation">"</span></span> <span class="token attr-name">additivity</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>false<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>rollingFile<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>console<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>logger</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>root</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>error<span class="token punctuation">"</span></span>  <span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>console<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>root</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）生成日志</p><p>（1）进入到/opt/module/applog路径，执行以下命令</p><pre><code>[molly@hadoop102 applog]$ java -jar gmall2020-mock-log-2021-01-22.jar</code></pre><p>（2）在/opt/module/applog/log目录下查看生成日志</p><pre><code>[molly@hadoop102 log]$ ll</code></pre><h3 id="3-4-2-集群日志生成脚本"><a href="#3-4-2-集群日志生成脚本" class="headerlink" title="3.4.2 集群日志生成脚本"></a>3.4.2 集群日志生成脚本</h3><p>在生成日志的时候模拟多台服务器生产的日志。在hadoop102的/home/molly目录下创建bin目录，这样脚本可以在服务器的任何目录执行。</p><pre><code>[molly@hadoop102 ~]$ echo $PATH
/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/molly/.local/bin:/home/molly/bin</code></pre><p>​ 1）在/home/molly/bin目录下创建脚本lg.sh</p><pre><code>[molly@hadoop102 bin]$ vim lg.sh</code></pre><p>​ 2）在脚本中编写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#!/bin/bash
for i in hadoop102 hadoop103; do
    echo "========== $i =========="
    ssh $i "cd /opt/module/applog/; java -jar gmall2020-mock-log-2021-01-22.jar >/dev/null 2>&1 &"
done <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注：</p><p>（1）/opt/module/applog/为jar包及配置文件所在路径</p><p>（2）/dev/null代表linux的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞”。</p><p>标准输入0：从键盘获得输入 /proc/self/fd/0</p><p>标准输出1：输出到屏幕（即控制台） /proc/self/fd/1</p><p>错误输出2：输出到屏幕（即控制台） /proc/self/fd/2</p><p>3）修改脚本执行权限</p><pre><code>[molly@hadoop102 bin]$ chmod u+x lg.sh</code></pre><p>4）将jar包及配置文件s上传至hadoop103的/opt/module/applog/路径</p><p>5）启动脚本</p><pre><code>[molly@hadoop102 module]$ lg.sh </code></pre><p>6）分别在hadoop102、hadoop103的/opt/module/applog/log目录上查看生成的数据</p><pre><code>[molly@hadoop102 logs]$ ls
app.2020-06-14.log
[molly@hadoop103 logs]$ ls
app.2020-06-14.log</code></pre><h1 id="4-数据采集模块"><a href="#4-数据采集模块" class="headerlink" title="4 数据采集模块"></a>4 数据采集模块</h1><h2 id="4-1-集群所有进程查看脚本"><a href="#4-1-集群所有进程查看脚本" class="headerlink" title="4.1 集群所有进程查看脚本"></a>4.1 集群所有进程查看脚本</h2><p>1）在/home/molly/bin目录下创建脚本xcall.sh</p><pre><code>[molly@hadoop102 bin]$ vim xcall.sh</code></pre><p>2）在脚本中编写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#! /bin/bash

for i in hadoop102 hadoop103 hadoop104
do
    echo --------- $i ----------
    ssh $i "$*"
done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）修改脚本执行权限</p><pre><code>[molly@hadoop102 bin]$ chmod 777 xcall.sh</code></pre><p>4）启动脚本</p><pre><code>[molly@hadoop102 bin]$ xcall.sh jps</code></pre><h2 id="4-2-Hadoop安装"><a href="#4-2-Hadoop安装" class="headerlink" title="4.2 Hadoop安装"></a>4.2 Hadoop安装</h2><p>详见：<a href="https://m01ly.github.io/2020/11/12/bigdata-hdfs1/">Hadoop 教程（二）安装hadoop集群-完全分布式部署</a></p><p>1）集群规划：</p><table><thead><tr><th></th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode DataNode</td><td>DataNode</td><td>DataNode SecondaryNameNode</td></tr><tr><td>Yarn</td><td>NodeManager</td><td>Resourcemanager NodeManager</td><td>NodeManager</td></tr></tbody></table><p>注意：尽量使用离线方式安装。第一次启动需要格式化namenode.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>hdfs namenode -format<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-2-1-项目经验之HDFS存储多目录"><a href="#4-2-1-项目经验之HDFS存储多目录" class="headerlink" title="4.2.1 项目经验之HDFS存储多目录"></a>4.2.1 项目经验之HDFS存储多目录</h3><p>datanode和namenode都可以多目录存储。namenode不同目录的数据都是一样的，所以这里我们不配namenode的多目录。但是datanode的多目录中每个目录存储的数据是不一样的，可以多目录（磁盘挂载到的对应目录。因此用不同磁盘来存储Datanode，实现方式就是：datanode配置多目录）</p><p>1）生产环境服务器磁盘情况</p><p>从当前服务器可以看到：4个磁盘挂载在不同的目录。</p><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637661734977.png" alt="1637661734977"></p><p>2）在hdfs-site.xml文件中配置多目录，注意新挂载磁盘的访问权限问题。</p><p>HDFS的DataNode节点保存数据的路径由dfs.datanode.data.dir参数决定，其默认值为file://${hadoop.tmp.dir}/dfs/data，若服务器有多个磁盘，必须对该参数进行修改。如服务器磁盘如上图所示，则该参数应修改为如下的值。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.datanode.data.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>注意：每台服务器挂载的磁盘不一样，所以每个节点的多目录配置可以不一致。单独配置即可。</p><h3 id="4-2-2-集群数据均衡"><a href="#4-2-2-集群数据均衡" class="headerlink" title="4.2.2 集群数据均衡"></a>4.2.2 集群数据均衡</h3><p><strong>1）节点间数据均衡</strong>:就是102 103 104之间的</p><p>下面由于人工操作会导致三台节点的存储数据不均衡，如下图所示：<img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637662122810.png" alt="1637662122810"></p><p>解决办法：开启数据均衡命令：</p><pre class="line-numbers language-bash"><code class="language-bash">start-balancer.sh -threshold 10<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。</p><p>停止数据均衡命令：</p><pre class="line-numbers language-bash"><code class="language-bash">stop-balancer.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>2）磁盘间数据均衡</strong></p><p>hadoop3.x的新特性：针对磁盘间的数据均衡</p><p>（1）生成均衡计划（我们只有一块磁盘，不会生成计划）</p><pre class="line-numbers language-bash"><code class="language-bash">hdfs diskbalancer -plan hadoop103<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）执行均衡计划</p><pre class="line-numbers language-bash"><code class="language-bash">hdfs diskbalancer -execute hadoop103.plan.json<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）查看当前均衡任务的执行情况</p><pre class="line-numbers language-bash"><code class="language-bash">hdfs diskbalancer -query hadoop103<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）取消均衡任务</p><pre class="line-numbers language-bash"><code class="language-bash">hdfs diskbalancer -cancel hadoop103.plan.json<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-2-3-项目经验之支持LZO压缩配置"><a href="#4-2-3-项目经验之支持LZO压缩配置" class="headerlink" title="4.2.3 项目经验之支持LZO压缩配置"></a>4.2.3 项目经验之支持LZO压缩配置</h3><p>1）hadoop本身并不支持lzo压缩，故需要使用twitter提供的<a target="_blank" rel="noopener" href="https://github.com/twitter/hadoop-lzo">hadoop-lzo</a>开源组件。hadoop-lzo需依赖hadoop和lzo进行编译。</p><p>2）将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-3.1.3/share/hadoop/common/</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 common<span class="token punctuation">]</span>$ <span class="token function">pwd</span>
/opt/module/hadoop-3.1.3/share/hadoop/common
<span class="token punctuation">[</span>molly@hadoop102 common<span class="token punctuation">]</span>$ <span class="token function">ls</span>
hadoop-lzo-0.4.20.jar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>3）同步hadoop-lzo-0.4.20.jar到hadoop103、hadoop104</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 common<span class="token punctuation">]</span>$ xsync hadoop-lzo-0.4.20.jar<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）core-site.xml增加配置支持LZO压缩</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codecs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>
​      org.apache.hadoop.io.compress.GzipCodec,
​      org.apache.hadoop.io.compress.DefaultCodec,
​      org.apache.hadoop.io.compress.BZip2Codec,
​      org.apache.hadoop.io.compress.SnappyCodec,
​      com.hadoop.compression.lzo.LzoCodec,
​      com.hadoop.compression.lzo.LzopCodec
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>io.compression.codec.lzo.class<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
​    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>com.hadoop.compression.lzo.LzoCodec<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5）同步core-site.xml到hadoop103、hadoop104</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop<span class="token punctuation">]</span>$ xsync core-site.xml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>6）启动及查看集群</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-dfs.sh
<span class="token punctuation">[</span>molly@hadoop103 hadoop-3.1.3<span class="token punctuation">]</span>$ sbin/start-yarn.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="4-2-4-测试LZO"><a href="#4-2-4-测试LZO" class="headerlink" title="4.2.4 测试LZO"></a>4.2.4 测试LZO</h3><p>（1）执行wordcount程序</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class<span class="token operator">=</span>com.hadoop.mapreduce.LzoTextInputFormat /input /lzo-output<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637662603236.png" alt="1637662603236"></p><h3 id="4-2-4-项目经验之LZO创建索引"><a href="#4-2-4-项目经验之LZO创建索引" class="headerlink" title="4.2.4 项目经验之LZO创建索引"></a>4.2.4 项目经验之LZO创建索引</h3><p>1）创建LZO文件的索引，LZO压缩文件的可切片特性依赖于其索引，故我们需要手动为LZO压缩文件创建索引。若无索引，则LZO文件的切片只有一个。</p><pre class="line-numbers language-bash"><code class="language-bash">hadoop jar /path/to/your/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer big_file.lzo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）测试</p><p>​ （1）将bigtable.lzo（200M）上传到集群的根目录</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ hadoop fs -mkdir /input
<span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ hadoop fs -put bigtable.lzo /input<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637662802344.png" alt="1637662802344"></p><p>（2）执行wordcount程序</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class<span class="token operator">=</span>com.hadoop.mapreduce.LzoTextInputFormat /input /output1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>允许发现切片只有一个，因此想起原理，Lzo切片是依赖于索引的，因此我们需要建索引</p><p>（3）对上传的LZO文件建索引</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /input/bigtable.lzo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637662962636.png" alt="1637662962636"></p><p>（4）再次执行WordCount程序</p><p>这里注意需要指定inputformat类为LzoText对应的类com.hadoop.mapreduce.LzoTextInputFormat</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class<span class="token operator">=</span>com.hadoop.mapreduce.LzoTextInputFormat /input /output2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>发现切片数为2。查看历史服务器去查看具体切片信息。<img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637662988690.png" alt="1637662988690"></p><p>map执行过程中切片信息。<img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637663231181.png" alt="1637663231181"></p><p>3）注意：如果以上任务，在运行过程中报如下异常</p><pre class="line-numbers language-bash"><code class="language-bash">Container <span class="token punctuation">[</span>pid<span class="token operator">=</span>8468,containerID<span class="token operator">=</span>container_1594198338753_0001_01_000002<span class="token punctuation">]</span> is running 318740992B beyond the <span class="token string">'VIRTUAL'</span> memory limit. Current usage: 111.5 MB of 1 GB physical memory used<span class="token punctuation">;</span> 2.4 GB of 2.1 GB virtual memory used. Killing container.
Dump of the process-tree <span class="token keyword">for</span> container_1594198338753_0001_01_000002 <span class="token keyword">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>解决办法：在hadoop102的/opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml文件中增加如下配置，然后分发到hadoop103、hadoop104服务器上，并重新启动集群。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token comment" spellcheck="true">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.pmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>
<span class="token comment" spellcheck="true">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.vmem-check-enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-2-5-项目经验之基准测试"><a href="#4-2-5-项目经验之基准测试" class="headerlink" title="4.2.5 项目经验之基准测试"></a>4.2.5 项目经验之基准测试</h3><p><strong>1） 测试HDFS写性能</strong></p><p>​ 测试内容：向HDFS集群写10个128M的文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mapreduce<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 6 -fileSize 128MB

2020-04-16 13:41:24,724 INFO fs.TestDFSIO: ----- TestDFSIO ----- <span class="token keyword">:</span> <span class="token function">write</span>
2020-04-16 13:41:24,724 INFO fs.TestDFSIO:       Date <span class="token operator">&amp;</span> time: Thu Apr 16 13:41:24 CST 2020
2020-04-16 13:41:24,724 INFO fs.TestDFSIO:     Number of files: 10
2020-04-16 13:41:24,725 INFO fs.TestDFSIO: Total MBytes processed: 1280
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:    Throughput mb/sec: 8.88
2020-04-16 13:41:24,725 INFO fs.TestDFSIO: Average IO rate mb/sec: 8.96
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:  IO rate std deviation: 0.87
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:   Test <span class="token function">exec</span> <span class="token function">time</span> sec: 67.61<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）测试HDFS读性能</p><p>测试内容：读取HDFS集群10个128M的文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mapreduce<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB
2020-04-16 13:43:38,857 INFO fs.TestDFSIO: ----- TestDFSIO ----- <span class="token keyword">:</span> <span class="token function">read</span>
2020-04-16 13:43:38,858 INFO fs.TestDFSIO:   Date <span class="token operator">&amp;</span> time: Thu Apr 16 13:43:38 CST 2020
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:         Number of files: 10
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:  Total MBytes processed: 1280
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:       Throughput mb/sec: 85.54
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:  Average IO rate mb/sec: 100.21
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:   IO rate std deviation: 44.37
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:      Test <span class="token function">exec</span> <span class="token function">time</span> sec: 53.61<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）删除测试生成数据</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mapreduce<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）使用Sort程序评测MapReduce（要求性能特别好，普通性能不要去跑）</p><p>（1）使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mapreduce<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）执行Sort程序</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mapreduce<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar <span class="token function">sort</span> random-data sorted-data<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）验证数据是否真正排好序了</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 mapreduce<span class="token punctuation">]</span>$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-2-6-项目经验之Hadoop参数调优"><a href="#4-2-6-项目经验之Hadoop参数调优" class="headerlink" title="4.2.6 项目经验之Hadoop参数调优"></a>4.2.6 项目经验之Hadoop参数调优</h3><p><strong>1）HDFS参数调优hdfs-site.xml</strong></p><p><strong>NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。</strong></p><p>对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.handler.count<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>10<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>dfs.namenode.handler.count=<img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637663793390.png" alt="1637663793390"> ，比如集群规模为8台时，此参数设置为41。</p><p><strong>2）YARN参数调优yarn-site.xml</strong></p><p>（1）情景描述：总共7台机器，每天几亿条数据，数据源-&gt;Flume-&gt;Kafka-&gt;HDFS-&gt;Hive</p><p>面临问题：数据统计主要用HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启的JVM重用，而且IO没有阻塞，内存用了不到50%。但是还是跑的非常慢，而且数据量洪峰过来时，整个集群都会宕掉。基于这种情况有没有优化方案。</p><p>（2）解决办法：</p><p>内存利用率不够。这个一般是Yarn的2个配置造成的，单个任务可以申请的最大内存大小，和Hadoop单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。</p><p>（a）yarn.nodemanager.resource.memory-mb</p><p>表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。</p><p>（b）yarn.scheduler.maximum-allocation-mb</p><p>单个任务可申请的最多物理内存量，默认是8192（MB）。</p><h2 id="4-3-Zookeeper安装"><a href="#4-3-Zookeeper安装" class="headerlink" title="4.3 Zookeeper安装"></a>4.3 Zookeeper安装</h2><h3 id="4-3-1-安装ZK"><a href="#4-3-1-安装ZK" class="headerlink" title="4.3.1 安装ZK"></a>4.3.1 安装ZK</h3><p>详见：</p><p>集群规划</p><table><thead><tr><th></th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>Zookeeper</td><td>Zookeeper</td><td>Zookeeper</td><td>Zookeeper</td></tr></tbody></table><h3 id="4-3-2-ZK集群启动停止脚本"><a href="#4-3-2-ZK集群启动停止脚本" class="headerlink" title="4.3.2 ZK集群启动停止脚本"></a>4.3.2 ZK集群启动停止脚本</h3><p>1）在hadoop102的/home/molly/bin目录下创建脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ vim zk.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>#在脚本中编写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#!/bin/bash

case $1 in
"start")&#123;
    for i in hadoop102 hadoop103 hadoop104
    do
        echo ---------- zookeeper $i 启动 ------------
        ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
    done
&#125;;;
"stop")&#123;
    for i in hadoop102 hadoop103 hadoop104
    do
        echo ---------- zookeeper $i 停止 ------------    
        ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
    done
&#125;;;
"status")&#123;
    for i in hadoop102 hadoop103 hadoop104
    do
        echo ---------- zookeeper $i 状态 ------------    
        ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
    done
&#125;;;
esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）增加脚本执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> u+x zk.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）Zookeeper集群启动脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ zk.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）Zookeeper集群停止脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ zk.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="4-4-Kafka安装"><a href="#4-4-Kafka安装" class="headerlink" title="4.4 Kafka安装"></a>4.4 Kafka安装</h2><h3 id="4-4-1-Kafka集群安装"><a href="#4-4-1-Kafka集群安装" class="headerlink" title="4.4.1 Kafka集群安装"></a>4.4.1 Kafka集群安装</h3><p>详见：<a href="https://m01ly.github.io/2020/11/15/bigdata-kafka1-setup/">kafka学习笔记（一） kafka搭建</a></p><p>配置文件：</p><pre class="line-numbers language-bash"><code class="language-bash">log.dirs<span class="token operator">=</span>/opt/moudule/kafka 2.11-2.4.1/datas
zookeeper.connect<span class="token operator">=</span>hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>集群规划：</p><table><thead><tr><th></th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>Kafka</td><td>Kafka</td><td>Kafka</td><td>Kafka</td></tr></tbody></table><h3 id="4-4-2-Kafka集群启动停止脚本"><a href="#4-4-2-Kafka集群启动停止脚本" class="headerlink" title="4.4.2 Kafka集群启动停止脚本"></a>4.4.2 Kafka集群启动停止脚本</h3><p>1）在/home/molly/bin目录下创建脚本kf.sh</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ vim kf.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在脚本中填写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#! /bin/bash

case $1 in
"start")&#123;
    for i in hadoop102 hadoop103 hadoop104
    do
        echo " --------启动 $i Kafka-------"
        ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
    done
&#125;;;
"stop")&#123;
    for i in hadoop102 hadoop103 hadoop104
    do
        echo " --------停止 $i Kafka-------"
        ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop"
    done
&#125;;;
esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）增加脚本执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> u+x kf.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）kf集群启动脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ kf.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查看zookeeper信息<img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637664623705.png" alt="1637664623705"></p><p>4）kf集群停止脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ kf.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-4-3-Kafka常用命令"><a href="#4-4-3-Kafka常用命令" class="headerlink" title="4.4.3 Kafka常用命令"></a>4.4.3 Kafka常用命令</h3><p>1）查看Kafka Topic列表</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka --list
<span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list
<span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>2）创建Kafka Topic</p><p>进入到/opt/module/kafka/目录下创建日志主题</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka --create --replication-factor 1 --partitions 1 --topic topic_log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）删除Kafka Topic</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --delete --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka --topic topic_log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）Kafka生产消息</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-console-producer.sh \
--broker-list hadoop102:9092 --topic topic_log
\<span class="token operator">></span>hello world
\<span class="token operator">></span>molly molly<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>5）Kafka消费消息</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-console-consumer.sh \
--bootstrap-server hadoop102:9092 --from-beginning --topic topic_log<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>–from-beginning：会把主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。</p><p>6）查看Kafka Topic详情</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka \
--describe --topic topic_log<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="4-4-4-项目经验之Kafka压力测试"><a href="#4-4-4-项目经验之Kafka压力测试" class="headerlink" title="4.4.4 项目经验之Kafka压力测试"></a>4.4.4 项目经验之Kafka压力测试</h3><p><strong>1）Kafka压测</strong></p><p>用Kafka官方自带的脚本，对Kafka进行压测。Kafka压测时，可以查看到哪个地方出现了瓶颈（<strong>CPU，内存，网络IO</strong>）。一般都是网络IO达到瓶颈。</p><p>kafka-consumer-perf-test.sh<br>kafka-producer-perf-test.sh</p><p><strong>2）Kafka Producer压力测试</strong></p><p>（1）在/opt/module/kafka/bin目录下面有这两个文件。我们来测试一下</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-producer-perf-test.sh --topic <span class="token function">test</span> --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers<span class="token operator">=</span>hadoop102:9092,hadoop103:9092,hadoop104:9092<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>说明：</p><p>record-size是一条信息有多大，单位是字节。<br>num-records是总共发送多少条信息。<br>throughput 是每秒多少条信息，设成-1，表示不限流，可测出生产者最大吞吐量。</p><p><strong>（2）Kafka会打印下面的信息</strong></p><pre class="line-numbers language-bash"><code class="language-bash">100000 records sent, 95877.277085 records/sec <span class="token punctuation">(</span>9.14 MB/sec<span class="token punctuation">)</span>, 187.68 ms avg latency, 424.00 ms max latency, 155 ms 50th, 411 ms 95th, 423 ms 99th, 424 ms 99.9th.<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>参数解析：本例中一共写入10w条消息，吞吐量为9.14 MB/sec，每次写入的平均延迟为187.68毫秒，最大的延迟为424.00毫秒。</p><p><strong>3）Kafka Consumer压力测试</strong></p><p>Consumer的测试，如果这四个指标（IO，CPU，内存，网络）都不能改变，考虑增加分区数来提升性能。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 kafka<span class="token punctuation">]</span>$ bin/kafka-consumer-perf-test.sh --broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic <span class="token function">test</span> --fetch-size 10000 --messages 10000000 --threads 1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>参数说明：</p><p>–zookeeper 指定zookeeper的链接信息<br>–topic 指定topic的名称<br>–fetch-size 指定每次fetch的数据的大小<br>–messages 总共要消费的消息个数</p><p>测试结果说明：</p><pre class="line-numbers language-bash"><code class="language-bash">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
2019-02-19 20:29:07:566, 2019-02-19 20:29:12:170, 9.5368, 2.0714, 100010, 21722.4153<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>开始测试时间，测试结束数据，共消费数据9.5368MB，吞吐量2.0714MB/s，共消费100010条，平均每秒消费21722.4153条。</p><h3 id="4-4-5-项目经验之Kafka机器数量计算"><a href="#4-4-5-项目经验之Kafka机器数量计算" class="headerlink" title="4.4.5 项目经验之Kafka机器数量计算"></a>4.4.5 项目经验之Kafka机器数量计算</h3><p>Kafka机器数量（经验公式）=2x（峰值生产速度x副本数/100）+1<br>先拿到峰值生产速度，再根据设定的副本数，就能预估出需要部署Kafka的数量。<br>比如我们的峰值生产速度是50M/s。副本数为2。<br>Kafka机器数量=2<em>（50</em>2/100）+ 1=3台</p><h3 id="4-4-6-项目经验值Kafka分区数计算"><a href="#4-4-6-项目经验值Kafka分区数计算" class="headerlink" title="4.4.6 项目经验值Kafka分区数计算"></a>4.4.6 项目经验值Kafka分区数计算</h3><p>1）创建一个只有1个分区的topic</p><p>2）测试这个topic的producer吞吐量和consumer吞吐量。</p><p>3）假设他们的值分别是Tp和Tc，单位可以是MB/s。</p><p>4）然后假设总的目标吞吐量是Tt，那么分区数=Tt / min（Tp，Tc）</p><p>例如：producer吞吐量=20m/s；consumer吞吐量=50m/s，期望吞吐量100m/s；</p><p>分区数=100 / 20 =5分区</p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42641909/article/details/89294698">https://blog.csdn.net/weixin_42641909/article/details/89294698</a></p><p>分区数一般设置为：3-10个</p><h1 id="5-项目1-采集用户行为数据"><a href="#5-项目1-采集用户行为数据" class="headerlink" title="5 项目1 采集用户行为数据"></a>5 项目1 采集用户行为数据</h1><p>如下图所示，用户行为经过埋点进行收集然后存放到logserver上，这个时候利用Flume（第一层）从日志服务器logserver上采集数据送到kafka中，再通过一个flume（第二层）接收，最后存储到HDFS中。</p><p>再看flume架构：</p><p>第一层flume，我们source选择为taildirSource,channel选Kafka Channel（这里不需要sink，因为KafkaChanel直接将数据存到Kafka中了）。</p><p>第二层flume：我们source选择为KafkaSource,channel选fileChannel,sink选择HDFS Sink。</p><p>其中flume安装详见：<a href="https://m01ly.github.io/2020/11/15/bigdata-flume1-setup/">flume学习笔记（一） flume搭建</a></p><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637724868076.png" alt="1637724868076"></p><p><strong>日志采集Flume集群规划</strong>：</p><p>我们将第一层flume安装在hadoop102和hadoop103上，第二层flume安装在hadoop104.</p><table><thead><tr><th></th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>Flume(采集日志)</td><td>（第一层）Flume</td><td>（第一层） Flume</td><td>（第二层 Flum</td></tr></tbody></table><h2 id="5-1-第一层flume采集"><a href="#5-1-第一层flume采集" class="headerlink" title="5.1 第一层flume采集"></a>5.1 第一层flume采集</h2><h3 id="5-1-1-项目经验之Flume组件选型"><a href="#5-1-1-项目经验之Flume组件选型" class="headerlink" title="5.1.1 项目经验之Flume组件选型"></a>5.1.1 项目经验之Flume组件选型</h3><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637722930078.png" alt="1637722930078"></p><p>Flume直接读log日志的数据，log日志的格式是app.yyyy-mm-dd.log。注意其中logInterceptor主要对原始日志进行初步数据处理，删除空数据。</p><p><strong>1）Source</strong></p><p>（1）Taildir Source相比Exec Source、Spooling Directory Source的优势</p><p><strong>TailDir Source：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。</strong></p><p><strong>Exec Source</strong>可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失。</p><p><strong>Spooling Directory Source</strong>监控目录，支持断点续传。</p><p>（2）batchSize大小如何设置？</p><p>+答：Event 1K左右时，500-1000合适（默认为100）</p><p><strong>2）Channel</strong></p><p><strong>采用Kafka Channel，省去了Sink，提高了效率。KafkaChannel数据存储在Kafka里面，所以数据是存储在磁盘中。</strong></p><p>注意在Flume1.7以前，Kafka Channel很少有人使用，因为发现parseAsFlumeEvent这个配置起不了作用。也就是无论parseAsFlumeEvent配置为true还是false，都会转为Flume Event。这样的话，造成的结果是，会始终都把Flume的headers中的信息混合着内容一起写入Kafka的消息中，这显然不是我所需要的，我只是需要把内容写入即可。</p><h3 id="5-1-2-日志采集Flume配置"><a href="#5-1-2-日志采集Flume配置" class="headerlink" title="5.1.2 日志采集Flume配置"></a>5.1.2 日志采集Flume配置</h3><p>1）Flume的具体配置如下：</p><p>​ （1）在/opt/module/flume/conf目录下创建file-flume-kafka.conf文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ vim file-flume-kafka.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在文件配置如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#为各组件命名
a1.sources = r1
a1.channels = c1

#描述source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*
a1.sources.r1.positionFile = /opt/module/flume/taildir_position.json
a1.sources.r1.interceptors =  i1
a1.sources.r1.interceptors.i1.type = com.molly.flume.interceptor.ETLInterceptor$Builder
#描述channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092
a1.channels.c1.kafka.topic = topic_log
a1.channels.c1.parseAsFlumeEvent = false
#绑定source和channel以及sink和channel的关系
a1.sources.r1.channels = c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意：com.molly.flume.interceptor.ETLInterceptor是自定义的拦截器的全类名。需要根据用户自定义的拦截器做相应修改。</p><h3 id="5-1-3-Flume拦截器"><a href="#5-1-3-Flume拦截器" class="headerlink" title="5.1.3 Flume拦截器"></a>5.1.3 Flume拦截器</h3><p>在第一层flume中对原始数据进行清洗</p><p>1）创建Maven工程flume-interceptor</p><p>2）创建包名：com.molly.flume.interceptor</p><p>3）在pom.xml文件中添加如下配置</p><p>maven-compiler-plugin是打包插件。com.alibaba注意加<scope>compile</scope>，把该组件打到包中。</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.flume<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>flume-ng-core<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.9.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">></span></span>provided<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>

    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.alibaba<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>fastjson<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>1.2.62<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">></span></span>compile<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-compiler-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>2.3.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>source</span><span class="token punctuation">></span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>source</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>target</span><span class="token punctuation">></span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>target</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-assembly-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRefs</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>descriptorRef</span><span class="token punctuation">></span></span>jar-with-dependencies<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRef</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>descriptorRefs</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>make-assembly<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>
                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>single<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>
                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>
                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>
            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>
        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4）在com.molly.flume.interceptor包下创建JSONUtils类</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>molly<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>interceptor<span class="token punctuation">;</span>
<span class="token keyword">import</span> com<span class="token punctuation">.</span>alibaba<span class="token punctuation">.</span>fastjson<span class="token punctuation">.</span>JSON<span class="token punctuation">;</span>
<span class="token keyword">import</span> com<span class="token punctuation">.</span>alibaba<span class="token punctuation">.</span>fastjson<span class="token punctuation">.</span>JSONException<span class="token punctuation">;</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">JSONUtils</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">boolean</span> <span class="token function">isJSONValidate</span><span class="token punctuation">(</span>String log<span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token keyword">try</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            JSON<span class="token punctuation">.</span><span class="token function">parse</span><span class="token punctuation">(</span>log<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">JSONException</span> e<span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> <span class="token boolean">false</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5）在com.molly.flume.interceptor包下创建LogInterceptor类</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>molly<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>interceptor<span class="token punctuation">;</span>
<span class="token keyword">import</span> com<span class="token punctuation">.</span>alibaba<span class="token punctuation">.</span>fastjson<span class="token punctuation">.</span>JSON<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>Context<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>Event<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>interceptor<span class="token punctuation">.</span>Interceptor<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>nio<span class="token punctuation">.</span>charset<span class="token punctuation">.</span>StandardCharsets<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Iterator<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>List<span class="token punctuation">;</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">ETLInterceptor</span> <span class="token keyword">implements</span> <span class="token class-name">Interceptor</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">initialize</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> Event <span class="token function">intercept</span><span class="token punctuation">(</span>Event event<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span> body <span class="token operator">=</span> event<span class="token punctuation">.</span><span class="token function">getBody</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        String log <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">String</span><span class="token punctuation">(</span>body<span class="token punctuation">,</span> StandardCharsets<span class="token punctuation">.</span>UTF_8<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>JSONUtils<span class="token punctuation">.</span><span class="token function">isJSONValidate</span><span class="token punctuation">(</span>log<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> event<span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> null<span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> List<span class="token operator">&lt;</span>Event<span class="token operator">></span> <span class="token function">intercept</span><span class="token punctuation">(</span>List<span class="token operator">&lt;</span>Event<span class="token operator">></span> list<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        Iterator<span class="token operator">&lt;</span>Event<span class="token operator">></span> iterator <span class="token operator">=</span> list<span class="token punctuation">.</span><span class="token function">iterator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">while</span> <span class="token punctuation">(</span>iterator<span class="token punctuation">.</span><span class="token function">hasNext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            Event next <span class="token operator">=</span> iterator<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">intercept</span><span class="token punctuation">(</span>next<span class="token punctuation">)</span><span class="token operator">==</span>null<span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
                iterator<span class="token punctuation">.</span><span class="token function">remove</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> list<span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">Builder</span> <span class="token keyword">implements</span> <span class="token class-name">Interceptor<span class="token punctuation">.</span>Builder</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> Interceptor <span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">ETLInterceptor</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">configure</span><span class="token punctuation">(</span>Context context<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
       <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>6）打包</p><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637725114820.png" alt="1637725114820"></p><p>7）需要先将打好的包放入到hadoop102的/opt/module/flume/lib文件夹下面。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 lib<span class="token punctuation">]</span>$ <span class="token function">ls</span> <span class="token operator">|</span> <span class="token function">grep</span> interceptor
flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>8）分发Flume到hadoop103、hadoop104</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ xsync flume/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>9）hadoop102消费Flume数据</strong></p><p>为了查看第一次flume是否起作用，我们开启一个kafka消费端来消费kafkaChannel中的数据。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102~<span class="token punctuation">]</span>kafka-console-consumer.sh --topic topic_log --bootstrap-server hadoop102:9092<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>10）分别在hadoop102、hadoop103上启动Flume</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 flume<span class="token punctuation">]</span>$ bin/flume-ng agent --name a1 --conf-file conf/file-flume-kafka.conf <span class="token operator">&amp;</span>
<span class="token punctuation">[</span>molly@hadoop103 flume<span class="token punctuation">]</span>$ bin/flume-ng agent --name a1 --conf-file conf/file-flume-kafka.conf <span class="token operator">&amp;</span>
<span class="token punctuation">[</span>molly@hadoop102 flume<span class="token punctuation">]</span>$ bin/flume-ng agent --name a1 --conf-file conf/file-flume-kafka.conf  -n a1 -Dflume.root.logger<span class="token operator">=</span>INFO,console<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>11）观看9)中的kafka消费端有数据在消费。</p><h3 id="5-1-4-日志采集Flume启动停止脚本"><a href="#5-1-4-日志采集Flume启动停止脚本" class="headerlink" title="5.1.4 日志采集Flume启动停止脚本"></a>5.1.4 日志采集Flume启动停止脚本</h3><p>1）在/home/molly/bin目录下创建脚本f1.sh</p><p>[molly@hadoop102 bin]$ vim f1.sh</p><p>​ 在脚本中填写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#! /bin/bash

case $1 in
"start")&#123;
        for i in hadoop102 hadoop103
        do
                echo " --------启动 $i 采集flume-------"
                ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log1.txt 2>&1  &"
        done
&#125;;;    
"stop")&#123;
        for i in hadoop102 hadoop103
        do
                echo " --------停止 $i 采集flume-------"
                ssh $i "ps -ef | grep file-flume-kafka | grep -v grep |awk  '&#123;print \$2&#125;' | xargs -n1 kill -9 "
        done

&#125;;;
esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>说明1：nohup，该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思，不挂断地运行命令。</p><p>说明2：awk 默认分隔符为空格</p><p>说明3：xargs 表示取出前面命令运行的结果，作为后面命令的输入参数。</p><p>2）增加脚本执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> u+x f1.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）f1集群启动脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ f1.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）f1集群停止脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ f1.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>集群规划</p><table><thead><tr><th></th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>Flume（消费Kafka）</td><td></td><td></td><td>Flume</td></tr></tbody></table><h2 id="5-2-第二层flume采集"><a href="#5-2-第二层flume采集" class="headerlink" title="5.2 第二层flume采集"></a>5.2 第二层flume采集</h2><p>集群规划，第二层flume是消费Kafka数据的Flume，部署在hadoop104上。</p><table><thead><tr><th></th><th>服务器hadoop102</th><th>服务器hadoop103</th><th>服务器hadoop104</th></tr></thead><tbody><tr><td>Flume（消费Kafka）</td><td></td><td></td><td>Flume</td></tr></tbody></table><h3 id="5-2-1-项目经验之Flume组件选型"><a href="#5-2-1-项目经验之Flume组件选型" class="headerlink" title="5.2.1 项目经验之Flume组件选型"></a>5.2.1 项目经验之Flume组件选型</h3><p>第二次Flume主要作用是消费Kafka中的数据，然后存储到HDFS中，因此Source选择KafkaSource,sink选择HDFSsink。同时在source端使用一个拦截器：拦截器作用是获取日志中的实际时间。</p><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637738855416.png" alt="1637738855416"></p><p>1）FileChannel和MemoryChannel区别</p><p>MemoryChannel传输数据速度更快，但因为数据保存在JVM的堆内存中，Agent进程挂掉会导致数据丢失，适用于对数据质量要求不高的需求。</p><p>FileChannel传输速度相对于Memory慢，但数据安全保障高，Agent进程挂掉也可以从失败中恢复数据。</p><p>选型：</p><p>金融类公司、对钱要求非常准确的公司通常会选择FileChannel</p><p>传输的是普通日志信息（京东内部一天丢100万-200万条，这是非常正常的），通常选择MemoryChannel。</p><p><strong>2）FileChannel优化</strong></p><p>通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。</p><p>官方说明如下：</p><p>checkpointDir和backupCheckpointDir也尽量配置在不同硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据</p><p><strong>3）Sink：HDFS Sink</strong></p><p>（1）HDFS存入大量小文件，有什么影响？</p><p><strong>元数据层面：</strong>每个小文件都有一份元数据，其中包括文件路径，文件名，所有者，所属组，权限，创建时间等，这些信息都保存在Namenode内存中。所以小文件过多，会占用Namenode服务器大量内存，影响Namenode性能和使用寿命</p><p><strong>计算层面：</strong>默认情况下MR会对每个小文件启用一个Map任务计算，非常影响计算性能。同时也影响磁盘寻址时间。</p><p>（2）HDFS小文件处理</p><p>官方默认的这三个参数配置写入HDFS后会产生小文件，hdfs.rollInterval、hdfs.rollSize、hdfs.rollCount</p><p>基于以上hdfs.rollInterval=3600，hdfs.rollSize=134217728，hdfs.rollCount =0几个参数综合作用，效果如下：</p><p>（1）文件在达到128M时会滚动生成新文件</p><p>（2）文件创建超3600秒时会滚动生成新文件</p><h3 id="5-2-2-Flume拦截器"><a href="#5-2-2-Flume拦截器" class="headerlink" title="5.2.2 Flume拦截器"></a>5.2.2 Flume拦截器</h3><p>由于flume默认会用linux系统时间，作为输出到HDFS路径的时间。如果数据是23:59分产生的。Flume消费kafka里面的数据时，有可能已经是第二天了，那么这部门数据会被发往第二天的HDFS路径。我们希望的是根据日志里面的实际时间，发往HDFS的路径，<strong>所以下面拦截器作用是获取日志中的实际时间</strong>。</p><p>1）在com.molly.flume.interceptor包下创建TimeStampInterceptor类</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>molly<span class="token punctuation">.</span>interceptor<span class="token punctuation">;</span>

<span class="token keyword">import</span> com<span class="token punctuation">.</span>alibaba<span class="token punctuation">.</span>fastjson<span class="token punctuation">.</span>JSONObject<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>Context<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>Event<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>interceptor<span class="token punctuation">.</span>Interceptor<span class="token punctuation">;</span>

<span class="token keyword">import</span> java<span class="token punctuation">.</span>nio<span class="token punctuation">.</span>charset<span class="token punctuation">.</span>StandardCharsets<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>ArrayList<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>List<span class="token punctuation">;</span>
<span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Map<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">TimeStampInterceptor</span> <span class="token keyword">implements</span> <span class="token class-name">Interceptor</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token keyword">private</span> ArrayList<span class="token operator">&lt;</span>Event<span class="token operator">></span> events <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ArrayList</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">initialize</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> Event <span class="token function">intercept</span><span class="token punctuation">(</span>Event event<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

        Map<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> String<span class="token operator">></span> headers <span class="token operator">=</span> event<span class="token punctuation">.</span><span class="token function">getHeaders</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        String log <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">String</span><span class="token punctuation">(</span>event<span class="token punctuation">.</span><span class="token function">getBody</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> StandardCharsets<span class="token punctuation">.</span>UTF_8<span class="token punctuation">)</span><span class="token punctuation">;</span>

        JSONObject jsonObject <span class="token operator">=</span> JSONObject<span class="token punctuation">.</span><span class="token function">parseObject</span><span class="token punctuation">(</span>log<span class="token punctuation">)</span><span class="token punctuation">;</span>

        String ts <span class="token operator">=</span> jsonObject<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"ts"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        headers<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">"timestamp"</span><span class="token punctuation">,</span> ts<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token keyword">return</span> event<span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> List<span class="token operator">&lt;</span>Event<span class="token operator">></span> <span class="token function">intercept</span><span class="token punctuation">(</span>List<span class="token operator">&lt;</span>Event<span class="token operator">></span> list<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        events<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>Event event <span class="token operator">:</span> list<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            events<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span><span class="token function">intercept</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token keyword">return</span> events<span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>

    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">Builder</span> <span class="token keyword">implements</span> <span class="token class-name">Interceptor<span class="token punctuation">.</span>Builder</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> Interceptor <span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
            <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">TimeStampInterceptor</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>

        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">configure</span><span class="token punctuation">(</span>Context context<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>
        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>
<span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）重新打包</p><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637739135017.png" alt="1637739135017"></p><p>3）需要先将打好的包放入到hadoop102的/opt/module/flume/lib文件夹下面。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 lib<span class="token punctuation">]</span>$ <span class="token function">ls</span> <span class="token operator">|</span> <span class="token function">grep</span> interceptor
flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>4）分发Flume到hadoop103、hadoop104</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ xsync flume/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="5-2-3-日志消费Flume配置"><a href="#5-2-3-日志消费Flume配置" class="headerlink" title="5.2.3 日志消费Flume配置"></a>5.2.3 日志消费Flume配置</h3><p>1）Flume的具体配置如下：</p><p>​ （1）在hadoop104的/opt/module/flume/conf目录下创建kafka-flume-hdfs.conf文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop104 conf<span class="token punctuation">]</span>$ vim kafka-flume-hdfs.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在文件配置如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">## 组件
a1.sources=r1
a1.channels=c1
a1.sinks=k1

## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics=topic_log
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.molly.flume.interceptor.TimeStampInterceptor$Builder

## channel1
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1/
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 6
## sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log-
a1.sinks.k1.hdfs.round = false

a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0
## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop
## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="5-2-4-日志消费Flume启动停止脚本"><a href="#5-2-4-日志消费Flume启动停止脚本" class="headerlink" title="5.2.4 日志消费Flume启动停止脚本"></a>5.2.4 日志消费Flume启动停止脚本</h3><p>1）在/home/molly/bin目录下创建脚本f2.sh</p><p>[molly@hadoop102 bin]$ vim f2.sh</p><p>​ 在脚本中填写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#! /bin/bash

case $1 in
"start")&#123;
        for i in hadoop104
        do
                echo " --------启动 $i 消费flume-------"
                ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log2.txt   2>&1 &"
        done
&#125;;;
"stop")&#123;
        for i in hadoop104
        do
                echo " --------停止 $i 消费flume-------"
                ssh $i "ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '&#123;print \$2&#125;' | xargs -n1 kill"
        done

&#125;;;
esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）增加脚本执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> u+x f2.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）f2集群启动脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ f2.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）f2集群停止脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ f2.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="5-3-项目经验之Flume内存优化"><a href="#5-3-项目经验之Flume内存优化" class="headerlink" title="5.3 项目经验之Flume内存优化"></a>5.3 项目经验之Flume内存优化</h2><p>1）问题描述：如果启动消费Flume抛出如下异常</p><p>ERROR hdfs.HDFSEventSink: process failed</p><p>java.lang.OutOfMemoryError: GC overhead limit exceeded</p><p>2）解决方案步骤：</p><p>（1）在hadoop102服务器的/opt/module/flume/conf/flume-env.sh文件中增加如下配置</p><p>export JAVA_OPTS=”-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote”</p><p>（2）同步配置到hadoop103、hadoop104服务器</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 conf<span class="token punctuation">]</span>$ xsync flume-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）Flume内存参数设置及优化</p><p>JVM heap一般设置为4G或更高</p><p>-Xmx与-Xms最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导致频繁fullgc。</p><p>-Xms表示JVM Heap(堆内存)最小尺寸，初始分配；-Xmx 表示JVM Heap(堆内存)最大允许的尺寸，按需分配。如果不设置一致，容易在初始化时，由于内存不够，频繁触发fullgc。</p><h2 id="5-4-采集通道启动-停止脚本"><a href="#5-4-采集通道启动-停止脚本" class="headerlink" title="5.4 采集通道启动/停止脚本"></a>5.4 采集通道启动/停止脚本</h2><h3 id="5-4-1-数据通道测试"><a href="#5-4-1-数据通道测试" class="headerlink" title="5.4.1 数据通道测试"></a>5.4.1 数据通道测试</h3><p>根据需求分别生成2020-06-14和2020-06-15日期的数据</p><p>1）修改/opt/module/applog/application.yml中业务日期为2020-06-14</p><p>#业务日期</p><p>mock.date=2020-06-14</p><p>2）执行脚本，生成2020-06-14日志数据</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ lg.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）再次修改/opt/module/applog/application.yml中业务日期2020-06-15</p><p>#业务日期</p><p>mock.date=2020-06-15</p><p>4）执行脚本，生成2020-06-15日志数据</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 ~<span class="token punctuation">]</span>$ lg.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5）在这个期间，不断观察Hadoop的HDFS路径上是否有数据</p><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637740975327.png" alt="1637740975327"></p><h3 id="5-4-2-采集通道启动-停止脚本"><a href="#5-4-2-采集通道启动-停止脚本" class="headerlink" title="5.4.2 采集通道启动/停止脚本"></a>5.4.2 采集通道启动/停止脚本</h3><p>1）在/home/molly/bin目录下创建脚本cluster.sh</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ vim cluster.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在脚本中填写如下内容</p><pre class="line-numbers language-sh"><code class="language-sh">#!/bin/bash
case $1 in
"start")&#123;
        echo ================== 启动 集群 ==================
        #启动 Zookeeper集群
        zk.sh start
        #启动 Hadoop集群
        hdp.sh start
        #启动 Kafka采集集群
        kf.sh start
        #启动 Flume采集集群
        f1.sh start
        #启动 Flume消费集群
        f2.sh start
        &#125;;;
"stop")&#123;
        echo ================== 停止 集群 ==================

        #停止 Flume消费集群
        f2.sh stop
        #停止 Flume采集集群
        f1.sh stop
        #停止 Kafka采集集群
        kf.sh stop
        #停止 Hadoop集群
        hdp.sh stop
        #停止 Zookeeper集群
        zk.sh stop
&#125;;;
esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）增加脚本执行权限</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ <span class="token function">chmod</span> u+x cluster.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）cluster集群启动脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ cluster.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4）cluster集群停止脚本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 module<span class="token punctuation">]</span>$ cluster.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="6-项目2-采集业务数据"><a href="#6-项目2-采集业务数据" class="headerlink" title="6 项目2 采集业务数据"></a>6 项目2 采集业务数据</h1><h2 id="6-1-电商业务流程"><a href="#6-1-电商业务流程" class="headerlink" title="6.1 电商业务流程"></a>6.1 电商业务流程</h2><p>电商的业务流程可以以一个普通用户的浏览足迹为例进行说明，用户点开电商首页开始浏览，可能会通过分类查询也可能通过全文搜索寻找自己中意的商品，这些商品无疑都是存储在后台的管理系统中的。</p><p>当用户寻找到自己中意的商品，可能会想要购买，将商品添加到购物车后发现需要登录，登录后对商品进行结算，这时候购物车的管理和商品订单信息的生成都会对业务数据库产生影响，会生成相应的订单数据和支付数据。</p><p>订单正式生成之后，还会对订单进行跟踪处理，直到订单全部完成。</p><p>电商的主要业务流程包括用户前台浏览商品时的商品详情的管理，用户商品加入购物车进行支付时用户个人中心&amp;支付服务的管理，用户支付完成后订单后台服务的管理，这些流程涉及到了十几个甚至几十个业务数据表，甚至更多。</p><h2 id="6-2-电商常识"><a href="#6-2-电商常识" class="headerlink" title="6.2 电商常识"></a>6.2 电商常识</h2><h3 id="6-2-1-SKU和SPU"><a href="#6-2-1-SKU和SPU" class="headerlink" title="6.2.1 SKU和SPU"></a>6.2.1 SKU和SPU</h3><p>SKU=Stock Keeping Unit（库存量基本单位）。现在已经被引申为产品统一编号的简称，每种产品均对应有唯一的SKU号。</p><p>SPU（Standard Product Unit）：是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息集合。</p><p>例如：iPhoneX手机就是SPU。一台银色、128G内存的、支持联通网络的iPhoneX，就是SKU。</p><p>SPU表示一类商品。同一SPU的商品可以共用商品图片、海报、销售属性等。</p><h3 id="6-2-2-平台属性和销售属性"><a href="#6-2-2-平台属性和销售属性" class="headerlink" title="6.2.2 平台属性和销售属性"></a>6.2.2 平台属性和销售属性</h3><p><strong>1.平台属性</strong></p><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637745317255.png" alt="1637745317255"></p><p><strong>2.销售属性</strong></p><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637745332754.png" alt="1637745332754"></p><h2 id="6-3-业务数据采集架构"><a href="#6-3-业务数据采集架构" class="headerlink" title="6.3 业务数据采集架构"></a>6.3 业务数据采集架构</h2><p>从2.2.2可以看到整体架构；对于需求2 采集业务数据的结构图如下所示：</p><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637746045465.png" alt="1637746045465"></p><p>业务数据库是直接存储到mysql数据库的，我们可以通过sqoop组件使用JDBC将数据传输到HDFS上存储。</p><p>sqoop安装教程见：</p><p>Hive安装教程见：</p><h2 id="6-4-同步策略"><a href="#6-4-同步策略" class="headerlink" title="6.4 同步策略"></a>6.4 同步策略</h2><p>数据同步策略的类型包括：全量同步、增量同步、新增及变化同步、特殊情况</p><p>Ø 全量表：存储完整的数据。</p><p>Ø 增量表：存储新增加的数据。</p><p>Ø 新增及变化表：存储新增加的数据和变化的数据。</p><p>Ø 特殊表：只需要存储一次。</p><h3 id="6-4-1-全量同步策略"><a href="#6-4-1-全量同步策略" class="headerlink" title="6.4.1 全量同步策略"></a>6.4.1 全量同步策略</h3><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637746273106.png" alt="1637746273106"></p><h3 id="6-4-2-增量同步策略"><a href="#6-4-2-增量同步策略" class="headerlink" title="6.4.2 增量同步策略"></a>6.4.2 增量同步策略</h3><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637746298966.png" alt="1637746298966"></p><h3 id="6-4-3-新增及变化策略"><a href="#6-4-3-新增及变化策略" class="headerlink" title="6.4.3 新增及变化策略"></a>6.4.3 新增及变化策略</h3><p><img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637746311874.png" alt="1637746311874"></p><h3 id="6-4-4-特殊策略"><a href="#6-4-4-特殊策略" class="headerlink" title="6.4.4 特殊策略"></a>6.4.4 特殊策略</h3><p>某些特殊的表，可不必遵循上述同步策略。</p><p><strong>例如</strong>没变化的客观世界的数据（比如性别，地区，民族，政治成分，鞋子尺码）可以只存一份。</p><h2 id="6-5-业务数据导入HDFS"><a href="#6-5-业务数据导入HDFS" class="headerlink" title="6.5 业务数据导入HDFS"></a>6.5 业务数据导入HDFS</h2><h3 id="6-5-1-分析表同步策略"><a href="#6-5-1-分析表同步策略" class="headerlink" title="6.5.1 分析表同步策略"></a>6.5.1 分析表同步策略</h3><p>在生产环境，个别小公司，为了简单处理，所有表全量导入。</p><p>中大型公司，由于数据量比较大，还是严格按照同步策略导入数据。</p><p>​ <img src="/2020/11/23/bigdata-datacollect1-userbehavior/1637746489890.png" alt="1637746489890"></p><h3 id="6-5-2-业务数据首日同步脚本"><a href="#6-5-2-业务数据首日同步脚本" class="headerlink" title="6.5.2 业务数据首日同步脚本"></a>6.5.2 业务数据首日同步脚本</h3><p><strong>1）脚本编写</strong></p><p>（1）在/home/atguigu/bin目录下创建</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">[</span>molly@hadoop102 bin<span class="token punctuation">]</span>$ vim mysql_to_hdfs_init.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加如下内容：</p><pre class="line-numbers language-sh"><code class="language-sh">#! /bin/bash
APP=gmall
sqoop=/opt/module/sqoop/bin/sqoop
# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$2" ] ;then
   do_date=$2
else 
   echo "请传入日期参数"
   exit
fi 
import_data()&#123;
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/$APP \
--username root \
--password 123456 \
--target-dir /origin_data/$APP/db/$1/$do_date \
--delete-target-dir \
--query "$2 where \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \
--compression-codec lzop \
--null-string '\\N' \
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
&#125;
import_order_info()&#123;
  import_data order_info "select
                            id, 
                            total_amount, 
                            order_status, 
                            user_id, 
                            payment_way,
                            delivery_address,
                            out_trade_no, 
                            create_time, 
                            operate_time,
                            expire_time,
                            tracking_no,
                            province_id,
                            activity_reduce_amount,
                            coupon_reduce_amount,                            
                            original_total_amount,
                            feight_fee,
                            feight_fee_reduce      
                        from order_info"
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="6-5-3-项目经验"><a href="#6-5-3-项目经验" class="headerlink" title="6.5.3 项目经验"></a>6.5.3 项目经验</h3><p>Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用–input-null-string和–input-null-non-string两个参数。导入数据时采用–null-string和–null-non-string。</p><h2 id="6-6-Hive进行业务数据访问"><a href="#6-6-Hive进行业务数据访问" class="headerlink" title="6.6 Hive进行业务数据访问"></a>6.6 Hive进行业务数据访问</h2><p>Hive安装教程详见：<a href="https://m01ly.github.io/2020/11/14/bigdata-hive1/">https://m01ly.github.io/2020/11/14/bigdata-hive1/</a></p><p>后面就是通过Hive去对数据进行初步的分析。</p><link href="//src.wangriyu.wang/lib/Aplayer/APlayer.min.css" rel="stylesheet"><script src="//src.wangriyu.wang/lib/Aplayer/APlayer.min.js"></script><div id="aplayer"></div><script src="/js/player.js"></script></div></div><div class="copyright"><p><span>本文标题:</span><a href="/2020/11/23/bigdata-datacollect1-userbehavior/">大数据实践（一）数仓采集项目</a></p><p><span>文章作者:</span><a href="/" title="回到主页">m01ly</a></p><p><span>发布时间:</span>2020-11-23, 15:10:21</p><p><span>最后更新:</span>2021-11-25, 10:30:35</p><p><span>原始链接:</span><a class="post-url" href="/2020/11/23/bigdata-datacollect1-userbehavior/" title="大数据实践（一）数仓采集项目">https://m01ly.github.io/2020/11/23/bigdata-datacollect1-userbehavior/</a></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target="_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。</p></div><nav id="article-nav"><div id="article-nav-newer" class="article-nav-title"><a href="/2020/11/25/bigdata-HBase1-setup/">HBase学习笔记（一） 安装教程</a></div><div id="article-nav-older" class="article-nav-title"><a href="/2020/11/22/bigdata-sqoop/">sqoop安装教程</a></div></nav></article><div id="toc" class="toc-article"><strong class="toc-title">文章目录</strong><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E6%95%B0%E4%BB%93%E6%A6%82%E5%BF%B5"><span class="toc-text">1 数仓概念</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E9%A1%B9%E7%9B%AE%E9%9C%80%E6%B1%82%E5%8F%8A%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-text">2 项目需求及架构设计</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E9%A1%B9%E7%9B%AE%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90"><span class="toc-text">2.1 项目需求分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E9%A1%B9%E7%9B%AE%E6%A1%86%E6%9E%B6"><span class="toc-text">2.2 项目框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B"><span class="toc-text">2.2.1 技术选型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B%E8%AE%BE%E8%AE%A1"><span class="toc-text">2.2.2 系统数据流程设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3-%E6%A1%86%E6%9E%B6%E7%89%88%E6%9C%AC%E9%80%89%E5%9E%8B"><span class="toc-text">2.2.3 框架版本选型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-4-%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%80%89%E5%9E%8B"><span class="toc-text">2.2.4 服务器选型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-5-%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E8%A7%84%E5%88%92%E8%AE%BE%E8%AE%A1"><span class="toc-text">2.2.5 集群资源规划设计</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E6%A8%A1%E5%9D%97"><span class="toc-text">3 数据生成模块</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E7%9B%AE%E6%A0%87%E6%95%B0%E6%8D%AE"><span class="toc-text">3.1 目标数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-%E9%A1%B5%E9%9D%A2"><span class="toc-text">3.1.1 页面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E4%BA%8B%E4%BB%B6"><span class="toc-text">3.1.2 事件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-3-%E6%9B%9D%E5%85%89"><span class="toc-text">3.1.3 曝光</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-4-%E5%90%AF%E5%8A%A8"><span class="toc-text">3.1.4 启动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-5-%E9%94%99%E8%AF%AF"><span class="toc-text">3.1.5 错误</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E6%95%B0%E6%8D%AE%E5%9F%8B%E7%82%B9"><span class="toc-text">3.2 数据埋点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-%E4%B8%BB%E6%B5%81%E5%9F%8B%E7%82%B9%E6%96%B9%E5%BC%8F%EF%BC%88%E4%BA%86%E8%A7%A3%EF%BC%89"><span class="toc-text">3.2.1 主流埋点方式（了解）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-%E5%9F%8B%E7%82%B9%E6%95%B0%E6%8D%AE%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84"><span class="toc-text">3.2.2 埋点数据日志结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-%E5%9F%8B%E7%82%B9%E6%95%B0%E6%8D%AE%E4%B8%8A%E6%8A%A5%E6%97%B6%E6%9C%BA"><span class="toc-text">3.2.3 埋点数据上报时机</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%92%8CJDK%E5%87%86%E5%A4%87"><span class="toc-text">3.3 服务器和JDK准备</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%87%86%E5%A4%87"><span class="toc-text">3.3.1 服务器准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%87%86%E5%A4%87%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="toc-text">3.3.2 阿里云服务器准备（可选）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-3-JDK%E5%87%86%E5%A4%87"><span class="toc-text">3.3.3 JDK准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-4-%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E"><span class="toc-text">3.3.4 环境变量配置说明</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E6%A8%A1%E6%8B%9F%E6%95%B0%E6%8D%AE"><span class="toc-text">3.4 模拟数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-1-%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E"><span class="toc-text">3.4.1 使用说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2-%E9%9B%86%E7%BE%A4%E6%97%A5%E5%BF%97%E7%94%9F%E6%88%90%E8%84%9A%E6%9C%AC"><span class="toc-text">3.4.2 集群日志生成脚本</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E6%A8%A1%E5%9D%97"><span class="toc-text">4 数据采集模块</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E9%9B%86%E7%BE%A4%E6%89%80%E6%9C%89%E8%BF%9B%E7%A8%8B%E6%9F%A5%E7%9C%8B%E8%84%9A%E6%9C%AC"><span class="toc-text">4.1 集群所有进程查看脚本</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-Hadoop%E5%AE%89%E8%A3%85"><span class="toc-text">4.2 Hadoop安装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C%E4%B9%8BHDFS%E5%AD%98%E5%82%A8%E5%A4%9A%E7%9B%AE%E5%BD%95"><span class="toc-text">4.2.1 项目经验之HDFS存储多目录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2-%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E5%9D%87%E8%A1%A1"><span class="toc-text">4.2.2 集群数据均衡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3-%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C%E4%B9%8B%E6%94%AF%E6%8C%81LZO%E5%8E%8B%E7%BC%A9%E9%85%8D%E7%BD%AE"><span class="toc-text">4.2.3 项目经验之支持LZO压缩配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-4-%E6%B5%8B%E8%AF%95LZO"><span class="toc-text">4.2.4 测试LZO</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-4-%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C%E4%B9%8BLZO%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95"><span class="toc-text">4.2.4 项目经验之LZO创建索引</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-5-%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C%E4%B9%8B%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95"><span class="toc-text">4.2.5 项目经验之基准测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-6-%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C%E4%B9%8BHadoop%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98"><span class="toc-text">4.2.6 项目经验之Hadoop参数调优</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-Zookeeper%E5%AE%89%E8%A3%85"><span class="toc-text">4.3 Zookeeper安装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-%E5%AE%89%E8%A3%85ZK"><span class="toc-text">4.3.1 安装ZK</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2-ZK%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2%E8%84%9A%E6%9C%AC"><span class="toc-text">4.3.2 ZK集群启动停止脚本</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-Kafka%E5%AE%89%E8%A3%85"><span class="toc-text">4.4 Kafka安装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1-Kafka%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85"><span class="toc-text">4.4.1 Kafka集群安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2-Kafka%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2%E8%84%9A%E6%9C%AC"><span class="toc-text">4.4.2 Kafka集群启动停止脚本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-3-Kafka%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="toc-text">4.4.3 Kafka常用命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-4-%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C%E4%B9%8BKafka%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95"><span class="toc-text">4.4.4 项目经验之Kafka压力测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-5-%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C%E4%B9%8BKafka%E6%9C%BA%E5%99%A8%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97"><span class="toc-text">4.4.5 项目经验之Kafka机器数量计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-6-%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C%E5%80%BCKafka%E5%88%86%E5%8C%BA%E6%95%B0%E8%AE%A1%E7%AE%97"><span class="toc-text">4.4.6 项目经验值Kafka分区数计算</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E9%A1%B9%E7%9B%AE1-%E9%87%87%E9%9B%86%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE"><span class="toc-text">5 项目1 采集用户行为数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E7%AC%AC%E4%B8%80%E5%B1%82flume%E9%87%87%E9%9B%86"><span class="toc-text">5.1 第一层flume采集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-1-%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C%E4%B9%8BFlume%E7%BB%84%E4%BB%B6%E9%80%89%E5%9E%8B"><span class="toc-text">5.1.1 项目经验之Flume组件选型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-2-%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86Flume%E9%85%8D%E7%BD%AE"><span class="toc-text">5.1.2 日志采集Flume配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-3-Flume%E6%8B%A6%E6%88%AA%E5%99%A8"><span class="toc-text">5.1.3 Flume拦截器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-4-%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86Flume%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2%E8%84%9A%E6%9C%AC"><span class="toc-text">5.1.4 日志采集Flume启动停止脚本</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E7%AC%AC%E4%BA%8C%E5%B1%82flume%E9%87%87%E9%9B%86"><span class="toc-text">5.2 第二层flume采集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-1-%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C%E4%B9%8BFlume%E7%BB%84%E4%BB%B6%E9%80%89%E5%9E%8B"><span class="toc-text">5.2.1 项目经验之Flume组件选型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-2-Flume%E6%8B%A6%E6%88%AA%E5%99%A8"><span class="toc-text">5.2.2 Flume拦截器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-3-%E6%97%A5%E5%BF%97%E6%B6%88%E8%B4%B9Flume%E9%85%8D%E7%BD%AE"><span class="toc-text">5.2.3 日志消费Flume配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-4-%E6%97%A5%E5%BF%97%E6%B6%88%E8%B4%B9Flume%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2%E8%84%9A%E6%9C%AC"><span class="toc-text">5.2.4 日志消费Flume启动停止脚本</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C%E4%B9%8BFlume%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96"><span class="toc-text">5.3 项目经验之Flume内存优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-%E9%87%87%E9%9B%86%E9%80%9A%E9%81%93%E5%90%AF%E5%8A%A8-%E5%81%9C%E6%AD%A2%E8%84%9A%E6%9C%AC"><span class="toc-text">5.4 采集通道启动&#x2F;停止脚本</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-1-%E6%95%B0%E6%8D%AE%E9%80%9A%E9%81%93%E6%B5%8B%E8%AF%95"><span class="toc-text">5.4.1 数据通道测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-2-%E9%87%87%E9%9B%86%E9%80%9A%E9%81%93%E5%90%AF%E5%8A%A8-%E5%81%9C%E6%AD%A2%E8%84%9A%E6%9C%AC"><span class="toc-text">5.4.2 采集通道启动&#x2F;停止脚本</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E9%A1%B9%E7%9B%AE2-%E9%87%87%E9%9B%86%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE"><span class="toc-text">6 项目2 采集业务数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-%E7%94%B5%E5%95%86%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B"><span class="toc-text">6.1 电商业务流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-%E7%94%B5%E5%95%86%E5%B8%B8%E8%AF%86"><span class="toc-text">6.2 电商常识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-1-SKU%E5%92%8CSPU"><span class="toc-text">6.2.1 SKU和SPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-2-%E5%B9%B3%E5%8F%B0%E5%B1%9E%E6%80%A7%E5%92%8C%E9%94%80%E5%94%AE%E5%B1%9E%E6%80%A7"><span class="toc-text">6.2.2 平台属性和销售属性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E6%9E%B6%E6%9E%84"><span class="toc-text">6.3 业务数据采集架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5"><span class="toc-text">6.4 同步策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-1-%E5%85%A8%E9%87%8F%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5"><span class="toc-text">6.4.1 全量同步策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-2-%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5"><span class="toc-text">6.4.2 增量同步策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-3-%E6%96%B0%E5%A2%9E%E5%8F%8A%E5%8F%98%E5%8C%96%E7%AD%96%E7%95%A5"><span class="toc-text">6.4.3 新增及变化策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-4-%E7%89%B9%E6%AE%8A%E7%AD%96%E7%95%A5"><span class="toc-text">6.4.4 特殊策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5HDFS"><span class="toc-text">6.5 业务数据导入HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-1-%E5%88%86%E6%9E%90%E8%A1%A8%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5"><span class="toc-text">6.5.1 分析表同步策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-2-%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A6%96%E6%97%A5%E5%90%8C%E6%AD%A5%E8%84%9A%E6%9C%AC"><span class="toc-text">6.5.2 业务数据首日同步脚本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-3-%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C"><span class="toc-text">6.5.3 项目经验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-Hive%E8%BF%9B%E8%A1%8C%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE"><span class="toc-text">6.6 Hive进行业务数据访问</span></a></li></ol></li></ol></div><style>.left-col .switch-area,.left-col .switch-btn{display:none}.toc-level-6 i,.toc-level-6 ol{display:none!important}</style><input type="button" id="tocButton" value="隐藏目录" title="点击按钮隐藏或者显示文章目录"><script>yiliaConfig.toc=["隐藏目录","显示目录",!0],$(".left-col").is(":hidden")&&$("#tocButton").attr("value",yiliaConfig.toc[1])</script><div class="share"><link rel="stylesheet" type="text/css" href="/share/iconfont.css"><link rel="stylesheet" href="/share/spongebob.min.css" type="text/css" media="all"><div class="social_share"><ul id="social_list" class="social_icon_list"></ul></div><script>var shareConfig={title:"大数据实践（一）数仓采集项目",url:window.location.href,author:"m01ly",img:"https:/img/avatar.jpg"}</script><script src="/share/qrcode.min.js"></script><script src="/share/spongebob.min.js"></script></div><section id="comments" style="margin:2em;padding:2em;background:rgba(255,255,255,.5)"><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script><div id="gitalk-container"></div><script type="text/javascript">var gitalk=new Gitalk({clientID:"41a199ade404435645c4",clientSecret:"1de34fbb95212de986a29fea6d6f22bb57b2d473",repo:"m01ly.github.io",owner:"m01ly",admin:["m01ly"],id:window.location.pathname});gitalk.render("gitalk-container")</script></section><div class="scroll" id="post-nav-button"><a href="/2020/11/25/bigdata-HBase1-setup/" title="上一篇: HBase学习笔记（一） 安装教程"><i class="fa fa-angle-left"></i> </a><a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a> <a href="/2020/11/22/bigdata-sqoop/" title="下一篇: sqoop安装教程"><i class="fa fa-angle-right"></i></a></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/01/04/pt-docker-escape/">docker逃逸常用方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/23/security-tools-kubebench/">kube-bench工具使用--</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/21/security-cve-recurrent-CVE-2019-5736/">Docker逃逸漏洞复现（CVE-2019-5736）</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/20/security-tools-kubehunter/">kube-hunter工具使用--</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/17/security-tools-kubesploit/">kubesploit工具使用--一个针对容器化环境的跨平台后渗透利用工具</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/07/security-tools/">security-tools</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/28/scan-nessus-compliance/">nessus扫描合规性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/22/cert-letsencrypt/">证书管理工具之letsencrypt</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/06/htps-recommend/">安全的TLS协议</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/07/leetcode-slidewindow/">滑动窗口相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/24/burpsuite-develop-detect-nginx/">开发burpsuite插件-识别nginx版本并列出已知CVE</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/21/burpsuite-develop/">从0开发burpsuite插件（Java）</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/26/machine-learning-classify-knn/">机器学习算法之KNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/07/openvas-develop/">openvas插件开发</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/30/pt-antSword/">渗透测试工具之蚁剑</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/11/linux-disk/">centos7把/mnt空间合并到/(根目录)</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/19/install-guide-elk-filebeats/">elk笔记三--利用elk+filebeat搭建SIEM系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/02/18/linux-jdk8/">linux安装jdk1.8</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/01/21/esc-DefectDojo/">DefectDojo安装与使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper3-API/">Zookeeper学习笔记（三） API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper2-framework/">Zookeeper学习笔记（二） 架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/01/bigdata-zookeeper1-setup/">Zookeeper学习笔记（一） 搭建教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/29/bigdata-Spark2-framework/">Spark学习笔记（二） 架构解析和RDD编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/29/bigdata-Spark1-setup/">Spark学习笔记（一） 搭建Spark</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase2-framework/">HBase学习笔记（二） HBase架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase4-phoenix/">HBase学习笔记（四） HBase整合phoenix和Hive</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase3-API/">HBase学习笔记（三） HBase的API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/25/bigdata-HBase1-setup/">HBase学习笔记（一） 安装教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/23/bigdata-datacollect1-userbehavior/">大数据实践（一）数仓采集项目</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/22/bigdata-sqoop/">sqoop安装教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/18/bigdata-kafka4-test/">kafka学习笔记（四） kafka面试集锦</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/17/bigdata-kafka3-API/">kafka学习笔记（三） kafka的API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/16/bigdata-flume3-monitor/">flume学习笔记（三） flum数据流监控及面试题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/16/bigdata-kafka2-framework/">kafka学习笔记（二） kafka框架深入</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-flume2-framework/">flume学习笔记（二） flum事务和部署架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-flume1-setup/">flume学习笔记（一） flume搭建</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive3/">Hive学习笔记（三） Hive的分区表和分桶表</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive5-example/">Hive学习笔记（五） Hive实战</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive4-optimize/">Hive学习笔记（四） Hive的企业级调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-hive2/">Hive学习笔记（二） Hive对数据基本操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/15/bigdata-kafka1-setup/">kafka学习笔记（一） kafka搭建</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/14/bigdata-hive1/">Hive学习笔记（一） Hive安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-mapreduce2-framework/">Hadoop 教程（五）mapreduce架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs1/">Hadoop 教程（二）安装hadoop集群-完全分布式部署及API使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs/">Hadoop 教程（一）hadoop介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-yarn-framework/">Hadoop 教程（六）yarn-架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-mapreduce1-setup/">Hadoop 教程（四）mapreduce介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/12/bigdata-hdfs3-framework/">Hadoop 教程（三）hdfs-架构解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/10/cipher-certificate-format/">证书的各种格式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/docker-guide/">docker使用大全</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/linux-cmd/">linux命令大全</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-metosploitInAliyun/">在阿里云主机反弹metosploit</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-info-collection/">信息收集</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/pt-tools/">最佳网络安全和黑客软件</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/18/mobilesecurity-experience/">小白如何在三天一步步逆向app，找到私钥</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/elk-login/">elk笔记二--通过X-Pack权限控制设置elk登录</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-centosInvm/">vm 安装centos 7教程详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/writeup-sqli-labs/">writeup-sqli-labs</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-elk-suricata/">elk笔记一---suricata+elk搭建入侵检测系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/11/install-guide-suricata/">centos7中安装suricata</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/10/pt-sqlbypass/">sql关键词绕过</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/04/pt-portinfo/">常见端口说明和攻击汇总</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/03/m01ly-wiki/">m01ly-wiki</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/03/htps-attack-heartbleed/">TLS攻击之心脏滴血</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/01/htps-attack-paddingoracle/">TLS 攻击之POODLE</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/31/blockcipher-padding/">分组密码--填充模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/31/blockcipher-operation-mode/">分组密码--工作模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/hexo-guide/">Hexo踩坑</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/scan-awvs-nessus/">AWVS和Nessus镜像安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/scan-zap/">ZAP的安装和使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/pt-tiquan/">提权</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/htps-tools/">TLS安全检测小工具</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/htps-build/">搭建https网站</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/apple/">竟然有人能把https/TLS1.2协议讲的这么详细</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/26/leetcode-binary/">二分查找相关的题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/16/leetcode-stackandqueue/">栈和队列相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/07/leetcode-list/">链表相关题目</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/06/leetcode-daily/">leetcode每日一题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/29/leetcode-sort/">排序算法</a></li></ul><script></script></div><footer id="footer"><div class="outer"><div id="footer-info"><div class="footer-left"><i class="fa fa-copyright"></i> 2017-2022 冀-18010769-1</div><div class="visit"><span id="busuanzi_container_site_pv" style="display:none"><span id="site-visit" title="本站到访人数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span> </span></span><span>| </span><span id="busuanzi_container_page_pv" style="display:none"><span id="page-visit" title="本页访问次数"><i class="fa fa-eye" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span></span></span></div><div class="footer-right"><i class="fa fa-heart"></i><a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架"> Hexo</a> Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a></div></div></div></footer></div><script type="application/javascript">var leftWidth,hide=!1;$(".hide-left-col").click(function(){hide=hide?($(".left-col").css("width",leftWidth),$(".left-col .intrude-less").fadeIn(200),$("#tocButton").fadeIn(200),"block"===$("#switch-btn").css("display")&&"block"===$("#switch-area").css("display")||$("#toc").fadeIn(200),$(".hide-left-col").css("left",leftWidth).html('<i class="fa fa-angle-double-left"></i>'),$(".mid-col").css("left",leftWidth),$("#post-nav-button").css("left",leftWidth),$("#post-nav-button > a:nth-child(2)").css("display","block"),!1):(leftWidth=$(".left-col")[0].style.width,$(".left-col").css("width",0),$(".left-col .intrude-less").fadeOut(200),$("#toc").fadeOut(100),$("#tocButton").fadeOut(100),$(".hide-left-col").css("left",0).html('<i class="fa fa-angle-double-right"></i>'),$(".mid-col").css("left",0),$("#post-nav-button").css("left",0),$("#post-nav-button > a:nth-child(2)").css("display","none"),$(".post-list").is(":visible")&&($("#post-nav-button .fa-bars,#post-nav-button .fa-times").toggle(),$(".post-list").toggle()),!0)})</script><script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.3.5/require.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[["$$","$$"],["$","$"],["\\(","\\)"]],processEscapes:!0,skipTags:["script","noscript","style","textarea","pre","code"]}}),MathJax.Hub.Queue(function(){var a,e=MathJax.Hub.getAllJax();for(a=0;a<e.length;a+=1)e[a].SourceElement().parentNode.className+=" has-jax"})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script><div class="scroll" id="scroll"><a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a> <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a> <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a></div><script>var oOpenInNew={post:".copyright a[href]",friends:"#js-friends a",socail:".social a"};for(var x in oOpenInNew)$(oOpenInNew[x]).attr("target","_blank")</script><script>var titleTime,originTitle=document.title;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="(つェ⊂)"+originTitle,clearTimeout(titleTime)):(document.title="(*´∇｀*)~ "+originTitle,titleTime=setTimeout(function(){document.title=originTitle},2e3))})</script><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><link href="//cdn.bootcss.com/aos/2.2.0/aos.css" rel="stylesheet"><script type="text/javascript">AOS.init({easing:"ease-out-back",once:!0})</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"live2d_models/live2d-widget-model-izumi"},"display":{"position":"right","width":100,"height":200,"hOffset":-50,"vOffset":-85},"mobile":{"show":false},"react":{"opacityDefault":0.9,"opacityOnHover":0.3},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false});</script></body></html>